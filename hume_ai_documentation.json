{
    "Welcome to Hume AI": {
        "Welcome to Hume AI": {
            "url": "https://dev.hume.ai/intro",
            "content": {
                "title": "Welcome to Hume AI | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nEmpathic Voice Interface\nText-to-speech\nExpression Measurement\nAPI Reference\nSDKs\nExample Code\nGet Support\nIntroduction\nWelcome to Hume AI\nCopy page\nHume AI builds AI models that enable technology to communicate with empathy and learn to make people happy.\nSo much of human communication—in-person, text, audio, or video—is shaped by emotional expression. These cues allow us to attend to each other’s well-being. Our platform provides the APIs needed to ensure that technology, too, is guided by empathy and the pursuit of human well-being.\nEmpathic Voice Interface\nHume’s Empathic Voice Interface (EVI) is the world’s first emotionally intelligent voice AI. It is the only API that measures nuanced vocal modulations and responds to them using an empathic large language model (eLLM), which guides language and speech generation. Trained on millions of human interactions, our eLLM unites language modeling and text-to-speech with better EQ, prosody, end-of-turn detection, interruptibility, and alignment.\nText-to-speech\nOctave TTS, the first text-to-speech system built on LLM intelligence. Unlike conventional TTS that merely “reads” words, Octave is a “speech-language model” that understands what words mean in context, unlocking a new level of expressiveness and nuance.\nExpression Measurement\nHume’s state-of-the-art expression measurement models for the voice, face, and language are built on 10+ years of research and advances in semantic space theory pioneered by Alan Cowen. Our expression measurement models are able to capture hundreds of dimensions of human expression in audio, video, and images.\nAPI Reference\nOur API reference provides detailed descriptions of our REST and WebSocket endpoints. Explore request and response formats, usage examples, and everything you need to integrate Hume APIs.\nEmpathic Voice Interface (EVI)\nAPI that measures nuanced vocal modulations and responds to them using an empathic large language model\nText-to-speech (TTS)\nSynthesize text to speech using Octave, Hume’s state-of-the-art speech language model\nExpression Measurement\nAnalyze facial, vocal, and linguistic expressions across 48+ dimensions to unlock deeper emotional insights\nSDKs\nJumpstart your development with SDKs built for Hume APIs. They handle authentication, requests, and workflows to make integration straightforward. With support for React, TypeScript, and Python, our SDKs provide the tools you need to build efficiently across different environments.\nReact SDK\nIntegrate Hume’s Empathic Voice Interface into React apps with tools for audio recording, playback, and API interaction\nTypeScript SDK\nWork with Hume’s APIs using type-safe utilities and API wrappers for TypeScript and JavaScript\nPython SDK\nAccess Hume’s APIs in Python with async/sync clients, error handling, and streaming tools\nExample Code\nExplore step-by-step guides and sample projects for integrating Hume APIs. Our GitHub repositories include ready-to-use code and open-source SDKs to support your development process in various environments.\nhume-api-examples\nBrowse sample code and projects designed to help you integrate Hume APIs\nGitHub Organization\nExplore all of Hume’s open-source SDKs, examples, and public-facing code\nGet Support\nNeed help? Our team is here to support you with any questions or challenges.\nDiscord\nJoin our Discord community for direct support from the Hume team\nWas this page helpful?\nYes\nNo\nGetting your API keys\nLearn how to obtain your API keys and understand the supported authentication strategies for securely accessing Hume APIs.\nNext\nBuilt with"
            }
        }
    },
    "Getting your API keys": {
        "Getting your API keys": {
            "url": "https://dev.hume.ai/docs/introduction/api-key",
            "content": {
                "title": "Getting your API keys | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nAPI keys\nAuthentication strategies\nAPI key authentication\nToken authentication\nRegenerating API keys\nIntroduction\nGetting your API keys\nCopy page\nLearn how to obtain your API keys and understand the supported authentication strategies for securely accessing Hume APIs.\nAPI keys\nEach Hume account is provisioned with an API key and Secret key. These keys are accessible from the Hume Portal.\nSign in: Visit the Hume Portal and log in, or create an account.\nView your API keys: Navigate to the API keys page to view your keys.\nOpen the API keys page from the left sidebar\nAuthentication strategies\nHume APIs support two authentication strategies:\nAPI key strategy: Use API key authentication for making server-side requests. API key authentication allows you to make authenticated requests by supplying a single secret using the X-Hume-Api-Key header. Do not expose your API key in client-side code. All Hume APIs support this authentication strategy.\nToken strategy: Use Token authentication for making client-side requests. With Token authentication you first obtain a temporary access token by making a server-side request first, and use the access token when making client-side requests. This allows you to avoid exposing the API key to the client. Access tokens expire after 30 minutes, and you must obtain a new one. Today, only our Empathic Voice Interface (EVI) and Text-to-speech APIs support this authentication strategy.\nAPI key authentication\nTo use API key authentication on REST API endpoints, include the API key in the X-Hume-Api-Key request header.\nEVI\nTTS\nExpression Measurement\n$ curl https://api.hume.ai/v0/evi/{path} \\\n>   --header 'Accept: application/json; charset=utf-8' \\\n>   --header \"X-Hume-Api-Key: <YOUR API KEY>\"\nFor WebSocket endpoints, include the API key as a query parameter in the URL.\nEVI\nExpression Measurement\n1 const ws = new WebSocket(`wss://api.hume.ai/v0/evi/chat?api_key=${apiKey}`);\nToken authentication\nTo use Token authentication you must first obtain an Access Token from the POST /oauth2-cc/token endpoint.\nThis is a unique endpoint that uses the “Basic” authentication scheme, with your API key as the username and the Secret key as the password. This means you must concatenate your API key and Secret key, separated by a colon (:), base64 encode this value, and then put the result in the Authorization header of the request, prefixed with Basic .\nYou must also supply the grant_type=client_credentials parameter in the request body.\ncURL\nTypeScript\nPython\n1 # Assumes `HUME_API_KEY` and `HUME_SECRET_KEY` are defined as environment variables\n2 response=$(curl -s 'https://api.hume.ai/oauth2-cc/token' \\\n3   -u \"${HUME_API_KEY}:${HUME_SECRET_KEY}\" \\\n4   -d 'grant_type=client_credentials')\n5\n6 # Uses `jq` to extract the access token from the JSON response body\n7 accessToken=$(echo $response | jq -r '.access_token')\nOn the client side, open an authenticated WebSocket by including the access token as a query parameter in the URL.\nEVI\n1 const ws = new WebSocket(`wss://api.hume.ai/v0/evi/chat?access_token=${accessToken}`);\nOr, make a REST request by including the access token in the Authorization header.\nEVI\n1 fetch('https://api.hume.ai/v0/evi/chats', {\n2   headers: {\n3     Authorization: `Bearer ${accessToken}`,\n4   },\n5 });\nRegenerating API keys\nAPI keys can be regenerated by clicking the Regenerate keys button on the API keys page. This permanently invalidates the current keys, requiring you to update any applications using them.\nRegenerate API keys confirmation message\nWas this page helpful?\nYes\nNo\nPrevious\nSupport\nFind the right support channel, get answers to your questions, and learn about available resources for Hume developers.\nNext\nBuilt with"
            }
        }
    },
    "Support": {
        "Support": {
            "url": "https://dev.hume.ai/support",
            "content": {
                "title": "Support | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nAsk AI - Instant Documentation Help\nDiscord Developer Community\nImportant Discord Resources\nKey Support Channels\nSpecialized Support Contacts\nPrograms and Opportunities\nAdditional Resources\nIntroduction\nSupport\nCopy page\nFind the right support channel, get answers to your questions, and learn about available resources for Hume developers.\nAt Hume, we’re committed to providing you with the resources and assistance you need to succeed with our technology. Here’s how to get help when you need it.\nAsk AI - Instant Documentation Help\nLooking for quick answers? Our Ask AI feature is available on this page and throughout our documentation. Simply click the Ask AI button in the bottom right corner to get immediate, AI-generated answers about Hume’s products, features, and APIs.\nThe AI assistant can help with:\nUnderstanding Hume’s features and capabilities.\nFinding specific information within our documentation.\nAnswering common questions about our API and products.\nProviding code examples and implementation guidance.\nTry it now by clicking the Ask AI button in the bottom-right corner of this page.\nFor direct assistance from the Hume team, our Discord community is the best channel to request assistance.\nDiscord Developer Community\nOur primary support channel is our Discord Developer Community Server, where you can get real-time assistance from our team and connect with other members of our community.\nJoin our Discord Community\nImportant Discord Resources\nFor the best support experience, please familiarize yourself with:\nServer Rules\nSupport Guidelines\nUser Verification\nKey Support Channels\nOnce you’ve joined our Discord server, you can find help in these dedicated channels:\n⁠#ask-ai: Get quick AI-generated answers for questions about Hume, including our company, consumer apps, and developer platform.\n⁠#api-support-chat: A chat-based channel for developers to request support for issues pertaining to consuming the API. This is a good place for general questions about our APIs, supported features, example code, or our documentation.\n⁠#api-support-forum: A forum-based channel for posting specific issues they you are experiencing in your development with Hume APIs. This channel is good for raising technical issues you are running into with our APIs or SDKs.\n⁠#hume-app-support: A support channel to submit feedback and request support for issues you experience while using Hume’s consumer apps: iOS App and Web App.\nSpecialized Support Contacts\nFor specific inquiries, please use the following dedicated email channels:\nGeneral and Account-related: hello@hume.ai - For general questions and account-related inquiries.\nLegal and Data Privacy: legal@hume.ai - For legal questions, data privacy concerns, or compliance inquiries.\nBilling and Payments: billing@hume.ai - For questions about your account, invoices, or payment issues.\nPress and Media: press@hume.ai - For press inquiries, interview requests, or media kit access.\nPrograms and Opportunities\nHume offers several specialized programs for researchers, startups, and businesses:\nResearcher Access Program: Academic researchers and research institutions can apply for special access to Hume’s API and technology for research purposes.\nApply for the Researcher Access Program\nEVI Startup Grant Program: Early-stage startups working in emotional AI can apply for our grant program to access Hume’s technology at reduced or no cost.\nApply for Startup Grant\nSales and Partnerships: For enterprise solutions, custom integrations, or business partnership opportunities, reach out through our dedicated form.\nContact Sales & Partnerships\nAdditional Resources\nDocumentation: Browse our technical documentation for guides, tutorials, and reference materials.\nPublic Roadmap: Stay updated on our public roadmap for upcoming features and improvements.\nPricing: View our pricing page for current plans and rates.\nFeature Requests: Submit and vote on feature requests to help us prioritize development.\nStatus Page: Check our system status for any ongoing service issues.\nIf you can’t find the answer you need through these channels, please reach out through our contact page and we’ll direct you to the appropriate resource.\nWas this page helpful?\nYes\nNo\nPrevious\nText-to-speech (TTS)\nIntroduction to Hume’s TTS API, including its features, usage limits, and key concepts for integration.\nNext\nBuilt with"
            }
        }
    },
    "Pricing": {
        "Pricing": {
            "url": "https://link.hume.ai/pricing",
            "content": {
                "title": "Pricing • Hume AI",
                "content": "Pricing\nOctave text-to-speech\nEVI & Expression Measurement\nOctave text-to-speech pricing\nFree\n$0\n/ month\nWhat's included:\n10,000 characters of text to speech per month (~10 minutes)\nUnlimited custom voices\nGET STARTED\nStarter\n$3\n/ month\nEverything in Free, plus:\n30,000 characters of text to speech per month (~30 minutes)\nUnlimited custom voices\n20 projects\nCommercial license\nGET STARTED\nMOST POPULAR\nCreator\n$10\n/ month\nEverything in Starter, plus:\n100,000 characters of text to speech per month (~100 minutes)\nUsage based pricing for additional characters ($0.20/1,000)\nUnlimited custom voices\n1,000 projects\nCommercial license\nGET STARTED\nPro\n$50\n/ month\nEverything in Creator, plus:\n500,000 characters of text to speech per month (~500 minutes)\nUsage based pricing for additional characters ($0.15/1,000)\nUnlimited custom voices\n3,000 projects\nCommercial license\nGET STARTED\nScale\n$150\n/ month\nEverything in Pro, plus:\n2,000,000 characters of text to speech per month (~2,000 minutes)\nUsage based pricing for additional characters ($0.13/1,000)\nUnlimited custom voices\n10,000 projects\nCommercial license\nGET STARTED\nBusiness\n$900\n/ month\nEverything in Scale, plus:\n10,000,000 characters of text to speech per month (~10,000 minutes)\nUsage based pricing for additional characters ($0.10/1,000)\nUnlimited custom voices\n20,000 projects\nCommercial license\nGET STARTED\nEnterprise\nCustom price\nEverything in Business, plus:\nAs much usage as you need\nCustom terms & assurance around DPA/SLAs\nSecurity questionnaires\nUnlimited custom voices\nSignificantly discounted pricing at scale\nPriority support\nCommercial license\nCONTACT SALES\nCompare our plans\nFree\nGET STARTED\nStarter\nGET STARTED\nCreator\nGET STARTED\nPro\nGET STARTED\nScale\nGET STARTED\nBusiness\nGET STARTED\nEnterprise\nCONTACT SALES\nPrice/month $0 $3 $10 $50 $150 $900 Custom\nMonthly characters included\nVIEW FEATURE INFO\n10,000 characters (~10 minutes) 30,000 characters (~30 minutes) 100,000 characters (~100 minutes) 500,000 characters (~500 minutes) 2,000,000 characters (~2,000 minutes) 10,000,000 characters (~10,000 minutes) As much as you need\nAdditional characters cost (usage-based)\nVIEW FEATURE INFO\n$0.20/1,000 $0.15/1,000 $0.13/1,000 $0.10/1,000 Custom\nProjects\n20 1,000 3,000 10,000 20,000 As much as you need\nCustom voices Unlimited Unlimited Unlimited Unlimited Unlimited Unlimited Unlimited\nVoice cloning Coming soon Coming soon Coming soon Coming soon Coming soon Coming soon Coming soon\nCommercial license\nEVI & Expression Measurement pricing\nPay as you go\nIdeal for individual developers, startups, and businesses that prefer a flexible pricing structure based on usage\n$20 in free credit\nOnly pay for what you use\nNo upfront payment or commitment\nTechnical support in Discord\nGET STARTED\nEnterprise\nFor businesses with high volume and advanced data control requirements\nHigh volume discounts\nDataset licenses\nOn-prem solutions\nCustom integrations and features\nDedicated technical support\nCONTACT SALES\nPricing\nEmpathic Voice Interface API\nEVI 1 (Legacy)\nEVI 2\nEnterprise\nPrice\n$0.102 / min\n$0.072 / min\nVolume discounts\nTranscription with expression measures\nExpressive TTS with prosody generation\nProsody generation\nInterruptibility\nVoice customizability\nLow latency\n900ms - 2000ms\n500ms - 800ms\nOn-premises solutions\nBase voices\n3 voices\n7 voices\nCustom voices\nMultilingual support\nEnglish only\nMultiple languages soon\nCustom languages\nExpression Measurement API\nPay as you go\nEnterprise\nVideo with audio\nFacial expression, Speech prosody, Vocal burst, Emotional language, Facemesh, Transcription\n$0.0276 / min\nVolume discounts\nAudio only\nSpeech prosody, Vocal burst, Emotional language, Transcription\n$0.0213 / min\nVolume discounts\nVideo only\nFacial expression, Facemesh\n$0.015 / min\nVolume discounts\nImages\nFacial expression, Facemesh\n$0.00068 / image\nVolume discounts\nText only\nEmotional language\n$0.00008 / word\nVolume discounts\nCustom Models API\nPay as you go\nEnterprise\nTraining\nBuild and customize models to suit your needs\nFree\nFree\nInference\nDeploy and use your trained models\nSame as Expression Measurement API\nVolume discounts"
            }
        }
    },
    "Overview": {
        "Overview": {
            "url": "https://dev.hume.ai/docs/expression-measurement/overview",
            "content": {
                "title": "Expression Measurement | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nIntro\nMeasurements\nModel training\nTry out the models\nSpecific expressions by modality\nTrain your own custom model\nExpression Measurement\nExpression Measurement\nCopy page\nHume’s state of the art expression measurement models for the voice, face, and language.\nIntro\nHume’s state of the art expression measurement models for the voice, face, and language are built on 10+ years of research and advances in computational approaches to emotion science (semantic space theory) pioneered by our team. Our expression measurement models are able to capture hundreds of dimensions of human expression in audio, video, and images.\nMeasurements\nFacial Expression, including subtle facial movements often seen as expressing love or admiration, awe, disappointment, or cringes of empathic pain, along 48 distinct dimensions of emotional meaning. Our Facial Expression model will also optionally output FACS 2.0 measurements, our model of facial movements including traditional Action Units (AUs such as “Inner brow raise”, “Nose crinkle”) and facial descriptions (“Smile”, “Wink”, “Hand over mouth”, “Hand over eyes”)\nSpeech Prosody, or the non-linguistic tone, rhythm, and timbre of speech, spanning 48 distinct dimensions of emotional meaning.\nVocal Burst, including laughs, sighs, huhs, hmms, cries and shrieks (to name a few), along 48 distinct dimensions of emotional meaning.\nEmotional Language, or the emotional tone of transcribed text, along 53 dimensions.\nExpressions are complex and multifaceted; they should not be treated as direct inferences of emotional experience. To learn more about the science behind expression measurement, visit the About the science page.\nTo learn more about how to use our models visit our API reference.\nModel training\nThe models were trained on human intensity ratings of large-scale, experimentally controlled emotional expression data gathered using the methods described in these papers: Deep learning reveals what vocal bursts express in different cultures and Deep learning reveals what facial expressions mean to people in different cultures.\nWhile our models measure nuanced expressions that people most typically describe with emotion labels, it’s important to remember that they are not a direct readout of what someone is experiencing. Sometimes, the outputs from facial and vocal models will show different emotional meanings, which is completely normal. Generally speaking, emotional experience is subjective and its expression is multimodal and context-dependent.\nTry out the models\nLearn how you can use the Expression Measurement API through both REST and WebSockets.\nREST\nUse REST endpoints to process batches of videos, images, text, or audio files.\nWebSocket\nUse WebSocket endpoints when you need real-time predictions, such as processing a webcam or microphone stream.\nREST and WebSocket endpoints provide access to all of the same Hume models, but with different speed and scale tradeoffs. All models share a common response format, which associates a score with each detected expression. Scores indicate the degree to which a human rater would assign an expression to a given sample of video, text or audio.\nSpecific expressions by modality\nOur models measure 53 expressions identified through the subtleties of emotional language and 48 expressions discerned from facial cues, vocal bursts, and speech prosody.\nExpression Language Face/Burst/Prosody\nAdmiration\nAdoration\nAesthetic Appreciation\nAmusement\nAnger\nAnnoyance\nAnxiety\nAwe\nAwkwardness\nBoredom\nCalmness\nConcentration\nConfusion\nContemplation\nContempt\nContentment\nCraving\nDesire\nDetermination\nDisappointment\nDisapproval\nDisgust\nDistress\nDoubt\nEcstasy\nEmbarrassment\nEmpathic Pain\nEnthusiasm\nEntrancement\nEnvy\nExcitement\nFear\nGratitude\nGuilt\nHorror\nInterest\nJoy\nLove\nNostalgia\nPain\nPride\nRealization\nRelief\nRomance\nSadness\nSarcasm\nSatisfaction\nShame\nSurprise (negative)\nSurprise (positive)\nSympathy\nTiredness\nTriumph\nTrain your own custom model\nOur Custom Models API builds on our expression measurement models and state-of-the-art eLLMs to bring custom insights to your application. Developed using transfer learning from our expression measurement models and eLLMs, our Custom Models API can predict almost any outcome more accurately than language alone, whether it’s toxicity, depressed mood, driver drowsiness, or any other metric important to your users.\nCustom Models\nBuild on our expression measurement models to bring custom insights to your application.\nWas this page helpful?\nYes\nNo\nPrevious\nProcessing batches of media files\nNext\nBuilt with"
            }
        }
    },
    "Quickstart": {
        "Next.js": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/quickstart/nextjs",
            "content": {
                "title": "EVI Next.js Quickstart | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nNext.js\nTypeScript\nPython\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nEmpathic Voice Interface (EVI)\nQuickstart\nEVI Next.js Quickstart\nCopy page\nA quickstart guide for implementing the Empathic Voice Interface (EVI) with Next.js.\nThis guide provides instructions for integrating EVI into your Next.js projects with Hume’s React SDK and includes detailed steps for using EVI with Next.js App Router and Pages Router.\nKickstart your project with our pre-configured Vercel template for the Empathic Voice Interface. Install with one click to instantly set up a ready-to-use project and start building with TypeScript right away!\nNext.js (App Router)\nNext.js (Pages Router)\nThis tutorial utilizes Hume’s React SDK to interact with EVI. It includes detailed steps for both the App Router in Next.js and is broken down into four key components:\nAuthentication: Generate and use an access token to authenticate with EVI.\nSetting up context provider: Set up the <VoiceProvider/>.\nStarting a chat and display messages: Implement the functionality to start a chat with EVI and display messages.\nThat’s it!: Audio playback and interruptions are handled for you.\nThe Hume React SDK abstracts much of the logic for managing the WebSocket connection, as well as capturing and preparing audio for processing. For a closer look at how the React package manages these aspects of the integration, we invite you to explore the source code here: @humeai/voice-react\nTo see this code fully implemented within a frontend web application using the App Router from Next.js, visit this GitHub repository: evi-next-js-app-router-quickstart.\n1\nPrerequisites\nBefore you begin, you will need to have an existing Next.js project set up using the App Router.\n2\nAuthenticate\nIn order to make an authenticated connection we will first need to generate an access token. Doing so will require your API key and Secret key. These keys can be obtained by logging into the portal and visiting the API keys page.\nIn the sample code below, the API key and Secret key have been saved to environment variables. Avoid hardcoding these values in your project to prevent them from being leaked.\nReact\n1 // ./app/page.tsx\n2 import ClientComponent from \"@/components/ClientComponent\";\n3 import { fetchAccessToken } from \"hume\";\n4\n5 export default async function Page() {\n6   const accessToken = await fetchAccessToken({\n7     apiKey: String(process.env.HUME_API_KEY),\n8     secretKey: String(process.env.HUME_SECRET_KEY),\n9   });\n10\n11   if (!accessToken) {\n12     throw new Error();\n13   }\n14\n15   return <ClientComponent accessToken={accessToken} />;\n16 }\n3\nSetup Context Provider\nAfter fetching our access token we can pass it to our ClientComponent. First we set up the <VoiceProvider/> so that our Messages and Controls components can access the context. We also pass the access token to the auth prop of the <VoiceProvider/> for setting up the WebSocket connection.\nTypeScript\n1 // ./components/ClientComponent.tsx\n2 \"use client\";\n3 import { VoiceProvider } from \"@humeai/voice-react\";\n4 import Messages from \"./Messages\";\n5 import Controls from \"./Controls\";\n6\n7 export default function ClientComponent({\n8   accessToken,\n9 }: {\n10   accessToken: string;\n11 }) {\n12   return (\n13     <VoiceProvider auth={{ type: \"accessToken\", value: accessToken }}>\n14       <Messages />\n15       <Controls />\n16     </VoiceProvider>\n17   );\n18 }\n4\nAudio input\n<VoiceProvider/> will handle the microphone and playback logic.\n5\nStarting session\nIn order to start a session, you can use the connect function. It is important that this event is attached to a user interaction event (like a click) so that the browser is capable of playing Audio.\nTypeScript\n1 // ./components/Controls.tsx\n2 \"use client\";\n3 import { useVoice, VoiceReadyState } from \"@humeai/voice-react\";\n4 export default function Controls() {\n5   const { connect, disconnect, readyState } = useVoice();\n6\n7   if (readyState === VoiceReadyState.OPEN) {\n8     return (\n9       <button\n10         onClick={() => {\n11           disconnect();\n12         }}\n13       >\n14         End Session\n15       </button>\n16     );\n17   }\n18\n19   return (\n20     <button\n21       onClick={() => {\n22         connect()\n23           .then(() => {\n24             /* handle success */\n25           })\n26           .catch(() => {\n27             /* handle error */\n28           });\n29       }}\n30     >\n31       Start Session\n32     </button>\n33   );\n34 }\n6\nDisplaying message history\nTo display the message history, we can use the useVoice hook to access the messages array. We can then map over the messages array to display the role (Assistant or User) and content of each message.\nTypeScript\n1 // ./components/Messages.tsx\n2 \"use client\";\n3 import { useVoice } from \"@humeai/voice-react\";\n4\n5 export default function Messages() {\n6   const { messages } = useVoice();\n7\n8   return (\n9     <div>\n10       {messages.map((msg, index) => {\n11         if (msg.type === \"user_message\" || msg.type === \"assistant_message\") {\n12           return (\n13             <div key={msg.type + index}>\n14               <div>{msg.message.role}</div>\n15               <div>{msg.message.content}</div>\n16             </div>\n17           );\n18         }\n19\n20         return null;\n21       })}\n22     </div>\n23   );\n24 }\n7\nInterrupt\nThis Next.js Quickstart will handle interruption events automatically!\nWas this page helpful?\nYes\nNo\nPrevious\nEVI TypeScript Quickstart\nA quickstart guide for implementing the Empathic Voice Interface (EVI) with TypeScript.\nNext\nBuilt with"
            }
        },
        "TypeScript": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/quickstart/typescript",
            "content": {
                "title": "EVI TypeScript Quickstart | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nNext.js\nTypeScript\nPython\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nEmpathic Voice Interface (EVI)\nQuickstart\nEVI TypeScript Quickstart\nCopy page\nA quickstart guide for implementing the Empathic Voice Interface (EVI) with TypeScript.\nThis tutorial provides step-by-step instructions for implementing EVI using Hume’s TypeScript SDK, and is broken down into five sections:\nAuthentication: Instantiate the Hume client using your API credentials.\nConnecting to EVI: Initialize a WebSocket connection to interact with EVI.\nCapturing & recording audio: Capture and prepare audio input to stream over the WebSocket.\nAudio playback: Play back the EVI’s audio output to the user.\nInterruption: Client-side management of user interruptions during the chat.\nThis guide references our TypeScript Quickstart example project. To see the full implementation, visit our API examples repository on GitHub: evi-typescript-quickstart.\n1\nAuthenticate\nTo establish an authenticated connection, first instantiate the Hume client with your API credentials. Visit our Getting your API keys page for details on how to obtain your credentials.\nThis example uses direct API key authentication for simplicity. For production browser environments, implement the Token authentication strategy instead to prevent exposing your API key in client-side code.\nTypeScript\n1 import { Hume, HumeClient } from 'hume';\n2\n3 // instantiate the Hume client and authenticate\n4 const client = new HumeClient({\n5   apiKey: import.meta.env.HUME_API_KEY,\n6 });\n2\nConnect\nWith the Hume client instantiated with your credentials, you can now establish an authenticated WebSocket connection with EVI and assign WebSocket event handlers. For now you can include placeholder event handlers to update in later steps.\nTypeScript\n1 import { Hume, HumeClient } from 'hume';\n2\n3 // Instantiate the Hume client and authenticate\n4 const client = new HumeClient({\n5   apiKey: import.meta.env.HUME_API_KEY,\n6 });\n7\n8 // Connect to EVI\n9 const socket = await client.empathicVoice.chat.connect({\n10   configId: import.meta.env.HUME_CONFIG_ID,\n11 });\n12\n13 // Define event handlers and assign them to WebSocket\n14 socket.on('open', handleWebSocketOpenEvent);\n15 socket.on('message', handleWebSocketMessageEvent);\n16 socket.on('error', handleWebSocketErrorEvent);\n17 socket.on('close', handleWebSocketCloseEvent);\n3\nAudio input\nNext we’ll go over capturing and streaming audio input over the WebSocket. First, handle user permissions to access the microphone. Next, use the Media Stream API to access the audio stream, and the MediaRecorder API to capture and base64 encode the audio chunks. Finally, stream the audio input by sending each chunk over the WebSocket as audio_input messages using the SDK’s sendAudioInput method.\nTypeScript\n1 import {\n2   convertBlobToBase64,\n3   ensureSingleValidAudioTrack,\n4   getAudioStream,\n5   getBrowserSupportedMimeType,\n6 } from 'hume';\n7\n8 /**--- Audio Recording State ---*/\n9 let recorder: MediaRecorder | null = null;\n10 let audioStream: MediaStream | null = null;\n11 const mimeTypeResult = getBrowserSupportedMimeType();\n12 const mimeType: MimeType = mimeTypeResult.success \n13   ? mimeTypeResult.mimeType \n14   : MimeType.WEBM;\n15\n16 // Define function for capturing audio\n17 async function startAudioCapture(): Promise<void> {\n18   try {\n19     audioStream = await getAudioStream();\n20     // Validate the stream\n21     ensureSingleValidAudioTrack(audioStream);\n22\n23     recorder = new MediaRecorder(audioStream, { mimeType });\n24     recorder.ondataavailable = handleAudioDataAvailable;\n25     recorder.onerror = (event) => {\n26       console.error(\"MediaRecorder error:\", event);\n27     }\n28     recorder.start(50);\n29   } catch (error) {\n30     console.error(\n31       \"Failed to initialize or start audio capture:\", error\n32     );\n33     throw error;\n34   }\n35 }\n36\n37 // Define a WebSocket open event handler to capture audio\n38 async function handleWebSocketOpen(): Promise<void> {\n39   console.log('WebSocket connection opened.');\n40   try {\n41     await startAudioCapture();\n42   } catch (error) {\n43     console.error(\"Failed to capture audio:\", error);\n44     alert(\"Failed to access microphone. Disconnecting.\");\n45     if (\n46       socket && \n47       socket.readyState !== WebSocket.CLOSING && \n48       socket.readyState !== WebSocket.CLOSED\n49     ) {\n50       socket.close();\n51     }\n52   }\n53 }\nAccepted audio formats include: mp3, wav, aac, ogg, flac, webm, avr, cdda, cvs/vms, aiff, au, amr, mp2, mp4, ac3, avi, wmv, mpeg, ircam.\n4\nAudio output\nEVI responds with multiple message types over the WebSocket:\nuser_message: This message encapsulates the transcription of the audio input. Additionally, it includes expression measurement predictions related to the speaker’s vocal prosody.\nassistant_message: EVI dispatches an AssistantMessage for every sentence within the response. This message not only relays the content of the response but also features predictions regarding the expressive qualities of the generated audio response.\naudio_output: An AudioOutput message accompanies each AssistantMessage. This contains the actual audio (binary) response corresponding to an AssistantMessage.\nassistant_end: EVI delivers an AssistantEnd message as the final piece of communication, signifying the conclusion of the response to the audio input.\nTo play the audio output from the response, define your logic for converting the received binary to a Blob, and create an HTMLAudioElement to play the audio.\nThen update the client’s message event handler to invoke the logic to play back the audio when received. To manage playback for the incoming audio, you can implement a basic queue and sequentially play back the audio.\nTypeScript\n1 /**--- Audio Playback State ---*/\n2 const audioQueue: Blob[] = [];\n3 const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)();\n4 let currentSource: AudioBufferSourceNode | null = null;\n5 let isPlaying = false;\n6\n7 // Plays the next audio chunk from the queue if available \n8 // and not already playing.\n9 async function playNextAudioChunk(): Promise<void> {\n10   if (isPlaying || audioQueue.length === 0) return;\n11   isPlaying = true;\n12\n13   const audioBlob = audioQueue.shift();\n14   if (!audioBlob) {\n15     isPlaying = false;\n16     return;\n17   }\n18\n19   try {\n20     // Safari requires a user gesture–driven resume\n21     if (audioContext.state === 'suspended') {\n22       await audioContext.resume();\n23     }\n24     // Decode the blob into an AudioBuffer\n25     const arrayBuffer = await audioBlob.arrayBuffer();\n26     const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);\n27\n28     // Create a source node and play it\n29     const source = audioContext.createBufferSource();\n30     source.buffer = audioBuffer;\n31     source.connect(audioContext.destination);\n32     source.start();\n33\n34     // Track it so we can stop mid-stream if needed\n35     currentSource = source;\n36     source.onended = () => {\n37       currentSource = null;\n38       isPlaying = false;\n39       playNextAudioChunk();\n40     };\n41   } catch (error) {\n42     console.error(\"Error during audio playback:\", error);\n43     isPlaying = false;\n44   }\n45 }\n46\n47 // Define a WebSocket message event handler to play audio output\n48 function handleWebSocketMessage(\n49   message: Hume.empathicVoice.SubscribeEvent\n50 ) {\n51   switch (message.type) {\n52     case 'audio_output':\n53       // Decode and queue audio for playback\n54       const audioBlob = convertBase64ToBlob(\n55         message.data, \n56         mimeType\n57       );\n58       audioQueue.push(audioBlob);\n59       // Attempt to play immediately if not already playing\n60       playNextAudioChunk();\n61       break;\n62   }\n63 }\n5\nInterrupt\nInterruptibility is a distinguishing feature of EVI. If you send an audio input through the WebSocket while receiving response messages for a previous audio input, the response to the previous audio input will stop. Additionally, the interface will send back a user_interruption message, and begin responding to the new audio input.\nTypeScript\n1 // Function for stopping the audio and clearing the queue\n2 function stopAudioPlayback(): void {\n3   // Stop any in-flight buffer source\n4   if (currentSource) {\n5     try {\n6       currentSource.stop();\n7     } catch {}\n8     currentSource.disconnect();\n9     currentSource = null;\n10   }\n11   // Clear the queue and reset state\n12   audioQueue.length = 0;\n13   isPlaying = false;\n14 }\n15\n16 // Update WebSocket message event handler to handle interruption\n17 function handleWebSocketMessage(\n18   message: Hume.empathicVoice.SubscribeEvent\n19 ) {\n20   switch (message.type) {\n21     case 'user_message':\n22       // Stop playback if user starts speaking\n23       stopAudioPlayback();\n24       break;\n25     case 'audio_output':\n26       // Decode and queue audio for playback\n27       const audioBlob = convertBase64ToBlob(\n28         message.data,\n29         mimeType\n30       );\n31       audioQueue.push(audioBlob);\n32       // Attempt to play immediately if not already playing\n33       playNextAudioChunk();\n34       break;\n35     case 'user_interruption':\n36       // Stop playback immediately when the user interrupts\n37       console.log(\"User interruption detected.\");\n38       stopAudioPlayback();\n39       break;\n40   }\n41 }\nWas this page helpful?\nYes\nNo\nPrevious\nEVI Python Quickstart\nA quickstart guide for implementing the Empathic Voice Interface (EVI) with Python.\nNext\nBuilt with"
            }
        },
        "Python": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/quickstart/python",
            "content": {
                "title": "EVI Python Quickstart | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nNext.js\nTypeScript\nPython\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nEmpathic Voice Interface (EVI)\nQuickstart\nEVI Python Quickstart\nCopy page\nA quickstart guide for implementing the Empathic Voice Interface (EVI) with Python.\nThis guide provides detailed instructions for integrating EVI into your Python projects using Hume’s Python SDK. It is divided into seven key components:\nEnvironment setup: Download package and system dependencies to run EVI.\nDependency imports: Import all necessary dependencies into your script.\nDefining a WebSocketHandler class: Create a class to manage the WebSocket connection.\nAuthentication: Use your API credentials to authenticate your EVI application.\nConnecting to EVI: Set up a secure WebSocket connection to interact with EVI.\nHandling audio: Capture audio data from an input device, and play audio produced by EVI.\nAsynchronous event loop: Initiate and manage an asynchronous event loop that handles simultaneous, real-time execution of message processing and audio playback.\nTo see a full implementation within a terminal application, visit our API examples repository on GitHub: evi-python-quickstart\nHume’s Python SDK supports EVI using Python versions 3.9, 3.10, and 3.11 on macOS and Linux platforms. The full specification be found on the Python SDK GitHub page.\n1\nEnvironment setup\nBefore starting the project, it is essential to set up the development environment.\nCreating a virtual environment (optional)\nSetting up a virtual environment is a best practice to isolate your project’s dependencies from your global Python installation, avoiding potential conflicts.\nYou can create a virtual environment using either Python’s built-in venv module or the conda environment manager. See instructions for both below:\nvenv\nconda\nCreate the virtual environment.\nNote that when you create a virtual environment using Python’s built-in venv tool, the virtual environment will use the same Python version as the global Python installation that you used to create it.\nCreating the virtual environment with venv\n$ python -m venv evi-env\nActivate the virtual environment using the appropriate command for your system platform.\nActivating the virtual environment with venv\n$ source evi-env/bin/activate\nThe code above demonstrates virtual environment activation on a POSIX platform with a bash/zsh shell. Visit the venv documentation to learn more about using venv on your platform.\nPackage dependenices\nThere are two package dependencies for using EVI:\nHume Python SDK (required)\nThe hume[microphone] package contains the Hume Python SDK. This guide employs EVI’s WebSocket and message handling infrastructure as well as various asynchronous programming and audio utilities.\nInstalling the Hume Python SDK package\n$ pip install \"hume[microphone]\"\nEnvironment variables (recommended)\nThe python-dotenv package contains the logic for using environment variables to store and load sensitive variables such as API credentials from a .env file.\nInstalling the environment variable package\n$ pip install python-dotenv\nIn sample code snippets below, the API key, Secret key, and an EVI configuration ID have been saved to environment variables.\nWhile not strictly required, using environment variables is considered best practice because it keeps sensitive information like API keys and configuration settings separate from your codebase. This not only enhances security but also makes your application more flexible and easier to manage across different environments.\nSystem dependencies\nFor audio playback and processing, additional system-level dependencies are required. Below are download instructions for each supported operating system:\nmacOS\nLinux\nTo ensure audio playback functionality, macOS users will need to install ffmpeg, a powerful multimedia framework that handles audio and video processing.\nA common way to install ffmpeg on macOS is by using a package manager such as Homebrew. To do so, follow these steps:\nInstall Homebrew onto your system according to the instructions on the Homebrew website.\nOnce Homebrew is installed, you can install ffmpeg with brew:\nInstalling ffmpeg with Homebrew\n$ brew install ffmpeg\nIf you prefer not to use Homebrew, you can download a pre-built ffmpeg binary from the ffmpeg website or use other package managers like MacPorts.\n2\nDependency imports\nThe following import statements are used in the example project to handle asynchronous operations, environment variables, audio processing, and communication with the Hume API:\nImport statements\nStatement explanations\nImports\n1 import asyncio\n2 import base64\n3 import datetime\n4 import os\n5 from dotenv import load_dotenv\n6 from hume.client import AsyncHumeClient\n7 from hume.empathic_voice.chat.socket_client import ChatConnectOptions, ChatWebsocketConnection\n8 from hume.empathic_voice.chat.types import SubscribeEvent\n9 from hume.empathic_voice.types import UserInput\n10 from hume.core.api_error import ApiError\n11 from hume import MicrophoneInterface, Stream\n3\nDefining a WebSocketHandler class\nNext, we define a WebSocketHandler class to encapsulate WebSocket functionality in one organized component. The handler allows us to implement application-specific behavior upon the socket opening, closing, receiving messages, and handling errors. It also manages the continuous audio stream from a microphone.\nBy using a class, you can maintain the WebSocket connection and audio stream state in one place, making it simpler to manage both real-time communication and audio processing.\nBelow are the key methods:\nMethod Description\n__init__() Initializes the handler, setting up placeholders for the WebSocket connection.\nset_socket(socket: ChatWebsocketConnection) Associates the WebSocket connection with the handler.\non_open() Called when the WebSocket connection is established, enabling any necessary initialization.\non_message(data: SubscribeEvent) Handles incoming messages from the WebSocket, processing different types of messages.\non_close() Invoked when the WebSocket connection is closed, allowing for cleanup operations.\non_error(error: Exception) Manages errors that occur during WebSocket communication, providing basic error logging.\nExample WebSocketHandler Structure\n4\nAuthentication\nIn order to establish an authenticated connection, we instantiate the Hume client with our API key and include our Secret key in the query parameters passed into the WebSocket connection.\nYou can obtain your API credentials by logging into the Hume Platform and visiting the API keys page.\nAuthenticating EVI\n1 async def main() -> None:\n2   # Retrieve any environment variables stored in the .env file\n3   load_dotenv()\n4\n5   # Retrieve the API key, Secret key, and EVI config id from the environment variables\n6   HUME_API_KEY = os.getenv(\"HUME_API_KEY\")\n7   HUME_SECRET_KEY = os.getenv(\"HUME_SECRET_KEY\")\n8   HUME_CONFIG_ID = os.getenv(\"HUME_CONFIG_ID\")\n9\n10   # Initialize the asynchronous client, authenticating with your API key\n11   client = AsyncHumeClient(api_key=HUME_API_KEY)\n12\n13   # Define options for the WebSocket connection, such as an EVI config id and a secret key for token authentication\n14   options = ChatConnectOptions(config_id=HUME_CONFIG_ID, secret_key=HUME_SECRET_KEY)\n15   \n16   # ...\n5\nConnecting to EVI\nWith the Hume client instantiated with our credentials, we can now establish an authenticated WebSocket connection with EVI and pass in our handlers.\nConnecting to EVI\n1 async def main() -> None:\n2   # ...\n3   # Define options for the WebSocket connection, such as an EVI config id and a secret key for token authentication\n4   options = ChatConnectOptions(config_id=HUME_CONFIG_ID, secret_key=HUME_SECRET_KEY)\n5\n6   # Instantiate the WebSocketHandler\n7   websocket_handler = WebSocketHandler()\n8\n9   # Open the WebSocket connection with the configuration options and the handler's functions\n10     async with client.empathic_voice.chat.connect_with_callbacks(\n11       options=options,\n12       on_open=websocket_handler.on_open,\n13       on_message=websocket_handler.on_message,\n14       on_close=websocket_handler.on_close,\n15       on_error=websocket_handler.on_error\n16     ) as socket:\n17     \n18       # Set the socket instance in the handler\n19       websocket_handler.set_socket(socket)\n20       # ...\n6\nHandling audio\nThe MicrophoneInterface class captures audio input from the user’s device and streams it over the WebSocket connection.\nAudio playback occurs when the WebSocketHandler receives audio data over the WebSocket connection in its asynchronous byte stream from an audio_output message.\nIn this example, byte_strs is a stream of audio data that the WebSocket connection populates.\nCapturing and sending audio to EVI\n1 async def main() -> None:\n2   # Open the WebSocket connection with the configuration options and the handler's functions\n3   async with client.empathic_voice.chat.connect_with_callbacks(...) as socket:\n4     # Set the socket instance in the handler\n5     websocket_handler.set_socket(socket)\n6\n7     # Create an asynchronous task to continuously detect and process input from the microphone, as well as play audio\n8     microphone_task = asyncio.create_task(\n9       MicrophoneInterface.start(\n10         socket,\n11         byte_stream=websocket_handler.byte_strs\n12       )\n13     )\n14     \n15     # Await the microphone task\n16     await microphone_task\nSpecifying a microphone device\nYou can specify your microphone device using the device parameter in the MicrophoneInterface object’s start method.\nTo view a list of available audio devices, run the following command:\nBelow is an example output:\nExample audio device list\n$    0 DELL U2720QM, Core Audio (0 in, 2 out)\n>    1 I, Phone 15 Pro Max Microphone, Core Audio (1 in, 0 out)\n> >  2 Studio Display Microphone, Core Audio (1 in, 0 out)\n>    3 Studio Display Speakers, Core Audio (0 in, 8 out)\n>    4 MacBook Pro Microphone, Core Audio (1 in, 0 out)\n> <  5 MacBook Pro Speakers, Core Audio (0 in, 2 out)\n>    6 Pro Tools Audio Bridge 16, Core Audio (16 in, 16 out)\n>    7 Pro Tools Audio Bridge 2-A, Core Audio (2 in, 2 out)\n>    8 Pro Tools Audio Bridge 2-B, Core Audio (2 in, 2 out)\n>    9 Pro Tools Audio Bridge 32, Core Audio (32 in, 32 out)\n>   10 Pro Tools Audio Bridge 64, Core Audio (64 in, 64 out)\n>   11 Pro Tools Audio Bridge 6, Core Audio (6 in, 6 out)\n>   12 Apowersoft Audio Device, Core Audio (2 in, 2 out)\n>   13 ZoomAudioDevice, Core Audio (2 in, 2 out)\nIf the MacBook Pro Microphone is the desired device, specify device 4 in the Microphone context. For example:\nPython\n1 # Specify device 4 in MicrophoneInterface\n2 MicrophoneInterface.start(\n3   socket,\n4   device=4,\n5   allow_user_interrupt=True,\n6   byte_stream=websocket_handler.byte_strs\n7 )\nFor troubleshooting faulty device detection - particularly with systems using ALSA, the Advanced Linux Sound Architecture, the device may also be directly specified using the sounddevice library:\nSetting default sounddevice library device\n1 # Directly import the sounddevice library\n2 import sounddevice as sd\n3\n4 # Set the default device prior to scheduling audio input task\n5 sd.default.device = 4\nAllowing interruption\nThe allow_interrupt parameter in the MicrophoneInterface class allows control over whether the user can send a message while the assistant is speaking:\nAllowing an interrupt\n1 # Specify allowing interruption\n2 MicrophoneInterface.start(\n3   socket,\n4   allow_user_interrupt=True,\n5   byte_stream=websocket_handler.byte_strs\n6 )\nallow_interrupt=True: Allows the user to send microphone input even when the assistant is speaking. This enables more fluid, overlapping conversation.\nallow_interrupt=False: Prevents the user from sending microphone input while the assistant is speaking, ensuring that the user does not interrupt the assistant. This is useful in scenarios where clear, uninterrupted communication is important.\n7\nAsynchronous event loop\nInitialize, execute, and manage the lifecycle of the asynchronous event loop, making sure that the main() coroutine and its runs effectively and that the application shuts down cleanly after the coroutine finishes executing.\nInitialize the async event loop in global scope\n1 asyncio.run(main())\nWas this page helpful?\nYes\nNo\nPrevious\nConfiguring EVI\nGuide to configuring the Empathic Voice Interface (EVI).\nNext\nBuilt with"
            }
        }
    },
    "Voices": {
        "Voices": {
            "url": "https://dev.hume.ai/docs/text-to-speech-tts/voices",
            "content": {
                "title": "Octave Voices | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nSpecify a voice, or don’t!\nDynamic voice generation\nSpecifying a voice\nSaving generated voices\nFetching and deleting voices\nText-to-speech (TTS)\nOctave Voices\nCopy page\nGuide to specifying and generating voices with Octave TTS.\nThe Octave TTS API enables you to generate expressive voices through simple descriptions or specify a voice from your saved voices or from Hume’s Voice Library. This guide explains how to specify voices for synthesis, generate new ones, and save your favorites for reuse.\nSpecify a voice, or don’t!\nChoose a voice from the voice library or let Octave dynamically generate a new one based on your text. Optionally, add a description to guide speech delivery. These descriptions help shape the voice’s characteristics, whether you’re using a saved voice or generating a new one.\nDynamic voice generation\nWhen a voice is not specified, the generated speech is delivered in a dynamically generated voice. The text and description fields determine the characteristics of the voice.\ncURL\nPython\nTypeScript\n1 curl \"https://api.hume.ai/v0/tts\" \\\n2   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n3   --json '{\n4     \"utterances\": [\n5       {\n6         \"text\": \"Welcome to my application!\"\n7       }\n8     ]\n9   }'\nSee our prompting guide for tips on designing a voice, and our Acting instructions guide for how to guide speech delivery.\nSpecifying a voice\nWhen specifying a voice for the generated speech to be delivered in, you can specify a custom voice that you have designed and saved, either through the Platform UI or through the API, or a voice from Hume’s Voice Library.\nThe provider field in your request will default to CUSTOM_VOICE. When specifying one of your saved voices you do not need to specify a provider. However, when specifying a voice from Hume’s Voice Library you must specify HUME_AI as the provider.\nIn the example below, we specify a custom voice for the first utterance and a voice from Hume’s Voice Library voice in the second:\ncURL\nPython\nTypeScript\n1 curl \"https://api.hume.ai/v0/tts\" \\\n2   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n3   --json '{\n4     \"utterances\": [\n5       {\n6         \"text\": \"Welcome to my application!\",\n7         \"voice\": {\n8           \"id\": \"6e138d63-e6a9-4360-b1b9-da0bb77e3a58\"\n9         }\n10       },\n11       {\n12         \"text\": \"Thank you!\",\n13         \"voice\": {\n14           \"id\": \"5bb7de05-c8fe-426a-8fcc-ba4fc4ce9f9c\",\n15           \"provider\": \"HUME_AI\"\n16         }\n17       }\n18     ]\n19   }'\nSaving generated voices\nWhen you generate a voice you would like to reuse, you can save it to your voice library. This requires the generation_id from your TTS API response. Call the /v0/tts/voices endpoint with this ID and a name. Once saved, you can use the voice in future requests by specifying either its name or id.\n1\nGenerate a voice and extract the generation_id\nSend a text-to-speech request without specifying a voice.\ncURL\nPython\nTypeScript\n1 curl \"https://api.hume.ai/v0/tts\" \\\n2   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n3   --json '{\n4     \"utterances\": [\n5       {\n6         \"text\": \"Welcome to my application!\"\n7       }\n8     ]\n9   }'\nThe response will include the generation_id, which you’ll use to save the voice.\nResponse\n1 {\n2   \"generations\": [\n3     {\n4       \"audio\": \"//PExAAspDoWXjDNQgk3HJJJZNbaEPZMmTJk7QIEC...\",\n5       \"duration\": 1.589667,\n6       \"encoding\": {\n7         \"format\": \"mp3\",\n8         \"sample_rate\": 48000\n9       },\n10       \"file_size\": 26496,\n11       \"generation_id\": \"41f7c154-fbb2-4372-8ecc-e6b7bf6ace01\",\n12       \"snippets\": [\n13         [\n14           {\n15             \"audio\": \"//PExAAspDoWXjDNQgk3HJJJZNbaEPZMmTJk7QIEC...\",\n16             \"generation_id\": \"41f7c154-fbb2-4372-8ecc-e6b7bf6ace01\",\n17             \"id\": \"37a108c4-5de7-4507-8a54-0521f5cb0383\",\n18             \"text\": \"Welcome to my application!\",\n19             \"utterance_index\": 0\n20           }\n21         ]\n22       ]\n23     }\n24   ],\n25   \"request_id\": \"7903e4a7-6642-491a-aa96-c6b359dd1042707439\"\n26 }\n2\nSave the voice for reuse\nUse the generation_id from the response to save the voice with a custom name.\nRequest\nPython\nTypeScript\n1 curl \"https://api.hume.ai/v0/tts/voices\" \\\n2   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n3   --json '{\n4       \"name\": \"NewVoice\",\n5       \"generation_id\": \"41f7c154-fbb2-4372-8ecc-e6b7bf6ace01\"\n6   }'\nThe response will provide the voice’s name and id for use in future TTS requests.\nResponse\n1 {\n2   \"name\": \"NewVoice\",\n3   \"id\": \"41f7c154-fbb2-4372-8ecc-e6b7bf6ace01\",\n4   \"provider\": \"CUSTOM_VOICE\"\n5 }\nFetching and deleting voices\nYou can either fetch a list of your saved voices, or a list of Hume’s Voice Library voices, with the list voices endpoint. Should you want to delete a voice, you can do so using the delete voices endpoint.\nWas this page helpful?\nYes\nNo\nPrevious\nOctave Prompting Guide\nA guide to effectively prompting Octave for voice creation and voice modulation.\nNext\nBuilt with"
            }
        }
    },
    "Prompting": {
        "Prompting": {
            "url": "https://dev.hume.ai/docs/text-to-speech-tts/prompting",
            "content": {
                "title": "Octave Prompting Guide | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nKey principles for effective prompting\nCreating effective voice descriptions\nDevelop detailed character profiles\nIncorporate emotional context\nExample voice prompts\nCrafting text for optimal delivery\nMatch text tone to delivery\nAlign text with voice descriptions\nUtilize natural punctuation\nTechnical considerations\nText normalization\nMultilingual support\nTesting and refinement\nVoice refinement process\nContent adaptation\nBest practices\nText-to-speech (TTS)\nOctave Prompting Guide\nCopy page\nA guide to effectively prompting Octave for voice creation and voice modulation.\nOctave is a breakthrough speech-language model that combines LLM intelligence with advanced text-to-speech capabilities. Unlike traditional TTS systems that simply convert text to audio, Octave understands context, meaning, and the intricate relationships between voice, performance, and content. This deep comprehension allows Octave to generate nuanced, context-aware speech that naturally adapts to what’s being said and how it should be delivered.\nKey principles for effective prompting\nThe effectiveness of Octave’s speech generation depends primarily on two factors:\nCharacter matching - the alignment between voice description and speaker identity.\nSemantic alignment - the relationship between voice style and content.\nFor example, a voice description of a “calm, reflective elderly woman reminiscing about her childhood” paired with the text “The sun dipped below the horizon, casting golden hues over the fields of my youth” creates strong semantic alignment. However, using the same voice description with text like “Let’s get ready to rumble!” would create a mismatch between the character’s nature and the content’s style.\nIn the TTS API, the text and description fields serve as the prompt for Octave. See our voices guide to understand how the description field functions differently with and without a specified voice.\nCreating effective voice descriptions\nDevelop detailed character profiles\nCreate comprehensive character descriptions to guide Octave’s voice generation. Include relevant details about:\nDemographics (age, gender, background)\nSpeaking style (pace, energy, formality)\nPersonality traits\nProfessional role or context\nEmotional disposition\nExample description:\n“A middle-aged university professor with a gentle but authoritative voice, speaking deliberately and clearly, with occasional moments of dry humor and genuine enthusiasm for the subject matter.”\nIncorporate emotional context\nOctave can interpret and express emotional states when they’re clearly conveyed in the voice description or text content. Consider including:\nEmotional state descriptions: “speaking with quiet determination”\nSituational context: “delivering news of a surprising victory”\nCharacter background: “a wise mentor sharing life lessons”\nExample voice prompts\nThese examples demonstrate different approaches to voice description:\nTitle Description\nBritish Romance Novel Narrator ”The speaker is a sophisticated British female narrator with a gentle, warm voice, recounting the ending of a classic romance novel.”\nFilm Narrator ”An American, deep middle-aged male film trailer narrator for a film about chickens.”\n70-year old Literary Voice ”A reflective 70 year old black woman with a calming tone, reminiscing about the profound impact of literature on her life, speaking slowly and poetically.”\nMeditation Guru ”A mindfulness instructor with a gentle, soothing voice that flows at a slow, measured pace with natural pauses.”\nElderly Scottish Gentleman ”An elderly Scottish gentleman with a thick brogue, expressing awe and admiration.”\nCalifornia Surfer ”The speaker is an excited Californian surfer dude, with a loud, stoked, and enthusiastic tone.”\nCrafting text for optimal delivery\nMatch text tone to delivery\nThe emotional and stylistic qualities you want in the generated speech should be reflected in your input text. Octave analyzes the text’s meaning to inform its delivery. For example:\nFor angry delivery: “I am absolutely furious about this situation! This needs to STOP immediately!”\nFor calm explanation: “Let me explain this carefully. The process works in three simple steps.”\nAlign text with voice descriptions\nWhen providing both text and a voice description, ensure they complement each other semantically and stylistically. This creates a cohesive output where the voice characteristics match the content’s requirements.\nExample alignment:\nVoice description: “A seasoned sports announcer with energetic, dynamic delivery”\nMatching text: “And there’s the pitch! A massive swing and… it’s going, going, GONE! An incredible moment here at Central Stadium!”\nUtilize natural punctuation\nStandard punctuation helps Octave understand your intended phrasing and emphasis:\nPeriods (.) for complete stops\nCommas (,) for natural pauses\nEm dashes (—) for dramatic breaks\nExclamation marks (!) for emphasis\nQuestion marks (?) for rising intonation\nAvoid special formatting characters or symbols, as they may interfere with natural speech generation. This includes: HTML tags, Markdown syntax, Emojis, special symbols (~ # %), and non-standard punctuation.\nTechnical considerations\nText normalization\nOctave performs best with normalized text that follows natural speech patterns. Consider these guidelines:\nSpell out numbers: “forty-two” instead of “42”\nWrite dates in full: “February twenty-third” instead of “2/23”\nExpress time naturally: “three thirty in the afternoon” instead of “15:30”\nSpell out abbreviations when meant to be spoken fully\nBreak up complex technical terms or codes into speakable segments\nNormalized text examples\n1 Original: \"Meeting at 9:30AM on 3/15/24 in rm101\"\n2 Normalized: \"Meeting at nine thirty AM on March fifteenth, twenty \n3 twenty-four in room one oh one\"\n4\n5 Original: \"Temperature outside is 72.5°F\"\n6 Normalized: \"Temperature outside is seventy-two point five degrees \n7 Fahrenheit\"\n8\n9 Original: \"Contact support@company.com or call 1-800-555-0123\"\n10 Normalized: \"Contact support at company dot com or call one eight \n11 hundred, five five five, zero one two three\"\nMultilingual support\nCurrently, Octave supports English and Spanish, with more languages planned for future releases. When working with multiple languages:\nProvide voice descriptions in the same language as the input text\nMaintain consistent language use within a single generation request\nFollow language-specific punctuation and formatting conventions\nTesting and refinement\nAs with any generative AI system, achieving optimal results with Octave often requires iteration and refinement. Here’s how to approach the testing and refinement process:\nVoice refinement process\nFollow these steps to refine your voice descriptions:\nStart with a basic voice description\nTest with various emotional states\nRefine based on performance\nValidate across different content types\nExample of voice description refinement\n1 Initial: \"A professional newsreader\"\n2 Refined: \"A veteran broadcast journalist with clear diction and \n3 measured pacing, naturally transitioning between serious and \n4 conversational tones while maintaining professional authority\"\nContent adaptation\nTest how your voice descriptions handle different types of content to ensure consistency and appropriate delivery:\nExample of content adaptation\n1 Voice: \"A wise mentor figure with gentle authority\"\n2\n3 Explanation: \"Let me show you how this works. First, we'll...\"\n4 Encouragement: \"You're getting it! See how much progress you've made?\"\n5 Correction: \"Ah, not quite. Let's take a step back and think about...\"\nBest practices\nTest variations of voice descriptions and text to find the most effective combinations for your use case\nKeep voice descriptions focused and coherent rather than trying to combine too many different characteristics\nUse natural language in both text and voice descriptions rather than technical or artificial-sounding constructs\nConsider the intended context and audience when crafting voice descriptions\nMaintain consistency in character voice across multiple utterances in the same context\nRemember that Octave’s intelligence comes from its understanding of context and meaning. The more clearly you can express your intended delivery through natural language, the better the model can generate appropriate speech output.\nWas this page helpful?\nYes\nNo\nPrevious\nOctave Acting Instructions Guide\nGuide to controlling voice expression in Octave TTS through acting instructions, speed settings, and silence parameters.\nNext\nBuilt with"
            }
        }
    },
    "Acting instructions": {
        "Acting instructions": {
            "url": "https://dev.hume.ai/docs/text-to-speech-tts/acting-instructions",
            "content": {
                "title": "Octave Acting Instructions Guide | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nProviding acting instructions\nBest practices\nExamples\nActing instructions vs. voice generation\nText-to-speech (TTS)\nOctave Acting Instructions Guide\nCopy page\nGuide to controlling voice expression in Octave TTS through acting instructions, speed settings, and silence parameters.\nOctave not only supports choosing or designing voices, but also offers sophisticated control over how speech is delivered. These controls, which we refer to as “acting instructions,” allow you to shape how Octave generates expressive speech from your text. Through acting instructions, you can specify emotional tone, vocal style, and delivery characteristics to achieve your desired speech output. This capability leverages Octave’s understanding of context and meaning to create naturally expressive speech that responds to nuanced direction, in some ways similar to how a voice actor would interpret a script.\nActing instructions enable you to specify aspects of speech delivery such as:\nEmotional tone: happiness, sadness, excitement, nervousness, etc.\nDelivery style: whispering, shouting, rushed speaking, measured pace, etc.\nSpeaking rate: the rate at which the speech is delivered, faster or slower.\nTrailing silence: injecting pauses in the speech for a specified duration in seconds.\nPerformance context: speaking to a crowd, intimate conversation, etc.\nSee our Prompting Guide for more detailed information and best practices for prompting Octave.\nIn the following section, we’ll explore the ways in which you can provide acting instructions to Octave through the API.\nProviding acting instructions\nThe TTS API offers parameters which allow you to control how an individual utterance is performed. These parameters can be used individually or combined for precise control over speech output:\ndescription: provide acting instructions in natural language.\nspeed: adjust the relative speaking rate on a non-linear scale from 0.25 (much slower) to 3.0 (much faster), where 1.0 represents normal speaking pace. Note that changes are not proportional to the value provided - for example, setting speed to 2.0 will make speech faster but not exactly twice as fast as the default.\ntrailing_silence: specify a duration of trailing silence (in seconds) to add to an utterance.\nIn this section we’ll leverage acting instructions to have Octave output speech for a session in guided meditation.\nBefore we apply acting instructions, let’s first take a look at a request that does not contain any acting instructions:\ncURL\nPython\nTypeScript\n1 curl \"https://api.hume.ai/v0/tts\" \\\n2   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n3   --json '{\n4     \"utterances\": [\n5       {\n6         \"text\": \"Let us begin by taking a deep breath in.\",\n7         \"voice\": {\n8           \"name\": \"Ava Song\",\n9           \"provider\": \"HUME_AI\"\n10         }\n11       },\n12       { \n13         \"text\": \"Now, slowly exhale.\"\n14       }\n15     ]\n16   }'\nWithout acting instructions, Octave will infer how to deliver the speech from the base voice’s description and the provided text input.\nIn the following steps, we’ll iteratively improve Octave’s delivery by specifying different types of acting instructions to better simulate guided meditation.\n1\nGuide delivery with natural language\nLet’s begin by providing a description to guide the delivery of these utterances to be calmer and more instructive:\ncURL\nPython\nTypeScript\n1 curl \"https://api.hume.ai/v0/tts\" \\\n2   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n3   --json '{\n4     \"utterances\": [\n5       {\n6         \"text\": \"Let us begin by taking a deep breath in.\",\n7         \"description\": \"calm, pedagogical\",\n8         \"voice\": {\n9           \"name\": \"Ava Song\",\n10           \"provider\": \"HUME_AI\"\n11         }\n12       },\n13       { \n14         \"text\": \"Now, slowly exhale.\",\n15         \"description\": \"calm, serene\"\n16       }\n17     ]\n18   }'\n2\nControl speed of delivery\nWhile the descriptions help to make the voice sound more appropriate for our use case, we now want to adjust the speed of delivery to be slower to create an atmosphere better suited for meditation:\ncURL\nPython\nTypeScript\n1 curl \"https://api.hume.ai/v0/tts\" \\\n2   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n3   --json '{\n4     \"utterances\": [\n5       {\n6         \"text\": \"Let us begin by taking a deep breath in.\",\n7         \"description\": \"calm, pedagogical\",\n8         \"voice\": {\n9           \"name\": \"Ava Song\",\n10           \"provider\": \"HUME_AI\"\n11         },\n12         \"speed\": \"0.65\"\n13       },\n14       { \n15         \"text\": \"Now, slowly exhale.\",\n16         \"description\": \"calm, serene\",\n17         \"speed\": \"0.65\"\n18       }\n19     ]\n20   }'\n3\nInjecting pauses\nFinally, in this guided meditation, it would be helpful to give the participants some time to actually take a breath! To achieve this we can introduce a pause between utterances by specifying a trailing silence duration for the first utterance.\nTo inject natural breaks within an utterance, try using [pause] or [long pause] in your text. Example: “Haha [pause] I didn’t realize this was going to be a formal event.”\ncURL\nPython\nTypeScript\n1 curl \"https://api.hume.ai/v0/tts\" \\\n2   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n3   --json '{\n4     \"utterances\": [\n5       {\n6         \"text\": \"Let us begin by taking a deep breath in.\",\n7         \"description\": \"calm, pedagogical\",\n8         \"voice\": {\n9           \"name\": \"Ava Song\",\n10           \"provider\": \"HUME_AI\"\n11         },\n12         \"speed\": 0.65,\n13         \"trailing_silence\": 4\n14       },\n15       { \n16         \"text\": \"Now, slowly exhale.\",\n17         \"description\": \"calm, serene\",\n18         \"speed\": 0.65\n19       }\n20     ]\n21   }'\nBy combining natural language descriptions, speed adjustments, and strategic pauses, you can achieve nuanced and effective speech delivery with Octave. This guided meditation example demonstrates how these parameters transform a basic utterance into a crafted experience with appropriate pacing and atmosphere. As you develop your own applications, consider how these controls can work together to create the ideal delivery for your specific use case.\nBest practices\nUse precise emotions: Instead of broad terms like “sad”, use specific emotions like “melancholy” or “frustrated”.\nCombine for nuance: Pair emotions with delivery styles, e.g., “excited but whispering” or “confident, professional tone”.\nIndicate pacing: Use terms like “rushed”, “measured”, “deliberate pause” to adjust speech rhythm.\nSpecify the audience: Instructions like “speaking to a child” or “addressing a large crowd” help shape delivery.\nKeep it concise: Short instructions like “sarcastic”, “angry”, “whispering”, “loudly” work best.\nUse speed for adjusting speech rate: Rather than using the description field to instruct slower or faster speech, leverage the speed parameter.\nExamples\nThe table below demonstrates how acting instructions can transform the same text into different delivery styles:\nText Input Acting Instruction Output Style\n”Are you serious?“ whispering, hushed Soft, secretive tone\n”We need to move, now!“ urgent, panicked Fast, tense delivery\n”Welcome, everyone.” warm, inviting Friendly, engaging tone\n”I can’t believe this…“ sarcastic Dry, exaggerated inflection\nActing instructions vs. voice generation\nWhen providing a description in your request, it’s important to understand how the field functions differently when specifying or not specifying a voice.\nVoice generation: when no voice is specified, Octave will generate an entirely new voice based on the provided text and description.\ncURL\nPython\nTypeScript\n1 curl \"https://api.hume.ai/v0/tts\" \\\n2   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n3   --json '{\n4     \"utterances\": [\n5       { \n6         \"text\": \"Today we'\\''ll explore ancient writing systems.\",\n7         \"description\": \"Eastern European scholar with a high quality voice, perfect for nonfiction and educational content.\"\n8       }\n9     ]\n10   }'\nActing instructions: the description field will be interpreted as acting instructions when you specify a voice. In the example below, Octave maintains the core characteristics of the selected voice, but modulates its performance according to your instructions.\ncURL\nPython\nTypeScript\n1 curl \"https://api.hume.ai/v0/tts\" \\\n2   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n3   --json '{\n4     \"utterances\": [\n5       { \n6         \"text\": \"Today we'\\''ll explore ancient writing systems.\",\n7         \"description\": \"excited, interested, projecting to a large classroom\",\n8         \"voice\": {\n9           \"name\": \"Literature Professor\",\n10           \"provider\": \"HUME_AI\"\n11         }\n12       }\n13     ]\n14   }'\nThis distinction is key to using Octave effectively. When you want consistent voice identity across multiple utterances but need to express different emotions or styles, acting instructions allow you to maintain the same voice while varying its delivery.\nWas this page helpful?\nYes\nNo\nPrevious\nOctave Continuation Guide\nGuide to maintaining coherent speech across multiple utterances and generations.\nNext\nBuilt with"
            }
        }
    },
    "Continuation": {
        "Continuation": {
            "url": "https://dev.hume.ai/docs/text-to-speech-tts/continuation",
            "content": {
                "title": "Octave Continuation Guide | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nWhat is continuation?\nKey aspects of continuation\nNarrative coherence\nLinguistic context\nConsistent voice\nText-to-speech (TTS)\nOctave Continuation Guide\nCopy page\nGuide to maintaining coherent speech across multiple utterances and generations.\nA key feature that sets Octave apart from traditional TTS systems is that the model understands what it’s saying. This linguistic comprehension powers Octave’s sophisticated continuation capabilities, allowing for coherent audio content across multiple segments. This guide explains how to use Octave’s continuation features to create natural-sounding audio content that spans multiple utterances while preserving flow and emotional continuity.\nWhat is continuation?\nWith Octave, continuation refers to maintaining contextual awareness between utterances to generate coherent, natural-sounding speech. An utterance is a unit of input that includes text to be synthesized and an optional description of how the speech should sound.\nContinuation allows you to use previous utterances as context to inform new speech generation. You can implement continuation in two ways:\nMultiple utterances in a single request: When you provide multiple utterances in one request, Octave automatically uses each utterance as context for the next one in the chain.\nUsing previous context: You can provide context as either:\nA context utterance\nA generation ID from previous output\nWhen using continuation, Octave will generate speech for all utterances in the utterances array of your request. However, any utterances provided within the context parameter serve only as reference and will not produce additional audio. This distinction allows you to build upon previous speech without duplicating audio output.\nThe primary use case for splitting your text input into multiple utterances, rather than providing all the text in a single utterance, is when you want to provide distinct acting instructions to different parts of your text input.\nKey aspects of continuation\nNarrative coherence\nWhen creating longer audio that exceeds a single utterance (such as audiobooks or educational materials), continuation ensures your audience experiences a cohesive narrative without awkward shifts in delivery, pacing, or emotional tone. The speech maintains appropriate energy levels and emotional progression, resulting in a more authentic listening experience where each new segment builds naturally from what came before.\nTry these examples to experience how Octave maintains narrative coherence, delivering the same phrase with completely different emotional tones based on the context of the preceding utterance:\nWith positive context (excited interpretation)\ncURL\nPython\nTypeScript\n$ curl \"https://api.hume.ai/v0/tts\" \\\n>   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n>   --json '{\n>     \"utterances\": [\n>       { \"text\": \"Our proposal has been accepted with full funding for the next three years!\" },\n>       { \"text\": \"I can'\\''t believe it!\" }\n>     ]\n>   }'\nWith negative context (disappointed interpretation)\ncURL\nPython\nTypeScript\n$ curl \"https://api.hume.ai/v0/tts\" \\\n>   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n>   --json '{\n>     \"utterances\": [\n>       { \"text\": \"After all our preparation... They'\\''ve decided to cancel the entire project...\" },\n>       { \"text\": \"I can'\\''t believe it!\" }\n>     ]\n>   }'\nLinguistic context\nContinuation also provides linguistic context for proper pronunciation, particularly with homographs—words that are spelled the same but pronounced differently based on meaning. For example, Octave can correctly differentiate between:\n“Take a bow.” (/bau/) vs. “Take a bow and arrow.” (/bō/)\n“Play the bass guitar.” (/bās/) vs. “Go bass fishing.” (/bas/)\n“I read the book yesterday.” (/red/) vs. “I will read the book tomorrow.” (/rēd/)\nTry these examples to see how Octave intelligently distinguishes between different pronunciations of the word “bow” based on contextual understanding:\nWith /bau/ pronunciation\ncURL\nPython\nTypeScript\n$ curl https://api.hume.ai/v0/tts \\\n>   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n>   --json '{\n>     \"utterances\": [\n>       { \"text\": \"What a fantastic performance!\" },\n>       { \"text\": \"Now take a bow.\" }\n>     ]\n>   }'\nWith /bō/ pronunciation\ncURL\nPython\nTypeScript\n$ curl https://api.hume.ai/v0/tts \\\n>   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n>   --json '{\n>     \"utterances\": [\n>       { \"text\": \"First take a quiver of arrows.\" },\n>       { \"text\": \"Now take a bow.\" }\n>     ]\n>   }'\nConsistent voice\nWhen continuing from an utterance, Octave intelligently handles voice consistency:\nIf you don’t specify a voice for a new utterance, Octave automatically continues using the same voice from the previous utterance.\nYou only need to specify a voice when you want to change from the currently established voice.\nThis applies to both generated voices (from descriptions) and saved voices from the voice library.\nBelow are sample requests which show how you can continue with the same voice:\nFor more information on specifying a voice in your request, see our voices guide.\nMultiple utterances in a single request\ncURL\nPython\nTypeScript\n$ curl \"https://api.hume.ai/v0/tts\" \\\n>   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n>   --json '{\n>     \"utterances\": [\n>       {\n>         \"text\": \"Gather around everyone! May I have your attention? Today we'\\''ll be learning about supermassive black holes at the center of galaxies.\",\n>         \"description\": \"projecting in a large museum auditorium, enthusiastic, joyful, ostentatious\",\n>         \"voice\": {\n>           \"name\": \"Donovan Sinclair\",\n>           \"provider\": \"HUME_AI\"\n>         },\n>         \"speed\": 1.3\n>       },\n>       {\n>         \"text\": \"I'\\''ve arranged for the museum guide to explain their special exhibit on black holes! I think you'\\''ll find it really helpful for the concepts we'\\''ve been covering in class!\",\n>         \"description\": \"pedagogical, enthusiastic, hinting\",\n>         \"speed\": 1.3\n>       }\n>     ]\n>   }'\nContinuing from previous generation using context\ncURL\nPython\nTypeScript\n$ # First request - capture the generation_id\n> GENERATION_ID=$(curl \"https://api.hume.ai/v0/tts\" \\\n>   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n>   --json '{\n>     \"utterances\": [\n>       {\n>         \"text\": \"Gather around everyone! May I have your attention? Today we'\\''ll be learning about supermassive black holes at the center of galaxies.\",\n>         \"description\": \"projecting in a large museum auditorium, enthusiastic, joyful, ostentatious\",\n>         \"voice\": {\n>           \"name\": \"Donovan Sinclair\",\n>           \"provider\": \"HUME_AI\"\n>         }\n>         \"speed\": 1.3\n>       }\n>     ]\n>   }' | jq -r '.generations[0].generation_id')\n>\n> # Second request using the generation_id from the first request\n> curl \"https://api.hume.ai/v0/tts\" \\\n>   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n>   --json '{\n>     \"utterances\": [\n>       {\n>         \"text\": \"I'\\''ve arranged for the museum guide to explain their special exhibit on black holes. I think you'\\''ll find it really helpful for the concepts we'\\''ve been covering in class.\",\n>         \"description\": \"pedagogical, enthusiastic, hinting\",\n>         \"speed\": 1.3\n>       },\n>     ],\n>     \"context\": {\n>       \"generation_id\": \"'$GENERATION_ID'\"\n>     }\n>   }'\nChanging voices mid-conversation\ncURL\nPython\nTypeScript\n$ curl \"https://api.hume.ai/v0/tts\" \\\n>   -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n>   --json '{\n>     \"utterances\": [\n>       {\n>         \"text\": \"Gather around everyone! May I have your attention? Today we'\\''ll be learning about supermassive black holes at the center of galaxies.\",\n>         \"description\": \"projecting in a large museum auditorium, enthusiastic, joyful, ostentatious\",\n>         \"voice\": {\n>           \"name\": \"Donovan Sinclair\",\n>           \"provider\": \"HUME_AI\"\n>         },\n>         \"speed\": 1.3\n>       },\n>       {\n>         \"text\": \"I'\\''ve arranged for the museum guide to explain their special exhibit on black holes. I think you'\\''ll find it really helpful for the concepts we'\\''ve been covering in class.\",\n>         \"description\": \"pedagogical, enthusiastic, hinting\",\n>         \"speed\": 1.3\n>       },\n>       {\n>         \"text\": \"Thank you, Professor! Hello, everyone! I'\\''m Vince from the astronomy department here at the museum. Welcome to our black hole visualization exhibit!\",\n>         \"description\": \"projecting in a large museum auditorium, professional, academic, welcoming, enthusiastic\",\n>         \"voice\": {\n>           \"name\": \"Vince Douglas\",\n>           \"provider\": \"HUME_AI\"\n>         }\n>       },\n>       {\n>         \"text\": \"It'\\''s quite fascinating how we can detect something we can'\\''t directly observe. Black holes don'\\''t emit light, but we can study their effects on nearby stars and gas.\",\n>         \"description\": \"expressing awe, enthusiastic, emphatic, passionate\"\n>       }\n>     ]\n>   }'\nThis intelligent handling of voice consistency saves development effort and ensures a seamless listening experience, making it easier to create dynamic, multi-character narratives without redundant voice specifications.\nWas this page helpful?\nYes\nNo\nPrevious\nHume MCP Server\nUse Hume AI’s Octave TTS with your favorite MCP clients like Claude Desktop, Cursor, and Windsurf.\nNext\nBuilt with"
            }
        }
    },
    "Hume MCP Server": {
        "Hume MCP Server": {
            "url": "https://dev.hume.ai/docs/text-to-speech-tts/mcp-server",
            "content": {
                "title": "Hume MCP Server | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nWhat for?\nAvailable Tools\nQuickstart\nPrerequisites\nSource Code\nPrompt Examples\nCommand Line Options\nEnvironment Variables\nDefault API Parameters\nRelated Resources\nText-to-speech (TTS)\nHume MCP Server\nCopy page\nUse Hume AI’s Octave TTS with your favorite MCP clients like Claude Desktop, Cursor, and Windsurf.\nThe Hume MCP Server implements the Model Context Protocol (MCP) for Hume AI’s Octave Text-To-Speech, allowing you to use MCP clients like Claude Desktop, Cursor, and Windsurf to collaborate with AI assistants on your voice projects.\nWhat for?\nIf you hope to narrate a large source text, such as a book, play, or long-form video, there’s a lot more to the project than just converting the text to speech. You have to\nDesign voices\nBreak the text into pieces\nAssign each line of dialogue to a voice\nSeparate acting instructions from spoken text\nLLMs can perform some of these tasks and help you keep these efforts organized. MCP is an industry protocol that lets you easily give an AI assistant the ability to use tools like Octave TTS on your behalf.\nAvailable Tools\nThe Hume MCP Server exposes the following tools to compatible MCP clients:\nTool Description\ntts\nSynthesize (and play) speech from text. This is the primary tool for generating speech with optional voice selection, acting instructions, and playback control.\nplay_previous_audio\nReplay previously generated audio by referencing its generation ID. Useful for comparing different versions or revisiting earlier speech samples.\nlist_voices\nList all available voices in your account’s library, including both custom voices and Hume-provided preset voices.\nsave_voice\nSave a generated voice to your library for reuse in future TTS requests, allowing you to build a collection of customized voices.\ndelete_voice\nRemove a voice from your custom voice library when it’s no longer needed.\nQuickstart\nTo get started with the Hume MCP Server, you’ll need to configure your MCP Client Application to use it:\nClaude Desktop\nCursor\nWindsurf\nAdd the following to the .mcpServers property in claude_desktop_config.json configuration file.\nClaude Desktop Configuration\n1 {\n2     \"mcpServers\": {\n3         \"hume\": {\n4             \"command\": \"npx\",\n5             \"args\": [\n6                 \"@humeai/mcp-server\"\n7             ],\n8             \"env\": {\n9                 \"HUME_API_KEY\": \"<your_hume_api_key>\"\n10             }\n11         }\n12     }\n13 }\nPrerequisites\nBefore you can use the Hume MCP Server, you’ll need:\nAn account and API Key from Hume AI\nNode.js installed on your system\n(optional) A command-line audio player\nffplay from FFMpeg is recommended, but the server will attempt to detect and use any of several common players\nThe MCP server calls Hume APIs on your behalf and will use credits from your account, incurring costs just as if you were making the API calls directly or doing Text-to-Speech through the web interface.\nSource Code\nThe Hume MCP Server is open source. You can view and contribute to the source code in the GitHub repository.\nPrompt Examples\nHere are some example prompts to help you get started with the Hume MCP Server.\nThese examples assume that the assistant has the ability to read and write from a filesystem. This usually already the case for MCP clients like Cursor that are attached to an editor. For standalone chat apps like Claude Desktop, you can give the assistant filesystem access through the Filesystem MCP Server.\nhttps://github.com/modelcontextprotocol/servers/tree/main/src/filesystem\nBasic Voice Generation\nReader Instructions\nAudiobook Narration Project\nVoice Variant Chaining\nCommand Line Options\nThe Hume MCP Server accepts several command line options to customize its behavior:\nOptions:\n  --workdir, -w <path>       Set working directory for audio files (default: system temp)\n  --(no-)embedded-audio-mode Enable/disable embedded audio mode (default: false)\n  --(no-)instant-mode        Enable/disable instant mode (default: false) (incurs 10% additional cost)\n  --help, -h                 Show help message\nEnvironment Variables\nYou can configure the behavior of the Hume MCP Server using these environment variables:\nVariable Description\nHUME_API_KEY\nYour Hume AI API key (required). You can obtain this from the Hume AI Platform.\nWORKDIR\nWorking directory for audio files (default: OS temp directory + “/hume-tts”). This is where generated audio files will be stored.\nEMBEDDED_AUDIO_MODE\nEnable/disable embedded audio mode (default: false, set to ‘true’ to enable).\nEmbedded audio files are a new addition to the MCP specification and most MCP client application do not yet support them. This can be useful if you are designing an MCP client specifically to work with Hume.\nINSTANT_MODE\nEnable/disable instant mode (default: false, set to ‘true’ to enable). Instant mode allows for faster TTS generation but incurs a 10% additional cost. This setting overrides the default instant_mode parameter sent to the TTS API.\nDefault API Parameters\nThe MCP Server applies several default parameters to API requests for convenience:\nTool Default Parameters Description\ntts strip_headers: true\nHeaders and non-speech text are automatically removed from the input.\ntts format.type: \"wav\"\nAll audio is generated in WAV format for best compatibility with audio players.\ntts instant_mode: true\nInstant mode is enabled by default for the TTS API (API default is false) for faster synthesis. This default can be overridden by setting the global instant mode option through the command line flag or environment variable.\nlist_voices page_size: 100\nReturns up to 100 voices per request (API default is 10) to minimize pagination needs.\nRelated Resources\nTTS Overview\nLearn more about Hume’s Octave TTS capabilities and features.\nPrompting Guide\nBest practices for prompting Octave for voice creation and voice modulation.\nActing Instructions\nGuide to controlling voice expression in Octave TTS.\nWas this page helpful?\nYes\nNo\nPrevious\nText-to-speech API FAQ\nWe’ve compiled a list of frequently asked questions from our developer community. If you don’t see your question here, join the discussion on our .\nNext\nBuilt with"
            }
        }
    },
    "FAQ": {
        "FAQ": {
            "url": "https://dev.hume.ai/docs/expression-measurement/faq",
            "content": {
                "title": "Expression Measurement API FAQ | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nExpression Measurement\nExpression Measurement API FAQ\nCopy page\nHow do I interpret my results?\nWhat can I do with my outputs?\nHow granular are the outputs of our speech prosody and language models?\nWhy am I seeing more face identifiers than the number of people in the video?\nWhy don't I see any vocal bursts in my file?\nWhy am I getting the \"Transcript confidence below threshold value\" error?\nWhich languages are supported?\nWhich programming languages and operating systems support the Expression Measurement API?\nWhen should I use Custom Models?\nWhat is Regression vs. Classification in Custom Model labeling and training?\nWhat are guidelines for building datasets for Custom Models?\nWas this page helpful?\nYes\nNo\nPrevious\nBilling\nNext\nBuilt with"
            }
        }
    },
    "Configuration": {
        "Build a configuration": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/configuration/build-a-configuration",
            "content": {
                "title": "Configuring EVI | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nBuild a configuration\nEVI version\nVoices\nSystem prompt\nLanguage model\nTools\nEvent messages\nTimeouts\nWebhooks\nSession settings\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nConfiguration options\nDefault configuration options\nCreating a configuration\nEmpathic Voice Interface (EVI)\nConfiguration\nConfiguring EVI\nCopy page\nGuide to configuring the Empathic Voice Interface (EVI).\nThe Empathic Voice Interface (EVI) is designed to be highly configurable, allowing developers to customize the interface to align with their specific requirements. Configuration of EVI can be managed through two primary methods: an EVI configuration and session settings.\nConfiguration options\nEVI configuration options affect the behavior and capabilities of the interface, and include the following configuration options:\nOption Description\nVoice Select a voice from a list of 8 preset options or create a custom voice. For further details, see our guide on creating custom voices.\nEVI version Select the version of EVI you would like to use. For details on similarities and differences between EVI versions 1 and 2, refer to our feature comparison.\nPrompt Provide a system prompt to guide how EVI should respond. For details on expressive prompt engineering, refer to our prompting guide.\nLanguage model Select a language model that best fits your application’s needs. For details on selecting a supplementary language model to meet your needs, such as optimizing for lowest latency, refer to our EVI FAQ. To incorporate your own language model, refer to our guide on using your own language model.\nTools Choose user-created or built-in tools for EVI to use during conversations. For details on creating tools and adding them to your configuration, see our tool use guide.\nEvent messages Configure messages that EVI will send in specific situations. For details on configuring event messages, see our API Reference.\nTimeouts Define limits on a chat with EVI to manage conversation flow. For details on specifying timeouts, see our API Reference.\nWebhooks Specify a webhook URL and subscribe to events, such as when a Chat session is started or ended. For details on subscribing to events, see our webhooks guide.\nConfigs, Prompts, and Tools are versioned to support iterative development—refine your setup over time and roll back to earlier versions whenever you need.\nDefault configuration options\nEVI is pre-configured with a set of default values, which are automatically applied if not specified in your configuration. The default configuration includes a preset voice, language model, and system prompt. EVI does not include any tools by default.\nDefaults vary slightly between EVI versions:\nEVI 1 EVI 2\nVoice ITO ITO\nLanguage model claude-3-5-sonnet-latest hume-evi-2\nSystem prompt Hume default Hume default\nTools None None\nConfiguration defaults may change over time. To keep your EVI setup stable, explicitly specify all options when defining your configuration. For reference, see the default system prompt in our Hume API Examples repository.\nCreating a configuration\nSee instructions below for creating an EVI configuration through the Platform.\n1\nNavigate to the Configurations page\nIn the Platform, find the EVI Configurations page. Click the Create Configuration button to begin.\n2\nSelect a template\nSelect a template to get started quickly, or create a configuration from scratch. This guide demonstrates creating a configuration from scratch.\n3\nChoose EVI version\nTo learn more about the differences between EVI versions 1 and 2, please see the feature comparison guide.\n4\nChoose voice\nSelect a voice from Hume’s 8 presets, or create your own custom voice. To learn more about voice customization options on the Hume Platform, please visit the Voices page. The system default voice will be used if no voice is selected.\n5\nSet up the LLM\nSelect a supported language model and specify a system prompt. The system prompt is crucial for defining your assistant’s personality, capabilities, and behavior. For guidance on writing effective prompts, visit our Prompting Guide. If no system prompt is provided, the system default prompt will be used.\n6\nAdd tools\nEVI comes with built-in tools (Web search and Hang up) that you can enable.\nTo add custom tools, click the + Add button, which allows you to either select from your existing custom tools or create a new one. For more information about tools and creating custom tools, visit the Tools page.\n7\nName config\nName your EVI configuration and add an optional description.\n8\nTest the configuration\nThe newly created configuration can now be tested. From the Config edit page, click Run in playground to test your configuration in the EVI Playground. This allows you to interact with EVI using your custom settings and verify that the configuration works as expected.\nSuccessful EVI config creation\nOnce in the EVI Playground, click Start call to begin testing your configuration. You can speak with EVI using your microphone or type messages in the chat interface.\nUsing an EVI configuration in the Playground\n9\nSet additional configuration options\nAdditional configuration options can be set after the initial config creation flow:\nEvent messages and timeouts can be configured through the Platform (either in the Playground or Config edit page)\nWebhooks can be configured through the API. They are not currently configurable through the Platform. For detailed instructions and code examples, see our webhooks guide.\nEvent message and timeout options in the Playground\nEvent message and timeout options in the Config edit page\n10\nApply the configuration\nAfter creating an EVI configuration, you can use it in your conversations with EVI by including the config_id in the query parameters of your connection request. Here’s how to locate your config_id:\nNavigate to the Configurations page.\nClick the More Options button next to your desired configuration.\nCopy the Configuration ID.\nSee the code examples below for how to apply your configuration:\nTypeScript\nPython\n1 import { Hume, HumeClient } from 'hume';\n2 // instantiate the HumeClient with credentials\n3 // avoid hard coding your API key, retrieve from environment variables\n4 const client = new HumeClient({\n5   apiKey: <YOUR_API_KEY>,\n6   secretKey: <YOUR_SECRET_KEY>,\n7 });\n8 // instantiate WebSocket connection with specified EVI config\n9 const socket = await client.empathicVoice.chat.connect({\n10   configId: <YOUR_CONFIG_ID> // specify config ID here\n11 });\nWas this page helpful?\nYes\nNo\nPrevious\nEVI Version\nDifferences between EVI versions.\nNext\nBuilt with"
            }
        },
        "EVI version": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/configuration/evi-version",
            "content": {
                "title": "EVI Version | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nBuild a configuration\nEVI version\nVoices\nSystem prompt\nLanguage model\nTools\nEvent messages\nTimeouts\nWebhooks\nSession settings\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nSpecifying an EVI version\nFeature comparison between versions\nEVI 2\nKey improvements\nFrequently asked questions\nEmpathic Voice Interface (EVI)\nConfiguration\nEVI Version\nCopy page\nDifferences between EVI versions.\nToday we support two versions of EVI. This guide details how to specify an EVI version in your configuration, as well as the differences between the two versions—from capabilities to pricing.\nSpecifying an EVI version\nIn the Platform UI you can simply select EVI 2 in the “Choose EVI version” step of the create configuration flow. If using the API to create your configuration, see our API reference for how to specify the EVI version in your configuration.\nFeature comparison between versions\nThis table provides a comprehensive comparison of features between EVI 1 and EVI 2, highlighting the new capabilities introduced in the latest version.\nFeature EVI 1 EVI 2\nVoice quality Similar to best TTS solutions Significantly improved naturalness, clarity, and expressiveness\nResponse latency ~900ms-2000ms ~500-800ms (about 2x faster)\nEmotional intelligence Empathic responses informed by expression measures End-to-end understanding of voice augmented with emotional intelligence training\nBase voices 3 core voice options (Kora, Dacher, Ito) 5 new high-quality base voice options with expressive personalities (8 total)\nVoice customizability Supported - can select base voices and adjust voice parameters Supported - extensive customization with parameter adjustments (e.g. pitch, huskiness, nasality)\nIn-conversation voice prompting Not supported Supported (e.g., “speak faster”, “sound more excited”, change accents)\nMultimodal processing Transcription augmented with high-dimensional voice measures Fully integrated voice and language processing within a single model, along with transcripts and expression measures\nSupplemental LLMs Supported Supported\nTool use and web search Supported Supported\nCustom language model (CLM) Supported Supported\nConfiguration options Extensive support Extensive support (same options as EVI 1)\nTypeScript SDK support Supported Supported\nPython SDK support Supported Supported\nMultilingual support English only Expanded support for multiple languages\nCost $0.102 per minute $0.0714 per minute (30% reduction)\nEVI 2\nThe Empathic Voice Interface 2 (EVI 2) introduces a new architecture that seamlessly integrates voice and language processing. This multimodal approach allows EVI 2 to understand and generate both language and voice, dramatically enhancing key features over EVI 1 while also enabling new capabilities.\nEVI 2 can converse rapidly and fluently with users, understand a user’s tone of voice, generate any tone of voice, and can even handle niche requests like rapping, changing its style, or speeding up its speech. The model specifically excels at emulating a wide range of personalities, including their accents and speaking styles. It is exceptional at maintaining personalities that are fun and interesting to interact with. Ultimately, EVI 2 is capable of emulating the ideal personality for every application and user.\nIn addition, EVI 2 allows developers to create custom voices by using a new voice modulation method. Developers can adjust EVI 2’s base voices along a number of continuous scales, including gender, nasality, and pitch. This first-of-its-kind feature enables creating voices that are unique to an application or even a single user. Further, this feature does not rely on voice cloning, which currently invokes more risks than any other capability of this technology.\nThe EVI 2 API is currently in beta. We are still making ongoing improvements to the model. In the coming weeks and months, EVI 2 will sound better, speak more languages, follow more complex instructions, and use a wider range of tools.\nKey improvements\nImproved voice quality\nEVI 2 uses an advanced voice generation model connected to our eLLM, which can process and generate both text and audio. This results in more natural-sounding speech with better word emphasis, higher expressiveness, and more consistent vocal output.\nFaster responses\nThe integrated architecture of EVI 2 reduces end-to-end latency by 40% vs EVI 1, now averaging around 500ms. This significant speed improvement enables more responsive and human-like conversations.\nEnhanced emotional intelligence\nBy processing voice and language in the same model, EVI 2 can better understand the emotional context of user inputs and generate more empathic responses, both in terms of content and vocal tone.\nCustom voices and personality\nEVI 2 offers new control over the AI’s voice characteristics. Developers can adjust various parameters to tailor EVI 2’s voice to their specific application needs. EVI 2 also supports in-conversation voice prompting, allowing users to dynamically modify EVI’s speaking style (e.g., “speak faster”, “sound excited”) during interactions.\nCost-effectiveness\nDespite its advanced capabilities, EVI 2 is 30% more cost-effective than its predecessor, with pricing reduced from $0.1020 to $0.0714 per minute.\nBeyond these improvements, EVI 2 also exhibits promising emerging capabilities including speech output in multiple languages. We will make these improvements available to developers as we scale up and improve the model.\nWe provide the same suite of tools to integrate and customize EVI 2 for your application as we do for EVI 1, and existing EVI developers can easily switch to the new system.\nFrequently asked questions\nCan I use my existing EVI 1 configs with EVI 2?\nIs EVI 2 ready for production deployment?\nHow does EVI 2's multimodal processing improve conversations?\nWhat safety measures has Hume implemented for EVI 2?\nAre there plans to support other languages?\nHow does EVI's speech recognition accuracy compare in various environments?\nWas this page helpful?\nYes\nNo\nPrevious\nVoices\nGuide to customizing the voice of the Empathic Voice Interface (EVI).\nNext\nBuilt with"
            }
        },
        "Voices": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/configuration/voices",
            "content": {
                "title": "Voices | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nBuild a configuration\nEVI version\nVoices\nSystem prompt\nLanguage model\nTools\nEvent messages\nTimeouts\nWebhooks\nSession settings\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nVoice attributes\nCrafting custom voices\nEmpathic Voice Interface (EVI)\nConfiguration\nVoices\nCopy page\nGuide to customizing the voice of the Empathic Voice Interface (EVI).\nThe Empathic Voice Interface (EVI) can be configured with any of our 8 base voices. You can also customize these voices by adjusting specific attributes. This guide explains each attribute and provides a tutorial for creating a custom voice. Visit the Playground to test the base voices.\nThe custom voices feature is experimental. Regular updates will focus on improving stability and expanding attribute options.\nVoice attributes\nThe following attributes can be modified to personalize any of the base voices:\nAttribute Description\nMasculine/Feminine The perceived tonality of the voice, reflecting characteristics typically associated with masculinity and femininity.\nAssertiveness The perceived firmness of the voice, ranging between whiny and bold.\nBuoyancy The perceived density of the voice, ranging between deflated and buoyant.\nConfidence The perceived assuredness of the voice, ranging between shy and confident.\nEnthusiasm The perceived excitement within the voice, ranging between calm and enthusiastic.\nNasality The perceived openness of the voice, ranging between clear and nasal.\nRelaxedness The perceived stress within the voice, ranging between tense and relaxed.\nSmoothness The perceived texture of the voice, ranging between smooth and staccato.\nTepidity The perceived liveliness behind the voice, ranging between tepid and vigorous.\nTightness The perceived containment of the voice, ranging between tight and breathy.\nEach voice attribute can be adjusted relative to the base voice’s characteristics. Values range from -100 to 100, with 0 as the default. Setting all attributes to their default values will keep the base voice unchanged.\nCrafting custom voices\nSee instructions below for customizing a voice through the Platform.\n1\nNavigate to the Voices page\nIn the Platform, find the EVI Voices page. Click the Create voice button to begin.\n2\nCreate a new custom voice\nEnter a name for your custom voice and select a base voice. Then, adjust the attributes.\n3\nTest your custom voice\nAs you make tweaks to the attributes, sample audio can be generated by clicking the ”▶” button in the Voice sample section at the bottom of the form.\n4\nUse your custom voice\nThe newly created voice can now be deployed. From the Voices page, click Use to create an EVI configuration with it.\nSuccessful voice creation\nWhen creating an EVI configuration, choose Custom voice and press the + Select button. Then, press Select existing custom voice… and confirm the custom voice you would like to use.\nSelecting a custom voice in an EVI configuration\nWas this page helpful?\nYes\nNo\nPrevious\nSystem Prompt\nDefine EVI’s behavior, responses, and style.\nNext\nBuilt with"
            }
        },
        "System prompt": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/configuration/system-prompt",
            "content": {
                "title": "System Prompt | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nBuild a configuration\nEVI version\nVoices\nSystem prompt\nLanguage model\nTools\nEvent messages\nTimeouts\nWebhooks\nSession settings\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nDefault system prompt\nBuilding your system prompt\nSpecifying a system prompt in your configuration\nEmpathic Voice Interface (EVI)\nConfiguration\nSystem Prompt\nCopy page\nDefine EVI’s behavior, responses, and style.\nA system prompt in the context of EVI is a set of instructions that shape the AI’s behavior, responses, and overall style during a chat session. It defines the specific goal or role for EVI—for example, acting as a customer support representative, a fitness coach, or a travel advisor—ensuring that the conversation aligns with your application’s intended focus.\nDefault system prompt\nIf you do not specify a system prompt in your configuration, EVI will automatically use a default system prompt. You can review our default system prompt for reference.\nBuilding your system prompt\nPrompts are managed as resources within our API and are version controlled. This enables you to:\nDevelop and refine system prompts over time.\nMaintain consistent behavior across chat sessions.\nReuse and reference existing prompts in different configurations.\nFor tips and best practices on creating effective system prompts, please refer to our Prompting Guide.\nWhile the Platform UI is the recommended interface for building your EVI configurations, should you require a programmatic interface, prompts can also be created programmatically through our API. See our API Reference for more details.\nSpecifying a system prompt in your configuration\nWhen creating a configuration, you have two options for defining the system prompt:\nReference an existing prompt: Specify the ID of a pre-existing, version-controlled Prompt. EVI will use the referenced prompt in the chat session.\nSpecifying an existing prompt\n$ curl -X POST https://api.hume.ai/v0/evi/configs \\\n>   -H \"X-Hume-Api-Key: <apiKey>\" \\\n>   -H \"Content-Type: application/json\" \\\n>   -d '{\n>     \"evi_version\": \"2\",\n>     \"name\": \"Sample Config\",\n>     \"prompt\": {\n>       \"id\": \"af699d45-2985-42cc-91b9-af9e5da3bac5\",\n>       \"version\": 0\n>     }\n>   }'\nDefine a new Prompt in the request: Provide the prompt text directly in your configuration request. In this case, a new prompt will be automatically created and associated with the configuration.\nSpecifying an existing prompt\n$ curl -X POST https://api.hume.ai/v0/evi/configs \\\n>   -H \"X-Hume-Api-Key: <apiKey>\" \\\n>   -H \"Content-Type: application/json\" \\\n>   -d '{\n>     \"evi_version\": \"2\",\n>     \"name\": \"Sample Config\",\n>     \"prompt\": {\n>       \"text\": \"Sample system prompt\"\n>     }\n>   }'\nSee our API reference for details on how to specify system prompts in your EVI configuration.\nWas this page helpful?\nYes\nNo\nPrevious\nLanguage Model\nChoose which supplemental language model to use for EVI’s response generation.\nNext\nBuilt with"
            }
        },
        "Language model": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/configuration/language-model",
            "content": {
                "title": "Language Model | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nBuild a configuration\nEVI version\nVoices\nSystem prompt\nLanguage model\nTools\nEvent messages\nTimeouts\nWebhooks\nSession settings\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nSupported language models\nHume’s eLLM\nExternal LLMs\nCustom language model\nEmpathic Voice Interface (EVI)\nConfiguration\nLanguage Model\nCopy page\nChoose which supplemental language model to use for EVI’s response generation.\nEVI supports specifying a supplemental language model to power response text generation during chat sessions. This configuration option allows you to tailor conversational output by selecting one of the supported language models.\nSee our API reference for how to specify a language model in your EVI configuration.\nSupported language models\nHume’s eLLM\nOur proprietary empathic large language model, hume-evi-2, is a multimodal system that processes both language and expression measures. It generates natural language responses and guides text-to-speech prosody, delivering emotionally nuanced output. Thanks to its independent design, it produces an initial response faster than many external LLMs while EVI 2’s integrated voice-language architecture ensures coherent and contextually aware interactions with control over personality and speaking style.\nExternal LLMs\nDevelopers may also choose from leading external language models such as Anthropic’s Claude 3.5 Sonnet, and many others. For a complete list of external LLMs Hume natively supports, see our API Reference.\nCustom language model\nFor specific application requirements, the API supports integrating custom language models, offering flexibility to tailor conversational behavior to your domain. For more details on leveraging a custom language model, see our custom language model guide\nThe cost of external supplemental LLMs is not included in EVI’s pricing.\nHume covers the costs of LLMs while we make optimizations that will make language generation much cheaper for our customers. This means that the LLM expenses are not included in EVI’s pricing, ensuring a single consistent price per minute regardless which supplemental LLM you choose. Developers can select any supported LLM without additional charges, making it easy to switch between different models based on your needs.\nWas this page helpful?\nYes\nNo\nPrevious\nTools\nEquip EVI with Tools to enable function calling during Chats.\nNext\nBuilt with"
            }
        },
        "Tools": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/configuration/tools",
            "content": {
                "title": "Tools | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nBuild a configuration\nEVI version\nVoices\nSystem prompt\nLanguage model\nTools\nEvent messages\nTimeouts\nWebhooks\nSession settings\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nTool types\nCreating tools\nImplementing tools\nEmpathic Voice Interface (EVI)\nConfiguration\nTools\nCopy page\nEquip EVI with Tools to enable function calling during Chats.\nEVI empowers your application by enabling the use of external functions—referred to as Tools—to enhance conversations. Tools allow EVI to trigger custom logic (for example, searching the web, fetching weather data, or updating a database) based on user input.\nTool use is only supported when specifying certain supplemental LLMs within your configuration. Currently, our function calling feature supports Anthropic and OpenAI models.\nFor best results, we suggest choosing a fast and intelligent LLM that performs well on function calling benchmarks. On account of its speed and intelligence, we recommend Claude 3.5 Haiku as the supplemental LLM in your EVI configuration when using Tools. Function calling is not available if you are using your own custom language model. We plan to support more function calling LLMs in the future.\nSee our API reference for details on how to configure Tools and Built-in Tools in your EVI configuration.\nTool types\nFunction Tools: These are custom functions you create and manage via our API. With Function Tools, you are responsible for defining the function, specifying its input parameters (using a JSON schema), and invoking it when EVI signals that a function call is needed.\nBuilt-in Tools: These Tools are natively integrated into EVI and do not require you to define or invoke any function logic. Hume provides built-in Tools such as:\nweb_search: Searches the web for real-time information when needed.\nhang_up: Closes the WebSocket connection with a status code of 1000 (normal closure), typically triggered when the conversation has ended.\nCreating tools\nWhile we recommend using our Platform UI for creating Tools, you can also create and manage Function Tools programmatically through our API. For complete details on creating a Tool via the API, see the API reference.\nImplementing tools\nFor a step-by-step walkthrough on creating Tools and understanding how both Function Tools and Built-in Tools are implemented within an EVI Chat session, refer to our Tool Use Guide.\nWas this page helpful?\nYes\nNo\nPrevious\nEvent Messages\nSchedule and author EVI responses for key Chat events.\nNext\nBuilt with"
            }
        },
        "Event messages": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/configuration/event-messages",
            "content": {
                "title": "Event Messages | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nBuild a configuration\nEVI version\nVoices\nSystem prompt\nLanguage model\nTools\nEvent messages\nTimeouts\nWebhooks\nSession settings\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nSupported events\nEmpathic Voice Interface (EVI)\nConfiguration\nEvent Messages\nCopy page\nSchedule and author EVI responses for key Chat events.\nEvent messages are sent by the server when specific events occur during a chat session. These messages are used to configure behaviors for EVI, such as controlling how EVI starts a new conversation.\nWhen enabling events you can optionally specify text which EVI will speak verbatim. If no text is provided, then EVI will infer what to say based on the Chat context.\nSee our API reference for how to specify event messages in your EVI configuration.\nSupported events\nOn new chat: Specifies the initial message EVI provides when a new chat is started, such as a greeting or welcome message.\nOn max duration timeout: Specifies the message EVI provides when the chat is disconnected due to reaching the maximum chat duration, such as a message mentioning the time limit for the chat has been reached.\nOn inactivity timeout: Specifies the message EVI provides when the chat is about to be disconnected due to a user inactivity timeout, such as a message mentioning a lack of user input for a period of time.\nEnabling an inactivity message allows developers to use this message event for “checking in” with the user if they are not responding to see if they are still active.\nIf the user does not respond in the number of seconds specified in the inactivity_timeout field, then EVI will say the message and will wait an additional 15 seconds before the Chat is ended. If the user responds before this grace period ends, the conversation will continue as normal.\nIf the inactivity message is not enabled, then reaching the inactivity timeout will immediately end the connection.\nWas this page helpful?\nYes\nNo\nPrevious\nTimeouts\nDefine and manage chat session duration limits.\nNext\nBuilt with"
            }
        },
        "Timeouts": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/configuration/timeouts",
            "content": {
                "title": "Timeouts | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nBuild a configuration\nEVI version\nVoices\nSystem prompt\nLanguage model\nTools\nEvent messages\nTimeouts\nWebhooks\nSession settings\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nTimeout options\nEmpathic Voice Interface (EVI)\nConfiguration\nTimeouts\nCopy page\nDefine and manage chat session duration limits.\nTimeout configurations allow you to control the lifespan of an EVI chat session by specifying time limits for user inactivity and overall session duration. These settings help manage when a chat session should end based on your integration requirements and work in concert with event messages.\nWhen timeouts are triggered, the server sends corresponding messages to inform clients about the upcoming disconnection or session termination. These configurations ensure that sessions remain active only as long as needed.\nThese settings can be combined with event messages to provide users with notifications prior to disconnection. For example, an inactivity timeout can trigger a message to “check in” with the user before ending the session.\nSee our API reference for details on how to configure timeouts in your EVI configuration.\nTimeout options\nInactivity Timeout: Specifies the duration (in seconds) of user inactivity after which the EVI WebSocket connection will be automatically disconnected.\nDefault: 600 seconds (10 minutes)\nAllowed Range: 30 to 1,800 seconds\nMax Duration Timeout: Specifies the maximum allowed duration (in seconds) for an EVI WebSocket connection before it is automatically disconnected.\nDefault: 1,800 seconds (30 minutes)\nAllowed Range: 30 to 1,800 seconds\nWas this page helpful?\nYes\nNo\nPrevious\nWebhooks\nGuide to EVI’s webhooks configuration option.\nNext\nBuilt with"
            }
        },
        "Webhooks": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/configuration/webhooks",
            "content": {
                "title": "Webhooks | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nBuild a configuration\nEVI version\nVoices\nSystem prompt\nLanguage model\nTools\nEvent messages\nTimeouts\nWebhooks\nSession settings\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nSupported events\nSubscribing to events\nHandling events\nSecurity\nVerifying Authenticity\nPreventing Replay Attacks\nEmpathic Voice Interface (EVI)\nConfiguration\nWebhooks\nCopy page\nGuide to EVI’s webhooks configuration option.\nEVI webhooks send structured payloads to your specified URL in real time, allowing your application to respond to key events during EVI Chat sessions. They enable you to connect EVI with your systems to monitor events, automate workflows, and gain valuable insights into user interactions.\nFor more information on EVI Chat sessions, check out our Chat history guide.\nSupported events\nThe following section details each supported event, including what triggers the event, the structure of its payload, and practical use cases to help you integrate it into your workflows.\nchat_started\nchat_ended\nSubscribing to events\nTo receive event notifications, define your webhook URL and specify the events you want to subscribe to within your EVI Config. The example below demonstrates how to configure a webhook URL for the chat_started and chat_ended events:\ncURL\nTypeScript\nPython\n$ curl -X POST https://api.hume.ai/v0/evi/configs \\\n>   -H \"X-Hume-Api-Key: <YOUR_API_KEY>\" \\\n>   -H \"Content-Type: application/json\" \\\n>   -d '{\n>     \"evi_version\": \"2\",\n>     \"name\": \"Sample Webhook Config\",\n>     \"webhooks\": [{\n>       \"url\": <YOUR_WEBHOOK_URL>,\n>       \"events\": [\"chat_started\", \"chat_ended\"]\n>     }]\n>   }'\nHandling events\nWhen EVI sends event payloads to your webhook URL, your application can process them by implementing a handler. Below are simplified example implementations in TypeScript and Python for handling chat_started and chat_ended events.\nFor complete implementations, check out our TypeScript Example Project and Python Example Project.\nTypeScript\nPython\n1 import { WebhookEvent } from \"hume/serialization/resources/empathicVoice/types/WebhookEvent\";\n2\n3 // Route to handle webhook events\n4 app.post(\"/hume-webhook\", (req: Request, res: Response) => {\n5   // Validate and parse using WebhookEvent\n6   const event = WebhookEvent.parseOrThrow(JSON.parse(req.body));\n7\n8   try {\n9     // Handle the specific event type\n10     switch (event.eventName) {\n11       case 'chat_started':\n12         console.info('Processing chat_started event:', event);\n13         // Add additional chat_started processing logic here\n14         break;\n15       \n16       case 'chat_ended':\n17         console.info(\"Processing chat_ended event:\", event);\n18         // Add additional chat_ended processing logic here\n19         break;\n20\n21       default:\n22         console.warn(`[Event Handling] Unsupported event type: '${event.eventName}'`);\n23         res.status(400).json({ error: `Unsupported event type: '${event.eventName}'` });\n24         return;\n25     }\n26\n27     res.json({ status: \"success\", message: `${event.event_name} processed` });\n28   } catch (error) {\n29     console.error(\"Error processing event:\", error);\n30     res.status(500).json({ error: \"Internal server error\" });\n31   }\n32 });\nSecurity\nTo ensure the authenticity and integrity of webhook payloads, EVI includes an HMAC signature and a timestamp in each request. Implementing verification safeguards your application from tampering and replay attacks.\nVerifying Authenticity\nEach webhook request contains the following headers:\nX-Hume-AI-Webhook-Signature: HMAC-SHA256 signature of the payload and timestamp, signed with your Webhook Secret.\nX-Hume-AI-Webhook-Timestamp: Unix timestamp indicating when the request was sent.\nTo verify authenticity:\nRetrieve the X-Hume-AI-Webhook-Signature and X-Hume-AI-Webhook-Timestamp headers.\nConcatenate the payload and timestamp, then compute the HMAC-SHA256 hash using your Webhook Secret.\nCompare the computed hash with the provided signature using a timing-safe comparison.\nTypeScript\nPython\n1 import * as crypto from 'crypto';\n2\n3 export function validateHmacSignature(payload: string, headers: IncomingHttpHeaders): void {\n4   // Retrieve the timestamp and signature from headers\n5   const timestamp = headers['x-hume-ai-webhook-timestamp'];\n6   if (!timestamp) {\n7     console.error('Error: Missing timestamp in the request headers.');\n8     throw new Error('Missing timestamp header');\n9   }\n10\n11   const signature = headers['x-hume-ai-webhook-signature'] as string;\n12   if (!signature) {\n13     console.error('Error: Missing signature in the request headers.');\n14     throw new Error('Missing signature header');\n15   }\n16\n17   // 2. Retrieve the API key from environment variables\n18   const apiKey = process.env.HUME_API_KEY;\n19   if (!apiKey) {\n20     console.error('Error: HUME_API_KEY is not set in environment variables.');\n21     throw new Error('Missing API key');\n22   }\n23\n24   // 3. Construct the message to be hashed by concatenating the payload and the timestamp\n25   const message = `${payload}.${timestamp}`;\n26   const expectedSig = crypto\n27     .createHmac('sha256', apiKey)\n28     .update(message)\n29     .digest('hex');\n30\n31   // 4. Compare the provided signature with the expected one using timing-safe comparison\n32   const signatureBuffer = Buffer.from(signature, 'utf8');\n33   const expectedSigBuffer = Buffer.from(expectedSig, 'utf8');\n34   const validSignature =\n35     signatureBuffer.length === expectedSigBuffer.length &&\n36     crypto.timingSafeEqual(signatureBuffer, expectedSigBuffer);\n37\n38   // 5. If the signatures do not match, throw an error\n39   if (!validSignature) {\n40     console.error(`Error: Invalid HMAC signature. Expected: ${expectedSig}, Received: ${signature}`);\n41     throw new Error('Invalid HMAC signature');\n42   }\n43\n44   console.info('HMAC validation successful!');\n45 }\nPreventing Replay Attacks\nValidate the X-Hume-AI-Webhook-Timestamp header to ensure the request is recent:\nCheck if the timestamp is within a predefined range (e.g., 3 minutes from the current time).\nReject requests with timestamps outside this range.\nTypeScript\nPython\n1 export function validateTimestamp(headers: IncomingHttpHeaders): void {\n2   // 1. Retrieve the timestamp from the headers\n3   const timestamp = headers['x-hume-ai-webhook-timestamp'] as string;\n4   if (!timestamp) {\n5     console.error('Error: Missing timestamp.');\n6     throw new Error('Missing timestamp');\n7   }\n8\n9   // 2. Attempt to parse the timestamp to a number\n10   let timestampInt: number;\n11   try {\n12     timestampInt = parseInt(timestamp, 10);\n13     if (isNaN(timestampInt)) {\n14       // parseInt can return NaN if the string isn't a valid integer\n15       throw new Error();\n16     }\n17   } catch (err) {\n18     console.error(`Error: Invalid timestamp format: ${timestamp}`);\n19     throw new Error('Invalid timestamp format');\n20   }\n21\n22   // 3. Get the current time in seconds\n23   const currentTime = Math.floor(Date.now() / 1000);\n24\n25   // 4. Check if the timestamp is more than 180 seconds behind the current time\n26   const TIMESTAMP_VALIDATION_WINDOW = 180;\n27   if (currentTime - timestampInt > TIMESTAMP_VALIDATION_WINDOW) {\n28     console.error(`Error: The timestamp on the request is too old. Current time: ${currentTime}, Timestamp: ${timestamp}`);\n29     throw new Error('The timestamp on the request is too old');\n30   }\n31\n32   console.info('Timestamp validation successful!');\n33 }\nWas this page helpful?\nYes\nNo\nPrevious\nSession Settings\nGuide to applying EVI configuration options in real time with Session Settings.\nNext\nBuilt with"
            }
        },
        "Session settings": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/configuration/session-settings",
            "content": {
                "title": "Session Settings | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nBuild a configuration\nEVI version\nVoices\nSystem prompt\nLanguage model\nTools\nEvent messages\nTimeouts\nWebhooks\nSession settings\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nSession settings options\nSystem prompt\nContext\nAudio\nTools\nBuilt-in tools\nVariables\nLanguage model API key\nCustom session ID\nEmpathic Voice Interface (EVI)\nConfiguration\nSession Settings\nCopy page\nGuide to applying EVI configuration options in real time with Session Settings.\nEVI Configs are persistent and version-controlled. In contrast, Session Settings are temporary and apply only to the current session. These options can be leveraged dynamically based on the requirements of each session to ensure optimal performance and user experience. You can apply session settings by sending a session_settings message during your Chat session.\nSession settings options\nSystem prompt\nInstructions used to shape EVI’s behavior, responses, and style for the session. When included in a Session Settings message, the provided Prompt overrides the existing one specified in the EVI configuration. If no Prompt was defined in the configuration, this Prompt will be the one used for the session.\nYou can use the Prompt to define a specific goal or role for EVI, specifying how it should act or what it should focus on during the conversation. For example, EVI can be instructed to act as a customer support representative, a fitness coach, or a travel advisor, each with its own set of behaviors and response styles.\nFor help writing a system prompt, see our Prompting Guide.\nContext\nThe context session setting enables you to inject additional context into the conversation, which is appended to the end of user messages for the session.\nWhen included in a Session Settings message, the provided context can be used to remind the LLM of its role in every user message, prevent it from forgetting important details, or add new relevant information to the conversation.\nFor more details, see our guide on context injection.\nAudio\nThe audio session settings allows you to configure details for the audio input used during the session. Ensures the audio is being correctly set up for processing.\nThis setting is only required when the audio input is encoded in PCM Linear 16 (16-bit, little-endian, signed PCM WAV data). For detailed instructions on how to configure session settings for PCM Linear 16 audio, please refer to the audio guide.\nTools\nTools are resources used by EVI to perform various tasks, such as calling external APIs. While tools can be specified in your EVI configuration, this session setting enables you to equip EVI with tools during the Chat session in real-time.\nThis setting supports use cases where you dynamically want to equip EVI with your tool(s) based on some condition within your application logic.\nFor more on tool use, see our Tool Use Guide.\nBuilt-in tools\nBuilt-in tools, like web search, are natively integrated. This means that, unlike (user-defined) Tools, the associated functions do not need to be defined or invoked by the user.\nThis session setting enables you to conditionally equip EVI with a built-in tool during a Chat session.\nFor further details, see our Tool Use Guide.\nVariables\nThis session settings allows you to assign values to dynamic variables referenced in your system prompt.\nUsing this field, you can personalize responses based on session-specific details. For more guidance, see our guide on using dynamic variables.\nLanguage model API key\nThird party API key for the supplemental language model.\nWhen provided, EVI will use this key instead of Hume’s API key for the supplemental LLM. This allows you to bypass rate limits and utilize your own API key as needed.\nUsing your own API key does not affect pricing for EVI usage. See our FAQ for more details.\nCustom session ID\nA custom session ID a user-defined, unique identifier for the session. This ID is used to manage conversational state, correlate frontend and backend data, and persist conversations across EVI sessions.\nIf included, the response sent from Hume to your backend will include this ID. This allows you to correlate frontend users with their incoming messages.\nIt is recommended to pass a custom_session_id if you are using a Custom Language Model. See our guide to using a custom language model with EVI to learn more.\nWas this page helpful?\nYes\nNo\nPrevious\nInterruptibility\nGuide to EVI’s interruptibility feature and how to manage interruptions on the client.\nNext\nBuilt with"
            }
        }
    },
    "Features": {
        "Interruptibility": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/features/interruptibility",
            "content": {
                "title": "Interruptibility | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nInterruptibility\nPause responses\nDynamic variables\nContext injection\nTool use\nResume chats\nChat history\nAudio reconstruction\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nHow interruption works\nHandling interruptions on the client side\nEmpathic Voice Interface (EVI)\nFeatures\nInterruptibility\nCopy page\nGuide to EVI’s interruptibility feature and how to manage interruptions on the client.\nInterruptibility is a key feature of EVI, allowing seamless, real-time interactions even when the user interjects mid-response. EVI handles interruptions on the backend (stopping response generation) and supports interruption on the frontend (managing audio playback) to maintain a natural conversation flow.\nEVI stops generating audio when interrupted, but you are responsible for stopping playback of any audio already received on the client side to ensure a seamless, responsive experience.\nHow interruption works\nEVI sends responses in chunks as assistant_messages, each accompanied by corresponding audio_output messages. The assistant messages contain both the content and expression measurement predictions, while the audio_output messages contain the generated audio. Once EVI completes generating a response, it sends an assistant_end message to indicate that the response is finished.\nWhen a user message is detected during response generation, EVI stops generating the current response and sends a user_interrupt message to signal this event. This user_interrupt message instructs the client to halt audio playback, clear any remaining audio in the queue, and prepare for new input from the user.\nHandling interruptions on the client side\nWhile backend interruptions are managed by EVI, frontend interruptions—specifically stopping audio playback—require client-side handling. Both user_interruption messages (during response generation) and user_message events (after the response is complete) should trigger the client to stop audio playback for the previous response.\nTo handle interruptions consistently, the client should perform the following actions whenever a user_interruption or user_message is received:\nStop audio playback: Immediately halt playback of any ongoing audio from the previous response.\nClear queued audio: Remove any remaining audio segments in the queue to prevent overlap with new responses.\nThis approach ensures that any user interaction interrupts audio playback as expected, maintaining a natural flow by promptly responding to new user input.\nIf you’re using our React SDK, interruption handling is built-in. To see how it’s implemented, you can review the source code here.\nWas this page helpful?\nYes\nNo\nPrevious\nPause Responses\nGuide to pausing EVI’s responses during a Chat session.\nNext\nBuilt with"
            }
        },
        "Pause responses": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/features/pause-responses",
            "content": {
                "title": "Pause Responses | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nInterruptibility\nPause responses\nDynamic variables\nContext injection\nTool use\nResume chats\nChat history\nAudio reconstruction\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nHow to pause responses\nEVI while paused\nEVI when resumed\nEmpathic Voice Interface (EVI)\nFeatures\nPause Responses\nCopy page\nGuide to pausing EVI’s responses during a Chat session.\nThe pausing feature allows you to halt EVI’s audio output while keeping the session active, which is useful for managing conversation flow. For instance, a developer might create a button that lets users pause EVI’s responses if they need time to brainstorm or reflect without interruption. During this pause, EVI continues to listen and transcribe, allowing the user to interject or resume the conversation without disrupting the session. When the user is ready, they can resume EVI’s response to continue the interaction seamlessly.\nHow to pause responses\nTo pause EVI’s responses, send a pause_assistant_message, which holds all Assistant messages until a resume_assistant_message is received. When resumed, EVI responds with consideration of any user input received during the pause.\nNext.js\nTypeScript\nPython\n1 import React from 'react';\n2 import { useVoice } from \"@humeai/voice-react\";\n3\n4 export default function Controls() {\n5   const { pauseAssistant, resumeAssistant } = useVoice();\n6\n7   return (\n8     <div>\n9       <button onClick={pauseAssistant}>Pause EVI</button>\n10       <button onClick={resumeAssistant}>Resume EVI</button>\n11     </div>\n12   );\n13 }\nEVI while paused\nResponse generation stops: EVI stops the generation and sending of new responses. (assistant_message and audio_output messages will not be received while paused.)\nTool use is disabled: Any response involving tool use will also be disabled while paused. (tool_call_message, tool_response_message, and tool_error_message messages will not be received while paused.)\nQueued messages sent: Messages and audio queued before the pause_assistant_message are still processed and sent.\nContinued listening: EVI continues to “listen” and transcribe user input during the pause. Transcription of user audio is saved and are sent to the LLM as User messages.\nCharges will continue to accrue while EVI is paused. If you wish to completely pause both input and output you should instead disconnect and resume the chat when ready.\nEVI when resumed\nWhen EVI receives a resume_assistant_message, it generates a response that takes into account all user input received during the pause.\nPausing vs. muting: Pausing EVI’s responses is distinct from muting user input. With muted input, EVI does not “hear” the user’s audio and therefore cannot respond to it. While paused, however, EVI continues to process user input and can respond when resumed.\nResponse to paused input: Upon resuming, EVI may respond to multiple points or questions raised during the pause. However, by default, EVI prioritizes the latest user input rather than attempting to address all earlier points. For instance, if the user asks two questions while EVI is paused, EVI will generally respond to the second question, unless instructed to address each item.\nWas this page helpful?\nYes\nNo\nPrevious\nDynamic Variables\nPersonalize EVI chats by leveraging dynamic variables in your system prompt.\nNext\nBuilt with"
            }
        },
        "Dynamic variables": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/features/dynamic-variables",
            "content": {
                "title": "Dynamic Variables | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nInterruptibility\nPause responses\nDynamic variables\nContext injection\nTool use\nResume chats\nChat history\nAudio reconstruction\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nUsing variables in your prompt\nAssigning values in session settings\nDefault dynamic variables\nEmpathic Voice Interface (EVI)\nFeatures\nDynamic Variables\nCopy page\nPersonalize EVI chats by leveraging dynamic variables in your system prompt.\nDynamic variables are placeholders you put in the system prompt, that you can fill with specific values at the beginning of the chat, and update to new values as the chat progresses. They are especially useful for giving EVI context that might change depending on the user or on the session - like the date, the user’s name role, or account balance, or any other dynamic or session-specific information.\nUsing variables in your prompt\nTo set up dynamic variables, first include placeholders for them in your system prompt. Use double curly braces ({{variable_name}}) to mark where each variable should appear in the text. This allows EVI to replace these placeholders dynamically with the specified values.\nSample prompt with dynamic variables\n1 Address the user by their name, {{name}}.\n2 If relevant, reference their age: {{age}}.\n3 It is {{is_philosopher}} that this user is a philosopher.\nVisit our prompting guide for more details on adding dynamic variables to your prompt.\nAssigning values in session settings\nAfter adding placeholders for dynamic variables in your prompt, set their values by sending a Session Settings message over the WebSocket within an active Chat session. This message includes a variables parameter, with each key matching a placeholder in your prompt and each value specifying the text EVI will use.\nSession settings\n1 {\n2   \"type\": \"session_settings\",\n3   \"variables\": {\n4     \"name\": \"David Hume\",\n5     \"age\": 65,\n6     \"is_philosopher\": true\n7   }\n8 }\nVariable values can be strings, numbers, or booleans; however, each value is ultimately converted to a string when injected into your system prompt.\nTo ensure dynamic variables are recognized correctly, follow these guidelines:\nOnly assign values to referenced variables: If a variable is given a value in the “variables” field but is not referenced in the system prompt, EVI will not use it in the conversation.\nDefine all referenced variables: If a variable is referenced in the system prompt but lacks a value in the variables field, warning W0106 can be expected: \"No values have been specified for the variables [variable_name], which can lead to incorrect text formatting. Please assign them values.\" This warning is also expected if there are spelling inconsistencies between the variable names in variables and those in the prompt.\nDefault dynamic variables\nHume provides built-in dynamic variables that are automatically populated and can be referenced in system prompts without needing to set their values in SessionSettings. The currently supported default variable is:\nnow: The current UTC datetime (e.g., \"Nov 08, 2024 09:25 PM UTC\")\nYou can reference now in your system prompt to dynamically include the current UTC date and time, as shown below.\nTime-aware prompt example\n1 The current datetime is {{now}}. Mention this time at the start of \n2 the call, or if the user asks what time it is. Convert this UTC \n3 datetime to other time zones if requested.\nIf you set a custom value for a default variable in SessionSettings, it will override the default value. For example, specifying a value for now in SessionSettings will replace the automatic UTC datetime with your custom value, offering flexibility when needed.\nWas this page helpful?\nYes\nNo\nPrevious\nContext Injection\nGuide to injecting context during an EVI Chat session.\nNext\nBuilt with"
            }
        },
        "Context injection": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/features/context-injection",
            "content": {
                "title": "Context Injection | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nInterruptibility\nPause responses\nDynamic variables\nContext injection\nTool use\nResume chats\nChat history\nAudio reconstruction\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nSetting up context\nExample: Supporting travel planning context\nManaging context during a session\nEmpathic Voice Interface (EVI)\nFeatures\nContext Injection\nCopy page\nGuide to injecting context during an EVI Chat session.\nContext injection supplies the model with additional information so it can tailor its responses without the user needing to explicitly instruct EVI.\nEVI supports context injection via a Session Settings message. The context field in a Session Settings message allows you to silently add information to the conversation, guiding EVI without triggering a response. This context is appended to the end of each user_message, ensuring that it is consistently referenced throughout the session.\nInjected context can be used to remind EVI of its role, keep important details active in the conversation, or add relevant updates as needed. This method is ideal for adapting EVI’s tone or focus based on real-time changes, helping it respond more accurately without requiring repetitive input from the user.\nInjected context is only active within the current session. If a chat is resumed, any previously injected context will not be carried over and must be re-injected if necessary.\nSetting up context\nTo inject context, send a Session Settings message with a context object that includes two fields:\ntext: The content you want to inject, providing specific guidance for EVI. For example, if the user expresses frustration, you might set the context to encourage an empathetic response.\ntype: Defines how long the context remains active. Options include:\npersistent: Appended to all user messages throughout the session, ideal for consistent guidance.\ntemporary: Applies only to the next user message, suitable for one-time adjustments.\neditable: Allows updates to the context over time, useful for evolving needs.\nIf type is not specified, it defaults to temporary.\nExample: Supporting travel planning context\nTo tailor EVI’s responses for a travel planning scenario, you can inject context at different persistence levels based on user actions and session needs:\nPersistent\nEditable\nTemporary\nThis context provides EVI with a consistent focus on vacation planning, helping it to make relevant suggestions or ask guiding questions throughout the session.\nSession settings\n1 {\n2   \"type\": \"session_settings\",\n3   \"context\": {\n4     \"text\": \"The user is trying to find a destination for their next vacation.\",\n5     \"type\": \"persistent\"\n6   }\n7 }\nManaging context during a session\nClearing context: Send a Session Settings message with “context”: null to remove the injected context when it’s no longer needed.\nUpdating context dynamically: Use editable context if you need to adjust context over time, allowing for real-time updates without additional messages.\nWas this page helpful?\nYes\nNo\nPrevious\nTool Use\nGuide to utilize function calling with EVI.\nNext\nBuilt with"
            }
        },
        "Tool use": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/features/tool-use",
            "content": {
                "title": "Tool Use | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nInterruptibility\nPause responses\nDynamic variables\nContext injection\nTool use\nResume chats\nChat history\nAudio reconstruction\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nSetup\nFunction calling\nUsing built-in tools\nInterruptibility\nCanceling a function call\nUpdating a function call\nHandling errors\nSpecifying fallback content\nFailure message flow\nEmpathic Voice Interface (EVI)\nFeatures\nTool Use\nCopy page\nGuide to utilize function calling with EVI.\nEVI simplifies the integration of external APIs through function calling. Developers can integrate custom functions that are invoked dynamically based on the user’s input, enabling more useful conversations. There are two key concepts for using function calling with EVI: Tools and Configurations (Configs):\nTools are resources that EVI uses to do things, like search the web or call external APIs. For example, tools can check the weather, update databases, schedule appointments, or take actions based on what occurs in the conversation. While the tools can be user-defined, Hume also offers natively implemented tools, like web search, which are labeled as “built-in” tools.\nConfigurations enable developers to customize an EVI’s behavior and incorporate these custom tools. Setting up an EVI configuration allows developers to seamlessly integrate their tools into the voice interface. A configuration includes prompts, user-defined tools, and other settings.\nCurrently, our function calling feature only supports OpenAI and Anthropic models. For the best results, we suggest choosing a fast and intelligent LLM that performs well on function calling benchmarks. On account of its speed and intelligence, we recommend Claude 3.5 Haiku as the supplemental LLM in your EVI configuration when using tools. Function calling is not available if you are using your own custom language model. We plan to support more function calling LLMs in the future.\nThe focus of this guide is on creating a Tool and a Configuration that allows EVI to use the Tool. Additionally, this guide details the message flow of function calls within a session, and outlines the expected responses when function calls fail. Refer to our Configuration Guide for detailed, step-by-step instructions on how to create and use an EVI Configuration.\nExplore these sample projects to see how Tool use can be implemented in TypeScript, Next.js, and Python.\nSetup\nFor EVI to leverage tools or call functions, a configuration must be created with the tool’s definition. Our step-by-step guide below walks you through creating a tool and adding it to a configuration, using either a no-code approach through our Portal or a full-code approach through our API.\nNo code\nFull code\n1\nCreate a Tool\nWe will first create a Tool with a specified function. In this example, we will create a tool for getting the weather. In the Portal, navigate to the EVI Tools page. Click the Create tool button to begin.\n2\nFill in Tool details\nNext, we will fill in the details for a weather tool named get_current_weather. This tool fetches the current weather conditions in a specified location and reports the temperature in either Celsius or Fahrenheit. We can establish the tool’s behavior by completing the following fields:\nName: Specify the name of the function that the language model will invoke. Ensure it begins with a lowercase letter and only contains letters, numbers, or underscores.\nDescription: Provide a brief description of what the function does.\nParameters: Define the function’s input parameters using a JSON schema.\nThe JSON schema defines the expected structure of a function’s input parameters. Here’s an example JSON schema we can use for the parameters field of a weather function:\nparameters\n1 {\n2   \"type\": \"object\",\n3   \"required\": [\"location\", \"format\"],\n4   \"properties\": {\n5     \"location\": {\n6       \"type\": \"string\",\n7       \"description\": \"The city and state, e.g. San Francisco, CA\"\n8     },\n9     \"format\": {\n10       \"type\": \"string\",\n11       \"enum\": [\"celsius\", \"fahrenheit\"],\n12       \"description\": \"The temperature unit to use. Infer this from the user's location.\"\n13     }\n14   }\n15 }\n3\nCreate a Configuration\nNext, we will create an EVI Configuration called Weather Assistant Config. This configuration will utilize the get_current_weather Tool created in the previous step. See our Configuration guide for step-by-step instructions on how to create a configuration. During the Set up LLM step, remember to select an Anthropic or OpenAI model for tool use support.\n4\nAdd Tool to Configuration\nFinally, we will specify the get_current_weather Tool in the Weather Assistant Config. Navigate to the Tools section of the EVI Config details page. Click the Add button to add a function to your configuration. Since we have already created a get_current_weather Tool in previous steps, we can simply select Add existing tool… from the dropdown to specify it.\nSelect the tool to add get_current_weather to your configuration, then complete the remaining steps to create the configuration.\nFunction calling\nIn this section, we will go over the end-to-end flow of a function call within a chat session. This flow will be predicated on having specified the Weather Assistant Config when establishing a connection with EVI. See our Configuration Guide for details on how to apply your configuration when connecting.\nCheck out the TypeScript and Python example projects for complete implementations of the weather Tool you’ll build in this tutorial.\n1\nDefine a function\nWe must first define a function for your Tool. This function will take the same parameters as those specified during your Tool’s creation.\nFor this tutorial, we will define a function that calls a weather API (e.g., the Geocoding API) to retrieve the weather for a designated city in a specified format. This weather function will accept location and format as its parameters.\nSee the code below for a sample implementation:\nTypeScript\nPython\n1 async function fetchWeather(location: string, format: string): Promise<string> {\n2   // Fetch the location's geographic coordinates using Geocoding API\n3   const locationApiURL = `https://geocode.maps.co/search?q=${location}&api_key=${YOUR_WEATHER_API_KEY}`;\n4   const locationResponse = await fetch(locationApiURL);\n5   const locationData = await locationResponse.json();\n6\n7   // Extract latitude and longitude from fetched location data\n8   const { lat, lon } = locationData[0];\n9\n10   // Fetch point metadata using the extracted location coordinates\n11   const pointMetadataEndpoint = `https://api.weather.gov/points/${parseFloat(\n12     lat\n13   ).toFixed(3)},${parseFloat(lon).toFixed(3)}`;\n14   const pointMetadataResponse = await fetch(pointMetadataEndpoint);\n15   const pointMetadata = await pointMetadataResponse.json();\n16\n17   // Extract weather forecast URL from point metadata\n18   const forecastUrl = pointMetadata.properties.forecast;\n19\n20   // Fetch the weather forecast using the forecast URL\n21   const forecastResponse = await fetch(forecastUrl);\n22   const forecastData = await forecastResponse.json();\n23   const forecast = JSON.stringify(forecastData.properties.periods);\n24\n25   // Return the temperature in the specified format\n26   return `${forecast} in ${format}`;\n27 }\nInstead of calling a weather API, you can hardcode a return value like 75F as a means to quickly test for the sake of this tutorial.\n2\nEVI signals function call\nOnce EVI is configured with your Tool, it will automatically infer when to signal a function call within a chat session. With EVI configured to use the get_current_weather Tool, we can now ask it: “what is the weather in New York?”\nLet’s try it out in the EVI Playground.\nWe can expect EVI to respond with a User Message and a Tool Call message:\nSample User Message\n1 {\n2   \"type\": \"user_message\",\n3   \"message\": {\n4     \"role\": \"user\",\n5     \"content\": \"What's the weather in New York?\"\n6   },\n7   // ...etc\n8 }\nSample Tool Call message\n1 {\n2   \"type\": \"tool_call\",\n3   \"tool_type\": \"function\",\n4   \"response_required\": true,\n5   \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n6   \"name\": \"get_current_weather\",\n7   \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n8 }\nCurrently, EVI does not support parallel function calling. Only one function call can be processed at a time.\n3\nExtract arguments from Tool Call message\nUpon receiving a Tool Call message from EVI, we will parse the parameters and extract the arguments.\nThe code below demonstrates how to extract the location and format arguments, which the user-defined fetch weather function is expecting, from a received Tool Call message.\nTypeScript\nPython\n1 import { Hume } from 'hume';\n2\n3 async function handleToolCallMessage(\n4   toolCallMessage: Hume.empathicVoice.ToolCallMessage,\n5   socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {\n6   if (toolCallMessage.name === \"get_current_weather\") {\n7     // 1. Parse the parameters from the Tool Call message\n8     const args = JSON.parse(toolCallMessage.parameters) as {\n9       location: string;\n10       format: string;\n11     };\n12     // 2. Extract the individual arguments\n13     const { location, format } = args;\n14     // ...etc.\n15   }\n16 }\n4\nInvoke function call\nNext, we will pass the extracted arguments into the previously defined fetch weather function. We will capture the return value to send back to EVI:\nTypeScript\nPython\n1 import { Hume } from 'hume';\n2\n3 async function handleToolCallMessage(\n4   toolCallMessage: Hume.empathicVoice.ToolCallMessage,\n5   socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {\n6   if (toolCallMessage.name === \"get_current_weather\") {\n7     // 1. Parse the parameters from the Tool Call message\n8     const args = JSON.parse(toolCallMessage.parameters) as {\n9       location: string;\n10       format: string;\n11     };\n12     // 2. Extract the individual arguments\n13     const { location, format } = args;\n14     // 3. Call fetch weather function with extracted arguments\n15     const weather = await fetchWeather(location, format);\n16     // ...etc.\n17   }\n18 }\n5\nSend function call result\nUpon receiving the return value of your function, we will send a Tool Response message containing the result. The specified tool_call_id must match the one received in the Tool Call message from EVI:\nTypeScript\nPython\n1 import { Hume } from 'hume';\n2\n3 async function handleToolCallMessage(\n4   toolCallMessage: Hume.empathicVoice.ToolCallMessage,\n5   socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {\n6   if (toolCallMessage.name === \"get_current_weather\") {\n7     // 1. Parse the parameters from the Tool Call message\n8     const args = JSON.parse(toolCallMessage.parameters) as {\n9       location: string;\n10       format: string;\n11     };\n12     // 2. Extract the individual arguments\n13     const { location, format } = args;\n14     // 3. Call fetch weather function with extracted arguments\n15     const weather = await fetchWeather(location, format);\n16     // 4. Construct a Tool Response message containing the result\n17     const toolResponseMessage = {\n18       type: \"tool_response\",\n19       toolCallId: toolCallMessage.toolCallId,\n20       content: weather,\n21     };\n22     // 5. Send Tool Response message to the WebSocket\n23     socket.sendToolResponseMessage(toolResponseMessage);\n24   }\n25 }\nLet’s try it in the EVI Playground. Enter the return value of your function in the input field below the Tool Call message, and click Send Response. In practice, you will use the actual return value from your function call. However, for demonstration purposes, we will assume a return value of “75F”.\n6\nEVI responds\nAfter the interface receives the Tool Response message, it will then send an Assistant Message containing the response generated from the reported result of the function call:\nSample assistant_message\n1 {\n2   \"type\": \"assistant_message\",\n3   \"message\": {\n4     \"role\": \"assistant\",\n5     \"content\": \"The current temperature in New York, NY is 75F.\"\n6   }\n7 }\nSee how it works in the EVI Playground.\nTo summarize, Tool Call serves as a programmatic tool for intelligently signaling when you should invoke your function. EVI does not invoke the function for you. You will need to define a function, invoke the function, and pass the return value of your function to EVI via a Tool Response message. EVI will generate a response based on the content of your message.\nUsing built-in tools\nUser-defined tools allow EVI to identify when a function should be invoked, but you will need to invoke the function itself. On the other hand, Hume also provides built-in tools that are natively integrated. This means that you don’t need to define the function; EVI handles both determining when the function needs to be called and invoking it.\nHume supports the following built-in tools:\nweb_search: Enables EVI to search the web for real-time information when needed.\nhang_up: Closes the WebSocket connection with status code 1000 when appropriate (e.g., after detecting a farewell, signaling the end of the conversation).\nThis section explains how to specify built-in tools in your configurations and details the message flow you can expect when EVI uses a built-in tool during a chat session.\n1\nSpecify built-in tool in EVI configuration\nLet’s begin by creating a configuration which includes the built-in web search tool. To specify the web search tool in your EVI configuration, during the Add tools step, ensure Web search is enabled. Refer to our Configuration Guide for more details on creating a configuration.\nAlternatively, you can specify the built-in tool by making a POST request to /configs with the following request body:\nRequest body\n1 {\n2   \"name\": \"Web Search Config\",\n3   \"language_model\": {\n4     \"model_provider\": \"OPEN_AI\",\n5     \"model_resource\": \"gpt-3.5-turbo\"\n6   },\n7   \"builtin_tools\": [\n8     { \n9       \"name\": \"web_search\",\n10       \"fallback_content\": \"Optional fallback content to inform EVI’s spoken response if web search is not successful.\"\n11     }\n12   ]\n13 }\nUpon success, expect EVI to return a response similar to this example:\nSample response body\n1 {\n2   \"id\": \"3a60e85c-d04f-4eb5-8076-fb4bd344d5d0\",\n3   \"version\": 0,\n4   \"version_description\": null,\n5   \"name\": \"Web Search Config\",\n6   \"created_on\": 1714421925626,\n7   \"modified_on\": 1714421925626,\n8   \"prompt\": null,\n9   \"voice\": null,\n10   \"language_model\": {\n11     \"model_provider\": \"OPEN_AI\",\n12     \"model_resource\": \"gpt-3.5-turbo\",\n13     \"temperature\": null\n14   },\n15   \"tools\": [],\n16   \"builtin_tools\": [\n17     {\n18       \"tool_type\": \"BUILTIN\",\n19       \"name\": \"web_search\",\n20       \"fallback_content\": \"Optional fallback content to inform EVI’s spoken response if web search is not successful.\"\n21     }\n22   ]\n23 }\n2\nEVI uses built-in tool\nNow that we’ve created an EVI configuration which includes the built-in web search tool, let’s test it out in the EVI Playground. Try asking EVI a question that requires web search, like “what is the latest news with AI research?”\nEVI will send a response generated from the web search results:\nLet’s review the message flow for when web search is invoked.\nWeb search message flow\n1 // 1. User asks EVI for the latest news in AI research\n2 {\n3   \"type\": \"user_message\",\n4   \"message\": {\n5     \"role\": \"user\",\n6     \"content\": \"What is the latest news with AI research?\"\n7   },\n8   // ...etc\n9 }\n10 // 2. EVI infers it needs to use web search, generates a search query, and invokes Hume's native web search function\n11 {\n12   \"name\": \"web_search\", \n13   \"parameters\": \"{\\\"query\\\":\\\"latest news AI research\\\"}\", \n14   \"tool_call_id\": \"call_zt1NYGpPkhR7v4kb4RPxTkLn\", \n15   \"type\": \"tool_call\", \n16   \"tool_type\": \"builtin\", \n17   \"response_required\": false\n18 }\n19 // 3. EVI sends back the web search results \n20 {\n21   \"type\": \"tool_response\", \n22   \"tool_call_id\": \"call_zt1NYGpPkhR7v4kb4RPxTkLn\", \n23   \"content\": \"{ \\”summary\\”:null, “references”: [{\\”content\\”:\\”Researchers have demonstrated a new method...etc.\\”, \\”url\\”:\\”https://www.sciencedaily.com/news/computers_math/artificial_intelligence/\\”, \\”name\\”:\\”Artificial Intelligence News -- ScienceDaily\\”}] }\", \n24   \"tool_name\": \"web_search\", \n25   \"tool_type\": \"builtin\"\n26 }\n27 // 4. EVI sends a response generated from the web search results\n28 {\n29   \"type\": \"assistant_message\", \n30   \"message\": {\n31     \"role\": \"assistant\", \n32     \"content\": \"Oh, there's some interesting stuff happening in AI research right now.\"\n33   },\n34   // ...etc\n35 }\n36 {\n37   \"type\": \"assistant_message\", \n38   \"message\": {\n39     \"role\": \"assistant\", \n40     \"content\": \"Just a few hours ago, researchers demonstrated a new method using AI and computer simulations to train robotic exoskeletons.\"\n41   },\n42   // ...etc\n43 }\nInterruptibility\nFunction calls can be interrupted to cancel them or to resend them with updated parameters.\nCanceling a function call\nJust as EVI is able to infer when to make a function call, it can also infer from the user’s input when to cancel one. Here is an overview of what the message flow would look like:\nCancel function call message flow\n1 // 1. User asks what the weather is in New York\n2 {\n3   \"type\": \"user_message\",\n4   \"message\": {\n5     \"role\": \"user\",\n6     \"content\": \"What's the weather in New York?\"\n7   },\n8   // ...etc\n9 }\n10 // 2. EVI infers it is time to make a function call\n11 {\n12   \"type\": \"tool_call\",\n13   \"tool_type\": \"function\",\n14   \"response_required\": true,\n15   \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n16   \"name\": \"get_current_weather\",\n17   \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n18 }\n19 // 3. User communicates sudden disinterested in the weather\n20 {\n21   \"type\": \"user_message\",\n22   \"message\": {\n23     \"role\": \"user\",\n24     \"content\": \"Actually, never mind.\"\n25   }\n26 }\n27 // 4. EVI infers the function call should be canceled\n28 {\n29     \"type\": \"assistant_message\",\n30     \"message\": {\n31       \"role\": \"assistant\",\n32       \"content\": \"If you change your mind or need any weather information in the future, feel free to let me know.\"\n33     },\n34     // ...etc\n35   }\nUpdating a function call\nSometimes we don’t necessarily want to cancel the function call, and instead want to update the parameters. EVI can infer the difference. Below is a sample flow of interrupting the interface to update the parameters of the function call:\nUpdate function call message flow\n1 // 1. User asks EVI what the weather is in New York\n2 {\n3   \"type\": \"user_message\",\n4   \"message\": {\n5     \"role\": \"user\",\n6     \"content\": \"What's the weather in New York?\"\n7   },\n8   // ...etc\n9 }\n10 // 2. EVI infers it is time to make a function call\n11 {\n12   \"type\": \"tool_call\",\n13   \"tool_type\": \"function\",\n14   \"response_required\": true,\n15   \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n16   \"name\": \"get_current_weather\",\n17   \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n18 }\n19 // 3. User communicates to EVI they want the weather in Los Angeles instead\n20 {\n21   \"type\": \"user_message\",\n22   \"message\": {\n23     \"role\": \"user\",\n24     \"content\": \"Actually, Los Angeles.\"\n25   }\n26 }\n27 // 4. EVI infers the parameters to function call should be updated\n28 {\n29   \"type\": \"tool_call\",\n30   \"response_required\": true,\n31   \"tool_call_id\": \"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n32   \"name\": \"get_current_weather\",\n33   \"parameters\": \"{\\\"location\\\":\\\"Los Angeles\\\",\\\"format\\\":\\\"celsius\\\"}\"\n34 }\n35 // 5. User sends results of function call to EVI\n36 {\n37   \"type\": \"tool_response\",\n38   \"tool_call_id\":\"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n39   \"content\":\"72F\"\n40 }\n41 // 6. EVI sends response container function call result\n42 {\n43   \"type\": \"assistant_message\",\n44   \"message\": {\n45     \"role\": \"assistant\",\n46     \"content\": \"The current weather in Los Angeles is 72F.\"\n47   },\n48   // ...etc\n49 }\nHandling errors\nIt’s possible for tool use to fail. For example, it can fail if the Tool Response message content was not in UTF-8 format or if the function call response timed out. This section outlines how to specify fallback content to be used by EVI to communicate a failure, as well as the message flow for when a function call failure occurs.\nSpecifying fallback content\nWhen defining your Tool, you can specify fallback content within the Tool’s fallback_content field. When the Tool fails to generate content, the text in this field will be sent to the LLM in place of a result. To accomplish this, let’s update the Tool we created during setup to include fallback content. We can accomplish this by publishing a new version of the Tool via a POST request to /tools/{id}:\nRequest body\n1 {\n2   \"version_description\": \"Adds fallback content\",\n3   \"description\": \"This tool is for getting the current weather.\",\n4   \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the users location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\",\n5   \"fallback_content\": \"Something went wrong. Failed to get the weather.\"\n6 }\nSample response body\n1 {\n2   \"tool_type\": \"FUNCTION\",\n3   \"id\": \"36f09fdc-4630-40c0-8afa-6a3bdc4eb4b1\",\n4   \"version\": 1,\n5   \"version_type\": \"FIXED\",\n6   \"version_description\": \"Adds fallback content\",\n7   \"name\": \"get_current_weather\",\n8   \"created_on\": 1714421925626,\n9   \"modified_on\": 1714425632084,\n10   \"fallback_content\": \"Something went wrong. Failed to get the weather.\",\n11   \"description\": null,\n12   \"parameters\": \"{ \\\"type\\\": \\\"object\\\", \\\"properties\\\": { \\\"location\\\": { \\\"type\\\": \\\"string\\\", \\\"description\\\": \\\"The city and state, e.g. San Francisco, CA\\\" }, \\\"format\\\": { \\\"type\\\": \\\"string\\\", \\\"enum\\\": [\\\"celsius\\\", \\\"fahrenheit\\\"], \\\"description\\\": \\\"The temperature unit to use. Infer this from the user's location.\\\" } }, \\\"required\\\": [\\\"location\\\", \\\"format\\\"] }\"\n13 }\nFailure message flow\nThis section outlines the sort of messages that can be expected when Tool use fails. After sending a Tool Response message, we will know an error, or failure, occurred when we receive the Tool Error message:\nBad function call response error flow\n1 // 1. User asks EVI what the weather is in New York\n2 {\n3   \"type\": \"user_message\",\n4   \"message\": {\n5     \"role\": \"user\",\n6     \"content\": \"What's the weather in New York?\"\n7   },\n8   // ...etc\n9 }\n10 // 2. EVI infers it is time to make a function call\n11 {\n12   \"type\": \"tool_call\",\n13   \"tool_type\": \"function\",\n14   \"response_required\": true,\n15   \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n16   \"name\": \"get_current_weather\",\n17   \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n18 }\n19 // 3. User sends results of function call to EVI (result not formatted correctly)\n20 {\n21   \"type\": \"tool_response\",\n22   \"tool_call_id\":\"call_5RWLt3IMQyayzGdvMQVn5AOQ\",\n23   \"content\":\"MALFORMED RESPONSE\"\n24 }\n25 // 4. EVI sends response communicating it failed to process the tool_response\n26 {\n27   \"type\": \"tool_error\",\n28   \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n29   \"error\": \"Malformed tool response: <error message here>\",\n30   \"fallback_content\": \"Something went wrong. Failed to get the weather.\",\n31   \"level\": \"warn\"\n32 }\n33 // 5. EVI generates a response based on the failure\n34 {\n35   \"type\": \"assistant_message\",\n36   \"message\": {\n37     \"role\": \"assistant\",\n38     \"content\": \"It looks like there was an issue retrieving the weather information for New York.\"\n39   },\n40   // ...etc\n41 }\nLet’s cover another type of failure scenario: what if the weather API the function was using was down? In this case, we would send EVI a Tool Error message. When sending the Tool Error message, we can specify fallback_content to be more specific to the error our function throws. This is what the message flow would be for this type of failure:\nFailed function call flow\n1 // 1. User asks EVI what the weather is in New York\n2 {\n3   \"type\": \"user_message\",\n4   \"message\": {\n5     \"role\": \"user\",\n6     \"content\": \"What's the weather in New York?\"\n7   },\n8   // ...etc\n9 }\n10 // 2. EVI infers it is time to make a function call\n11 {\n12   \"type\": \"tool_call\",\n13   \"tool_type\": \"function\",\n14   \"response_required\": true,\n15   \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n16   \"name\": \"get_current_weather\",\n17   \"parameters\": \"{\\\"location\\\":\\\"New York\\\",\\\"format\\\":\\\"fahrenheit\\\"}\"\n18 }\n19 // 3. Function failed, so we send EVI a message communicating the failure on our end\n20 {\n21   \"type\": \"tool_error\",\n22   \"tool_call_id\": \"call_m7PTzGxrD0i9oCHiquKIaibo\",\n23   \"error\": \"Malformed tool response: <error message here>\",\n24   \"fallback_content\": \"Function execution failure - weather API down.\",\n25   \"level\": \"warn\"\n26 }\n27 // 4. EVI generates a response based on the failure\n28 {\n29   \"type\": \"assistant_message\",\n30   \"message\": {\n31     \"role\": \"assistant\",\n32     \"content\": \"Sorry, our weather resource is unavailable. Can I help with anything else?\"\n33   },\n34   // ...etc\n35 }\nLet’s revisit our function for handling Tool Call messages from the Function Calling section. We can now add support for error handling by sending Tool Error messages to EVI. This will enable our function to handle cases where fetching the weather fails or the requested tool is not found:\nTypeScript\nPython\n1 import { Hume } from 'hume';\n2\n3 async function handleToolCallMessage(\n4   toolCallMessage: Hume.empathicVoice.ToolCallMessage,\n5   socket: Hume.empathicVoice.chat.ChatSocket): Promise<void> {\n6   if (toolCallMessage.name === \"get_current_weather\") {\n7     try{\n8       // parse the parameters from the Tool Call message\n9       const args = JSON.parse(toolCallMessage.parameters) as {\n10         location: string;\n11         format: string;\n12       };\n13       // extract the individual arguments\n14       const { location, format } = args;\n15       // call fetch weather function with extracted arguments\n16       const weather = await fetchWeather(location, format);\n17       // send Tool Response message to the WebSocket\n18       const toolResponseMessage = {\n19         type: \"tool_response\",\n20         toolCallId: toolCallMessage.toolCallId,\n21         content: weather,\n22       };\n23       socket.sendToolResponseMessage(toolResponseMessage);\n24     } catch (error) {\n25       // send Tool Error message if weather fetching fails\n26       const weatherToolErrorMessage = {\n27         type: \"tool_error\",\n28         toolCallId: toolCallMessage.toolCallId,\n29         error: \"Weather tool error\",\n30         content: \"There was an error with the weather tool\",\n31       };\n32       socket.sendToolErrorMessage(weatherToolErrorMessage);\n33     }\n34   } else {\n35     // send Tool Error message if the requested tool was not found\n36     const toolNotFoundErrorMessage = {\n37       type: \"tool_error\",\n38       toolCallId: toolCallMessage.toolCallId,\n39       error: \"Tool not found\",\n40       content: \"The tool you requested was not found\",\n41     };\n42     socket.sendToolErrorMessage(toolNotFoundErrorMessage);\n43   }\n44 }\nWas this page helpful?\nYes\nNo\nPrevious\nResuming Chats\nGuide to preserving context from previous Chat sessions.\nNext\nBuilt with"
            }
        },
        "Resume chats": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/features/resume-chats",
            "content": {
                "title": "Resuming Chats | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nInterruptibility\nPause responses\nDynamic variables\nContext injection\nTool use\nResume chats\nChat history\nAudio reconstruction\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nResuming a chat\nEmpathic Voice Interface (EVI)\nFeatures\nResuming Chats\nCopy page\nGuide to preserving context from previous Chat sessions.\nEVI supports reconnecting to an ongoing chat session, preserving all prior conversation context. This is especially useful in cases of unexpected network failures or when a user wishes to pick up the conversation at a later time, enabling continuity without losing progress.\nIf data retention is disabled, the ability to resume chats will not be supported.\nResuming a chat\nSee steps below for how to resume a chat:\nEstablish initial connection: Make the initial handshake request to establish the WebSocket connection. Upon successful connection, you will receive a ChatMetadata message:\nChat metadata\n1 {\n2   \"type\": \"chat_metadata\",\n3   \"chat_group_id\": \"8859a139-d98a-4e2f-af54-9dd66d8c96e1\",\n4   \"chat_id\": \"2c3a8636-2dde-47f1-8f9e-cea27791fd2e\"\n5 }\nStore the ChatGroup reference: Save the chat_group_id from the ChatMetadata message for future use.\nResume chat: To resume a chat, include the stored chat_group_id in the resumed_chat_group_id query parameter of subsequent handshake requests.\nNext.js\nTypeScript\nPython\n1 \"use client\";\n2 import { VoiceProvider } from \"@humeai/voice-react\";\n3\n4 export default function ClientComponent({\n5   accessToken,\n6 }: {\n7   accessToken: string;\n8 }) {\n9   return (\n10     <VoiceProvider \n11       auth={{ type: \"accessToken\", value: accessToken }}\n12       resumedChatGroupId='<YOUR_CHAT_GROUP_ID>'\n13     >\n14       // ...etc.\n15     </VoiceProvider>\n16   );\n17 }\nWhen resuming a chat, you can specify a different EVI configuration than the one used in the previous session. However, changing the system prompt or supplemental LLM may result in unexpected behavior from EVI.\nWas this page helpful?\nYes\nNo\nPrevious\nChat History\nGuide to accessing chat history with the EVI.\nNext\nBuilt with"
            }
        },
        "Chat history": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/features/chat-history",
            "content": {
                "title": "Chat History | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nInterruptibility\nPause responses\nDynamic variables\nContext injection\nTool use\nResume chats\nChat history\nAudio reconstruction\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nChats vs Chat Groups\nFetching Chats and Chat Groups\nViewing Chats in the Platform UI\nChat Events\nFetching Chat Events\nFetching chat events for a specific Chat\nFetching events for a specific Chat Group\nParsing Chat Events\nChat transcription\nExpression measurement\nEmpathic Voice Interface (EVI)\nFeatures\nChat History\nCopy page\nGuide to accessing chat history with the EVI.\nEVI captures detailed histories of conversations, allowing developers to review and analyze past interactions. This guide provides an overview of Chats and Chat Groups, instructions for retrieving chat transcripts and expression measurements, and steps to access reconstructed audio.\nIf data retention is disabled, Chat history will not be recorded, and previous Chat data and audio reconstruction will not be retrievable.\nChats vs Chat Groups\nEVI organizes conversation history into Chats and Chat Groups.\nChats: Represents a single session from the moment a WebSocket connection is established until it disconnects. Each Chat contains messages and events recorded during that specific session.\nChat Groups: Links related chats to provide continuity across interactions. A group can contain one or more chats, allowing ongoing conversations to be tracked even when users reconnect to continue from a previous interaction.\nWhen a new Chat session begins, it creates a new Chat Group by default. However, if the Chat resumes a previous session, it is added to the existing Chat Group, ensuring the conversation’s history and context are preserved across multiple Chats.\nFetching Chats and Chat Groups\nEach Chat has a unique ID and a chat_group_id field that links it to its associated Chat Group. Similarly, each Chat Group has its own unique ID, enabling the retrieval of individual sessions or entire groups of related sessions.\nChat ID: To obtain a Chat ID, use the list Chats endpoint. This ID allows you to retrieve details of individual sessions or resume a previous Chat. See sample code for fetching Chats below:\ncURL\nTypeScript\nPython\n$ curl -G https://api.hume.ai/v0/evi/chats \\\n>   -H \"X-Hume-Api-Key: <YOUR_API_KEY>\" \\\n>   -d page_number=0 \\\n>   -d page_size=10 \\\n>   -d ascending_order=false\nChat Group ID: Each Chat includes a chat_group_id field, which identifies the Chat Group it belongs to. To obtain a Chat Group ID directly, use the list Chat Groups endpoint. This ID is useful for accessing all Chats linked to a single conversation that spans multiple sessions. See sample code for fetching Chats below:\ncURL\nTypeScript\nPython\n$ curl -G https://api.hume.ai/v0/evi/chat_groups \\\n>   -H \"X-Hume-Api-Key: <YOUR_API_KEY>\" \\\n>   -d page_number=0 \\\n>   -d page_size=1 \\\n>   -d ascending_order=false\nWhile you can retrieve these IDs using the API, the Chat and Chat Group IDs are also included at the start of every Chat session in a chat_metadata message. This is particularly useful if your integration needs to associate data or actions with Chats as they are initiated.\nchat_metadata\n1 {\n2   \"type\": \"chat_metadata\",\n3   \"chat_group_id\": \"369846cf-6ad5-404d-905e-a8acb5cdfc78\",\n4   \"chat_id\": \"470a49f6-1dec-4afe-8b61-035d3b2d63b0\",\n5   \"request_id\": \"73c75efd-afa2-4e24-a862-91096b0961362258039\"\n6 }\nViewing Chats in the Platform UI\nYou can also view chat history and obtain Chat IDs through the Platform UI:\nGo to the Chat history page for a paginated list of past Chats, each displaying key details like the Chat ID, datetime, event count, and duration.\nClick “Open” on any Chat to view its details. The details page includes information such as status, start and end timestamps, duration, the Chat ID, Chat Group ID, associated Config ID (if any), and a paginated list of Chat Events.\nChat Events\nDuring each Chat session, EVI records events that detail interactions between the user and the system. These events provide a complete record of user input, assistant responses, tool usage, and system commands, enabling developers to review transcripts, analyze activity, and extract expression measurements. Below is the complete list of WebSocket messages recorded as Chat Events:\nuser_message: Transcriptions of the user’s spoken input, with expression measures of the user’s voice.\nuser_interruption: Instances where the user interrupts EVI’s speech.\nassistant_message: Transcriptions of EVI’s responses.\ntool_call_message: Records when a tool is invoked during the conversation.\ntool_response_message: Responses from tools that were invoked during the session.\npause_assistant_message: Commands to pause EVI’s responses.\nresume_assistant_message: Commands to resume EVI’s responses after a pause.\nThese events cannot be modified and represent an immutable record of the conversation for transcription and analysis purposes.\nFetching Chat Events\nThe Chat Events API provides endpoints to fetch events for a specific Chat or a Chat Group, allowing developers to retrieve detailed session data. Below are examples of how to use these endpoints:\nFetching chat events for a specific Chat\nUse the /chats/{chat_id}/events endpoint to fetch events for a single Chat:\ncURL\nTypeScript\nPython\n$ curl -G https://api.hume.ai/v0/evi/chats/<YOUR_CHAT_ID> \\\n>   -H \"X-Hume-Api-Key: <YOUR_API_KEY>\" \\\n>   -d page_number=0 \\\n>   -d page_size=10 \\\n>   -d ascending_order=false\nFetching events for a specific Chat Group\nUse the /chat_groups/{chat_group_id}/events endpoint to fetch events from all Chats within a specific Chat Group:\ncURL\nTypeScript\nPython\n$ curl -G https://api.hume.ai/v0/evi/chats/<YOUR_CHAT_GROUP_ID> \\\n>   -H \"X-Hume-Api-Key: <YOUR_API_KEY>\" \\\n>   -d page_number=0 \\\n>   -d page_size=10 \\\n>   -d ascending_order=false\nParsing Chat Events\nChat Events provide a detailed record of interactions during a Chat session, capturing both transcriptions and expression measurement predictions. This section demonstrates how to process these events to generate readable transcripts and analyze emotional trends.\nFor sample code demonstrating how to fetch and parse Chat Events, explore our example projects in TypeScript and Python.\nChat transcription\nTranscriptions of a conversation are stored in user_message and assistant_message events. These events include the speaker’s role and the corresponding text, allowing you to reconstruct the dialogue into a readable format.\nFor instance, you may need to create a transcript of a conversation for documentation or analysis. Transcripts can help review user intent, evaluate system responses, or provide written records for compliance or training purposes.\nThe following example demonstrates how to extract the Chat transcription from a list of Chat Events and save it as a text file named transcription_<CHAT_ID>.txt:\nTypeScript\nPython\n1 import fs from \"fs\"; \n2 import { ReturnChatEvent } from \"hume/api/resources/empathicVoice\";\n3\n4 function generateTranscript(chatEvents: ReturnChatEvent[]): void {\n5   // Filter events for user and assistant messages\n6   const relevantChatEvents = chatEvents.filter(\n7     (chatEvent) => chatEvent.type === \"USER_MESSAGE\" || chatEvent.type === \"AGENT_MESSAGE\"\n8   );\n9\n10   // Map each relevant event to a formatted line\n11   const transcriptLines = relevantChatEvents.map((chatEvent) => {\n12     const role = chatEvent.role === \"USER\" ? \"User\" : \"Assistant\";\n13     const timestamp = new Date(chatEvent.timestamp).toLocaleString(); // Human-readable date/time\n14     return `[${timestamp}] ${role}: ${chatEvent.messageText}`;\n15   });\n16\n17   // Join all lines into a single transcript string\n18   const transcript =  transcriptLines.join(\"\\n\");\n19   // Define the transcript file name\n20   const transcriptFileName = `transcript_${CHAT_ID}.txt`;\n21   // Write the transcript to a text file\n22   try {\n23     fs.writeFileSync(transcriptFileName, transcript, \"utf8\");\n24     console.log(`Transcript saved to ${transcriptFileName}`);\n25   } catch (fileError) {\n26     console.error(`Error writing to file ${transcriptFileName}:`, fileError);\n27   }\n28 }\nExpression measurement\nExpression measurement predictions are stored in the user_message events under the models.prosody.scores property. These predictions provide confidence levels for various emotions detected in the user’s speech.\nFor example, you might want to gauge the emotional tone of a conversation to better understand user sentiment. This information can guide customer support strategies or highlight trends in the expression measurement predictions over time.\nThe following example calculates the top 3 emotions from the user_messages by averaging their emotion scores across the Chat session:\nTypeScript\nPython\n1 import { ReturnChatEvent, EmotionScores } from \"hume/api/resources/empathicVoice\";\n2\n3 function getTopEmotions(chatEvents: ReturnChatEvent[]): Partial<EmotionScores> {\n4   // Extract user messages that have emotion features\n5   const userMessages = chatEvents.filter(\n6     (event) => event.type === \"USER_MESSAGE\" && event.emotionFeatures\n7   );\n8\n9   const totalMessages = userMessages.length;\n10\n11   // Infer emotion keys from the first user message\n12   const firstMessageEmotions = JSON.parse(userMessages[0].emotionFeatures!) as EmotionScores;\n13   const emotionKeys = Object.keys(firstMessageEmotions) as (keyof EmotionScores)[];\n14\n15   // Initialize sums for all emotions to 0 (no extra type assertions needed)\n16   const emotionSums: Record<keyof EmotionScores, number> = Object.fromEntries(\n17     emotionKeys.map((key) => [key, 0])\n18   ) as Record<keyof EmotionScores, number>;\n19\n20   // Accumulate emotion scores from each user message\n21   for (const event of userMessages) {\n22     const emotions = JSON.parse(event.emotionFeatures!) as EmotionScores;\n23     for (const key of emotionKeys) {\n24       emotionSums[key] += emotions[key];\n25     }\n26   }\n27\n28   // Compute average scores for each emotion\n29   const averageEmotions = emotionKeys.map((key) => ({\n30     emotion: key,\n31     score: emotionSums[key] / totalMessages,\n32   }));\n33\n34   // Sort by average score (descending) and pick the top 3\n35   averageEmotions.sort((a, b) => b.score - a.score);\n36   const top3 = averageEmotions.slice(0, 3);\n37\n38   // Build a Partial<EmotionScores> with only the top 3 emotions\n39   const result: Partial<EmotionScores> = {};\n40   for (const { emotion, score } of top3) {\n41     result[emotion] = score;\n42   }\n43\n44   return result;\n45 }\nWas this page helpful?\nYes\nNo\nPrevious\nAudio Reconstruction\nGuide to reconstructing the audio from previous Chats for playback.\nNext\nBuilt with"
            }
        },
        "Audio reconstruction": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/features/audio-reconstruction",
            "content": {
                "title": "Audio Reconstruction | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nInterruptibility\nPause responses\nDynamic variables\nContext injection\nTool use\nResume chats\nChat history\nAudio reconstruction\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nHow audio reconstruction works\nAudio reconstruction statuses\nFetching reconstructed audio for a Chat\nFetching reconstructed audio for a Chat Group\nPolling for completion\nDownloading the audio file\nEmpathic Voice Interface (EVI)\nFeatures\nAudio Reconstruction\nCopy page\nGuide to reconstructing the audio from previous Chats for playback.\nThe audio reconstruction feature allows you to listen to past conversations by stitching together all audio snippets from a Chat—including both user inputs and EVI’s responses—into a single audio file. This can be useful for reviewing interactions, quality assurance, or integrating playback functionality into your application.\nIf data retention is disabled, Chat history will not be recorded, and previous Chat data and audio reconstruction will not be retrievable.\nHow audio reconstruction works\nThe audio reconstruction process combines individual audio clips into a continuous file. Here are some important considerations:\nStorage duration: Reconstructed audio files are stored indefinitely.\nSigned URL expiration: The signed_audio_url expires after 60 minutes. If it expires before you download the file, you can generate a new URL by making another API request.\nNo merging of Chats: The API does not support combining multiple Chats within a Chat Group into a single audio file.\nAsynchronous process: Audio reconstruction is performed in the background. The time required depends on the conversation’s length and system load.\nAudio reconstruction statuses\nThe status of an audio reconstruction request will indicate its progress:\nQUEUED: The reconstruction job is waiting to be processed.\nIN_PROGRESS: The reconstruction is currently being processed.\nCOMPLETE: The audio reconstruction is finished and ready for download.\nERROR: An error occurred during the reconstruction process.\nCANCELED: The reconstruction job has been canceled.\nFetching reconstructed audio for a Chat\nTo fetch the reconstructed audio for a specific Chat, use the following endpoint: /chats/{chat_id}/audio.\ncURL\nTypeScript\nPython\n$ # Replace {chat_id} with your Chat ID\n> # Ensure your API key is set in the HUME_API_KEY environment variable\n> curl -X GET \"https://api.hume.ai/v0/evi/chats/{chat_id}/audio\" \\\n>     -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n>     -H \"Accept: application/json\"\nExample response (audio reconstruction initiated):\n1 // Sample response (audio reconstruction initiated)\n2 {\n3   \"id\": \"470a49f6-1dec-4afe-8b61-035d3b2d63b0\",\n4   \"user_id\": \"e6235940-cfda-3988-9147-ff531627cf42\",\n5   \"status\": \"QUEUED\",\n6   \"filename\": null,\n7   \"modified_at\": 1729875432555,\n8   \"signed_audio_url\": null,\n9   \"signed_url_expiration_timestamp_millis\": null  \n10 }\nIf audio reconstruction for a Chat or Chat Group hasn’t already occurred, calling the respective endpoint will automatically add the audio reconstruction process to our job queue.\nFetching reconstructed audio for a Chat Group\nTo fetch a paginated list of reconstructed audio for Chats within a Chat Group, use the following endpoint: /chat_groups/{chat_group_id}/audio.\ncURL\nTypeScript\nPython\n$ # Replace {chat_group_id} with your Chat Group ID\n> # Include pagination parameters as needed\n> # Ensure your API key is set in the HUME_API_KEY environment variable\n> curl -X GET \"https://api.hume.ai/v0/evi/chat_groups/{chat_group_id}/audio?page_number=1&page_size=10&ascending_order=false\" \\\n>     -H \"X-Hume-Api-Key: $HUME_API_KEY\" \\\n>     -H \"Accept: application/json\"\n1 // Sample response (audio reconstruction initiated)\n2 {\n3   \"id\": \"369846cf-6ad5-404d-905e-a8acb5cdfc78\",\n4   \"user_id\": \"e6235940-cfda-3988-9147-ff531627cf42\",\n5   \"num_chats\": 1,\n6   \"page_number\": 0,\n7   \"page_size\": 10,\n8   \"total_pages\": 1,\n9   \"pagination_direction\": \"DESC\",\n10   \"audio_reconstructions_page\": [\n11     {\n12       \"id\": \"470a49f6-1dec-4afe-8b61-035d3b2d63b0\",\n13       \"user_id\": \"e6235940-cfda-3988-9147-ff531627cf42\",\n14       \"status\": \"QUEUED\",\n15       \"filename\": null,\n16       \"modified_at\": 1729875432555,\n17       \"signed_audio_url\": null,\n18       \"signed_url_expiration_timestamp_millis\": null  \n19     }\n20   ]\n21 }\nPolling for completion\nSince the reconstruction process is asynchronous, you can poll the endpoint to check the status field until it changes to COMPLETE. Once the status is COMPLETE, the signed_audio_url and signed_url_expiration fields will be populated.\n1 // Sample response (reconstruction complete)\n2 {\n3   \"id\": \"470a49f6-1dec-4afe-8b61-035d3b2d63b0\",\n4   \"user_id\": \"e6235940-cfda-3988-9147-ff531627cf42\",\n5   \"status\": \"COMPLETE\",\n6   \"filename\": \"e6235940-cfda-3988-9147-ff531627cf42/470a49f6-1dec-4afe-8b61-035d3b2d63b0/reconstructed_audio.mp4\",\n7   \"modified_at\": 1729875432555,\n8   \"signed_audio_url\": \"https://storage.googleapis.com/...etc.\",\n9   \"signed_url_expiration_timestamp_millis\": 1730232816964  \n10 }\nDownloading the audio file\nAfter the reconstruction is complete, you can download the audio file using the signed_audio_url. The following cURL command saves the audio file using the original filename provided by the server:\n$ # Replace {signed_audio_url} with the URL from the API response\n> curl -O \"{signed_audio_url}\"\nWas this page helpful?\nYes\nNo\nPrevious\nAudio\nGuide to recording and playing audio for an EVI chat.\nNext\nBuilt with"
            }
        }
    },
    "Guides": {
        "Audio": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/guides/audio",
            "content": {
                "title": "Audio | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nAudio\nPrompting\nCustom language model\nPhone calling\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nOverview\nRecording\nMuting\nPlayback\nReceive audio\nPlay the audio from a queue\nHandle interruption\nEnable verbose transcriptions to prevent interruption delays\nEnqueue audio\nPlay audio from the queue\nHandle interruption\nEnable verbose transcriptions to prevent interruption delays\nHandling complex audio scenarios\nUnderstanding digital audio\nAudio formats\nAudio/WebM\nLinear PCM\nNon-supported formats\nClicking\nTroubleshooting audio\nEmpathic Voice Interface (EVI)\nGuides\nAudio\nCopy page\nGuide to recording and playing audio for an EVI chat.\nOverview\nThis detailed guide explains how to be successful recording and playing back audio with EVI.\nThe best way to handle audio is to use the Hume AI React SDK @humeai/voice-react, which takes care of everything in this guide out of the box. If you are using the Hume AI TypeScript SDK directly, or connecting to EVI from a different programming language, follow the instructions in this guide to handle audio recording and playback correctly.\nThings to keep in mind when working with audio in EVI:\nEVI is live. EVI audio is streamed, not pre-recorded. With EVI, you continuously send audio in small chunks, not whole files.\nEVI is a voice chat. EVI depends on advanced audio processing features like echo cancellation, noise suppression, and auto gain control, which must be enabled explicitly.\nAudio environments vary. Your users may be using different browsers, different devices, different operating systems, different hardware, and what works in one audio environment may not work in another.\nRecording\nWeb\niOS\nThis section applies if you are using the Hume TypeScript SDK directly. If you are using the Hume AI React SDK (@humeai/voice-react), please refer to Next.js sections of the Quickstart guide.\n1\nConnect to EVI\nBefore recording, open a WebSocket connection to the /v0/evi/chat endpoint. For more context, see the TypeScript quickstart guide.\n1 import { Hume, HumeClient } from 'hume';\n2 const client = new HumeClient(...);\n3 const socket = await client.empathicVoice.chat.connect(...);\nClient-side, not server-side - Typically, you should open the WebSocket connection to EVI on the client-side: either from your web frontend in JavaScript that runs in the user’s browser, or from inside your mobile app, because the server-side does not have direct access to the user’s microphone. Connecting to EVI from your backend is possible, but in this case you will have to transmit audio from the user’s device to your backend, and then from your backend to EVI, which will add latency.\nWebSocket before Microphone - Connect to the EVI WebSocket before you start recording from the microphone. Audio formats like wav and webm begin with a header that you must transmit in order for EVI to be able to interpret the audio correctly. If the WebSocket connection is not ready when you begin recording and attempt to send the first bytes, you may inadvertently cut off the header.\n2\nDetermine the audio format\nDifferent browsers support sending different audio formats, described by MIME types. Use the getBrowserSupportedMimeType function from the Hume TypeScript SDK to determine an appropriate MIME type.\n1 import {\n2   getBrowserSupportedMimeType,\n3 } from 'hume';\n4 const mimeType: MimeType = (() => {\n5   const result = getBrowserSupportedMimeType();\n6   return result.success ? result.mimeType : MimeType.WEBM;\n7 })();\n3\nStart the audio stream\nUse the getAudioStream helper from the Hume TypeScript SDK. This enables echo cancellation, noise suppression, and auto gain and wraps the standard MediaDevices.getUserMedia web interface.\nUse the ensureSingleValidAudioTrack helper to make sure that there is a usable audio track. This will throw an error if there isn’t a single audio track (for example, if the user doesn’t have a microphone).\n1 import {\n2   getAudioStream,\n3   ensureSingleValidAudioTrack,\n4 } from 'hume';\n5 let audioStream = await getAudioStream();\n6 ensureSingleValidAudioTrack(audioStream);\n4\nRecord, base64 encode, and transmit\nUse the MediaRecorder API to record audio from the microphone. Inside the .ondataavailable handler, encode the bytes of the audio into a base64 string and send it in an audio_input message to EVI.\n1   import { convertBlobToBase64 } from 'hume';\n2   let recorder = new MediaRecorder(audioStream, { mimeType });\n3   recorder.ondataavailable = async ({ data }) => {\n4     if (data.size < 1) return;\n5     const encodedAudioData = await convertBlobToBase64(data);\n6     socket.sendAudioInput({ data: encodedAudioData });\n7   };\n8   // capture audio input at a rate of 100ms (recommended for web)\n9   recorder.start(100);\n5\nAdd support for muting\nMost EVI integrations should allow the user to temporarily mute their microphone. The standard way to mute an audio stream is to send audio frames filled with empty data (versus not sending anything during mute). This helps distinguish between a muted-but-still-active audio stream and a stream that has become disconnected.\n1 recorder.ondataavailable = async ({ data }) => {\n2   if (data.size < 1) return;\n3   if (isMuted) {\n4     const silence = new Blob([new Uint8Array(data.size)], { type: mimeType });\n5     const encodedAudioData = await convertBlobToBase64(silence);\n6     socket.sendAudioInput({ data: encodedAudioData });\n7     return;\n8   }\n9   const encodedAudioData = await convertBlobToBase64(data);\n10   socket.sendAudioInput({ data: encodedAudioData });\n11 };\nThe above code snippets are lightly adapted from the EVI TypeScript Quickstart. View the full source code on GitHub to see the complete implementation.\nPlayback\nAt a high level, to play audio from EVI:\nListen for audio_output messages from EVI and base64 decode them.\nImplement a queue to store audio segments from EVI. Audio from EVI can arrive faster than it is spoken, so EVI will cut itself off if you play audio segments as soon as they arrive.\nHandle interruptions. You should stop playing the current audio segment and clear the queue when the user_interruption or user_message events are received.\nWeb\niOS\nReceive audio\nAfter connecting to EVI, listen for audio_output messages. Audio output messages have a data field that contains a base64-encoded WAV file. In a browser environment, you can use the convertBase64ToBlob function from the Hume TypeScript SDK to convert the base64 string to a Blob object.\n1 socket.on('message', (message) => {\n2   switch (message.type) {\n3     case 'audio_output':\n4       const blob = convertBase64ToBlob(message.data);\n5       ...\n6   }\n7 })\nPlay the audio from a queue\nEVI can generate audio segments faster than they are spoken. Instead of playing the audio segments directly, you should place them into a queue on receipt.\nTo play an audio segment, convert the Blob to an Object URL use the Audio constructor from object from the browser’s HTMLAudioElement API, and call .play. Use the .onended listener to know when the segment has completed and play the next segment in the queue.\n1 const audioQueue: Blob[] = [];\n2 // Keep track of the currently-playing audio so it can\n3 // be stopped in the case of interruption\n4 let currentAudio: HTMLAudioElement | null = null;\n5\n6 function playAudio() {\n7   if (!audioQueue.length || isPlaying) return;\n8   isPlaying = true;\n9   const audioBlob = audioQueue.shift();\n10   if (!audioBlob) return;\n11   const audioUrl = URL.createObjectURL(audioBlob);\n12   currentAudio = new Audio(audioUrl);\n13   currentAudio.play();\n14   currentAudio.onended = () => {\n15     isPlaying = false;\n16     if (audioQueue.length) playAudio();\n17   };\n18 }\n1 switch (message.type) {\n2   case 'audio_output':\n3     const blob = convertBase64ToBlob(message.data);\n4     audioQueue.push(blob);\n5     if (audioQueue.length >= 1) playAudio();\n6     ...\n7 }\nHandle interruption\nEVI produces a user_interruption event if it detects that the user intends to speak while it is generating audio. However, it is also possible that a user will speak after EVI has finished generating audio for its turn, but before the audio has finished playing inside the browser. In this case, EVI will not produce a user_interruption event but will produce a user_message event. In both cases, you should stop the currently playing audio and empty the queue.\n1 switch (message.type) {\n2   case 'user_interruption':\n3     stopAudio();\n4     break;\n5   case 'user_message':\n6     stopAudio();\n7     // Any additional handling for user messages\n8     break;\n9   ...\n10 }\n11\n12 function stopAudio(): void {\n13   currentAudio?.pause();\n14   currentAudio = null;\n15   isPlaying = false;\n16  \n17   // clear the audioQueue\n18   audioQueue.length = 0;\n19 }\nEnable verbose transcriptions to prevent interruption delays\nBy default, user_message events are only sent after the user has spoken for enough time to generate an accurate transcript. This can result in a perceptible delay in the user’s ability to interrupt EVI during the period when EVI is done generating its audio for the turn but before the browser has finished playing it.\nTo address this, you should set the query parameter verbose_transcription=true when opening the WebSocket connection to the /v0/evi/chat endpoint. This will cause EVI to send “interim” user messages, with an incomplete transcript, as soon as it detects that the user is speaking.\nYou should use these interim messages to stop the currently playing audio and clear the queue. Modify any other logic that uses the transcript from user_message events to either ignore messages with interim: true, or take into account how several interim messages transcribing the same segment of speech may be sent before the final message.\n1 socket = client.empathicVoice.chat.connect({\n2   ...,\n3   verbose_transcription: true,\n4 })\n5 switch (message.type) {\n6   ...\n7   case 'user_message':\n8     stopAudio();\n9     if (message.interim) {\n10       // ignore interim messages for any handling\n11       // of transcriptions\n12       break;\n13     }\n14     // Any additional handling for user messages\n15     break;\n16   ...\n17 }\nHandling complex audio scenarios\nUsers sometimes have multiple audio devices, play audio from multiple sources, or unplug devices while your app is in use. You should think about how your app should behave in these more complex scenarios. The answer to these questions will vary based on the purpose of your app, but here is a list of scenarios you should consider:\nGraceful permission handling - Always check for audio permissions before starting to record audio. If the user has not granted permission, display an appropriate message and give the user instructions how to grant the permission.\nDevice selection - Simple EVI integrations can hardcode the default microphone and audio playback device. Consider what to do when there are multiple devices available. Should you default to headphones, if they are available? Should you allow the user to select a device?\nDevice unavailability - Users unplug audio devices, or revoke permission to record audio. In this case, fall back to a different audio device if appropriate, or pause the chat and display a message to resume.\nBackground audio - If you are building a mobile app, does it make sense for your app to be able to play audio in the background (for example, if the user switches apps to go look something up in a web browser)? What should happen when the user starts a chat but there is already audio playing in the background (listening to music, perhaps), should your app interrupt it?\nUnderstanding digital audio\nA common source of issues when building with EVI is malformed or unsupported audio. This section explains what audio formats EVI supports, gives some conceptual background on how to understand digital audio more generally, and gives some advice for how to troubleshoot audio-related issues.\nAudio formats\nHume attempts to accomodate the widest range of audio formats supported by our tools and partners. However, we recommend converting to one of the industry’s most commonly supported audio formats for the sake of your own troubleshooting. Two excellent choices are:\nLinear PCM - A simple format for uncompressed audio that is easy to convert to and is supported by most audio processing tools.\nAudio/webm - This format is a web standard that allows sending compressed audio. It is supported by most browsers.\nAudio/WebM\nWebM is a container format that contains compressed audio, supported by all modern browsers.\nA WebM audio stream begins with a header that identifies the stream as being WebM, with metadata describing the codec with which the audio is compressed and other details about the audio, such as the sample rate. If you are sending audio in the WebM format (or any format with a header), take care not to cut off the header. Avoid starting the audio stream when the WebSocket connection is not open. If you have implemented a mute button, test what happens when the chat starts while mute is enabled.\nLinear PCM\nPCM (pulse-code modulation) is a method of representing audio as a sequence of “samples” that capture the amplitude of the audio signal at regular intervals. PCM actually describes a family of different representations that vary along several dimensions: sample rate, number of channels, bit depth, and more. You must communicate these details to EVI in order for EVI to be able to interpret the audio correctly. PCM is a headerless format, so there is no way to communicate these details in the stream of audio data itself. Instead, you must send a session_settings message specifying the sample rate, the number of channels, and an encoding. Presently, the only supported encoding is linear16, which is the most common. It describes a linear quantized PCM encoding where each sample is a 16-bit signed, little-endian integer.\n1 {\n2   \"type\": \"session_settings\",\n3   \"audio\": {\n4     \"format\": \"linear16\",\n5     \"sample_rate\": 44100,\n6     \"channels\": 1\n7   }\n8 }\nEnsure the details you specify in the session_settings message match the actual the audio you are sending. If the sample rate is incorrect, the audio may be distorted but still intelligible, which could interfere with EVI’s emotional analysis. It’s difficult to understand your user’s subtle emotional cues if they sound like Alvin and the Chipmunks.\nNon-supported formats\nMulaw (or μ-law) is a non-linear 8-bit PCM encoding that is commonly used in telephony, for example, it is used by Twilio Media Stream API. This encoding is not presently supported by EVI. If you are receiving audio from a telephony service that uses mulaw encoding, you will need to convert.\nClicking\n“Clicking” occurs when there is a discontinuity in the audio waveform. While segments received from EVI are continuous, you may hear a click if you stop audio playback abruptly, for example, when a user interruption occurs. You can eliminate the clicks by implementing a brief fadeout effect when stopping playback.\nTroubleshooting audio\nAudio issues can surface in several different ways:\nTranscription errors - you may see an error message over the chat WebSocket like\n1 {\"type\":\"error\",\"code\":\"I0118\",\"slug\":\"transcription_disconnected\",\"message\":\"Transcription socket disconnected.\"}\nfollowed by the chat ending. This results from an error that happened while attempting to transcribe speech from the audio you sent, and often indicates that the audimio is malformed.\nUnexpected silence - Another failure mode is when you send audio_input messages, the user is speaking, but you do not receive any messages back, neither user_message, nor assistant_message. This can happen when EVI believes it has successfully decoded the audio, but has assumed the wrong format, and while the bytes of your audio would contain speech if decoded in the correct format, they appear to be static or silence when decoded incorrectly.\nOnce you have observed a behavior that could indicate an audio issue, you can troubleshoot by directly inspecting the audio_input messages your application sends to EVI and attempt to decode it and play it back yourself.\nYou can find this by adding log statements to your application, or from the Network tab of your browser’s developer tools.\n1\nObserve outgoing Session Settings and Audio Input messages\nIf your application runs in a web browser, you can view all the transmitted WebSocket messages. Open the developer tools, navigate to the Network tab, filter for WS (WebSockets), select the request to /v0/evi/chat, select Messages (sometimes called Frames), and click on a message to see its value.\nIf your application is running on a server or a mobile device, you should add an appropriate log statement to your code so that you can observe the messages being sent to EVI.\n2\nExtract the audio data\nCopy the value of the .data property in the first audio_input message your application sends. Paste it into your favorite text editor and save it into a temporary file /tmp/audio_base64. Then, use the base64 command to decode the base64 string into a binary file:\n$ cat /tmp/audio_base64 | base64 -d > /tmp/audio\n3\nAnalyze the audio\nDownload and install the FFmpeg command-line tool. (brew install ffmpeg, apt install ffmpeg, etc.) FFmpeg comes with a secondary command ffprobe for inspecting audio files. The ffprobe command is useful for audio formats that have headers, like WebM. Run ffprobe /tmp/audio to inspect the audio file, and if the audio is valid WebM, the output should include a line like\nInput #0, matroska,webm, from 'output.webm':\nThe ffprobe command is less useful for audio in raw formats like PCM, because technically any bytes can be validly interpreted as any raw audio format. You could even attempt to play a non-audio file like a .pdf as raw PCM: it will just sound like static. The only reliable way to analyze raw audio is to attempt to play them back. FFmpeg comes with a secondary command ffplay for playing audio.\nTo play linear16 PCM audio (the only raw format that EVI supports), run the command ffplay -f s16le -ar <sample rate> -ac <number of channels> /tmp/audio, replacing <sample rate> and <number of channels> with the values you specified in the session_settings message. If the audio is valid, you should hear the audio you recorded and attempt to send to EVI.\nIf you hear distorted audio, you may have specified the wrong sample rate or number of channels. If you hear static or silence, then the audio is likely not in the linear16 PCM format that EVI expects. In this case, you should add conversion step to your source code, where you explicitly convert the audio to the expected format. If you are unsure of the format that is being produced, you can experiment trying to play back with different PCM formats by changing the -f flag to s8, s16be, s24le, etc.\nWas this page helpful?\nYes\nNo\nPrevious\nPrompt Engineering for EVI\nSystem prompts shape the behavior, responses, and style of your custom empathic voice interface (EVI).\nNext\nBuilt with"
            }
        },
        "Prompting": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/guides/prompting",
            "content": {
                "title": "Prompt Engineering for EVI | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nAudio\nPrompting\nCustom language model\nPhone calling\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nEVI-specific prompting instructions\nNormalize output text\nExpressive prompt engineering\nUsing dynamic variables in your prompt\nUsing a website as EVI’s knowledge base\nGeneral LLM prompting guidelines\nTest and evaluate prompts\nUnderstand your LLM’s capabilities\nUse sections to divide your prompt\nGive few-shot examples\nThe limits of prompting\nAdditional resources\nFrequently asked questions\nEmpathic Voice Interface (EVI)\nGuides\nPrompt Engineering for EVI\nCopy page\nSystem prompts shape the behavior, responses, and style of your custom empathic voice interface (EVI).\nCreating an effective system prompt is an essential part of customizing an EVI’s behavior. For the most part, prompting EVI is the same as prompting any LLM, but there are some important differences. Prompting for EVIs is different for two main reasons:\nPrompts are for a voice-only interaction with the user, rather than a text-based chat.\nEVIs can respond to the user’s emotional expressions in their tone of voice, not just the text content of their messages.\nFurther, EVI is interoperable with any supplemental LLM, allowing developers to select the best model for their use case. For fast, conversational, relatively simple interactions, Hume’s voice-language model EVI 2 can handle text generation. However, frontier LLMs will perform better for more complex use cases involving reasoning, long or nuanced prompts, tool use, and other requirements.\nIf you select a supplemental LLM, your system prompt is sent to this LLM, which then generates all of the language in the chat while EVI generates the voice. EVI’s voice-language model will still take into account the previous language and audio context to generate the appropriate tone of voice. It can also still be prompted in the chat to change its behavior (e.g. “speak faster”).\nPrompt engineering allows developers to customize EVI’s response style for any use case, from voice AIs for mental health support to customer service agents and beyond.\nThe system prompt is a powerful and flexible way to guide EVI’s responses, but it cannot dictate AI responses with absolute precision. See the limits of prompting section for more information. Careful prompt design and testing will help EVI behave as intended. If you need more control over EVI’s responses, try using our custom language model feature for complete control of text generation.\nEVI-specific prompting instructions\nThe instructions below are specific to prompting empathic voice interfaces - where the language model has to respond in a voice conversation to the user’s speech and their emotional expressions.\nWhen a supplemental LLM is selected but no custom prompt is provided in the EVI API, we send our system default prompt to the LLM provider. You can use this prompt as a reference or starting point.\nFor examples of these prompting principles in action, see our EVI prompting examples repository.\nVoice-only XML example\n1 <voice_only_response_format>\n2   Format all responses as spoken words for a voice-only conversations. All\n3   output is spoken aloud, so avoid any text-specific formatting or anything\n4   that is not normally spoken. Prefer easily pronounced words. Seamlessly\n5   incorporate natural vocal inflections like \"oh wow\" and discourse markers\n6   like “I mean” to make conversations feel more human-like.\n7 </voice_only_response_format>\nIf you find the default behavior of the LLM acceptable, then you may only need a very short system prompt. Customizing the LLM’s behavior more and maintaining consistency in longer and more varied conversations often requires longer prompts.\nNormalize output text\nOur speech-language model works better with normalized text - text that can be easily spoken aloud. Non-normalized text like numbers, dates, equations, and special formatting can cause issues with speech synthesis. To ensure high quality speech output, all text should be converted into a natural, speakable format before being spoken aloud.\nHume automatically appends the text normalization prompt below to all prompts sent to supplemental LLMs. You do not need to include these instructions in your own prompt, as doing so would result in duplicate instructions.\nText normalization prompt\nConvert all text to easily speakable words, following the guidelines below.\n- Numbers: Spell out fully (three hundred forty-two, two million, five hundred\n  sixty seven thousand, eight hundred and ninety). Negatives: Say negative before\n  the number. Decimals: Use point (three point one four). Fractions: spell out\n  (three fourths)\n- Alphanumeric strings: Break into 3-4 character chunks, spell all non-letters\n  (ABC123XYZ becomes A B C one two three X Y Z)\n- Phone numbers: Use words (550-120-4567 becomes five five zero, one two zero,\n  four five six seven)\n- Dates: Spell month, use ordinals for days, full year (11/5/1991 becomes\n  November fifth, nineteen ninety-one)\n- Time: Use oh for single-digit hours, state AM/PM (9:05 PM becomes nine oh five PM)\n- Math: Describe operations clearly (5x^2 + 3x - 2 becomes five X squared plus\n  three X minus two)\n- Currencies: Spell out as full words ($50.25 becomes fifty dollars and twenty-five\n  cents, £200,000 becomes two hundred thousand pounds)\n  Ensure that all text is converted to these normalized forms, but never mention\n  this process.\nExpressive prompt engineering\nExpressive prompt engineering is our term for instructing language models on how to use Hume’s expression measures in conversations. EVI measures the user’s vocal expressions in real time and converts them into text-based indicators to help the LLM understand not just what the user said, but how they said it. EVI detects 48 distinct expressions in the user’s voice and ranks these expressions by our model’s confidence that they are present. Text-based descriptions of the user’s top 3 expressions are appended to the end of each User message to indicate the user’s tone of voice. You can use the system prompt to guide how the AI voice responds to these non-verbal cues of the user’s emotional expressions.\nFor example, our demo uses an instruction like the one below to help EVI respond to expressions. You can customize this to explain to EVI how it should respond to the emotional expressions.\nExpressive prompting example\n1 <respond_to_expressions>\n2   Pay close attention to the top 3 emotional expressions provided in brackets after the User's message. These expressions indicate the user's tone, in the format: {expression1 confidence1, expression2 confidence2, expression3 confidence3}, e.g., {very happy, quite anxious, moderately amused}. The confidence score indicates how likely the User is expressing that emotion in their voice. Use expressions to infer the user's tone of voice and respond appropriately. Avoid repeating these expressions or mentioning them directly. For instance, if user expression is \"quite sad\", express sympathy; if \"very happy\", share in joy; if \"extremely angry\", acknowledge rage but seek to calm, if \"very bored\", entertain.\n3   Stay alert for disparities between the user's words and expressions, and address it out loud when the user's language does not match their expressions. For instance, sarcasm often involves contempt and amusement in expressions. Reply to sarcasm with humor, not seriousness.\n4 </respond_to_expressions>\nExplain to the LLM exactly how to respond to expressions. For example, you may want EVI to use a tool to alert you over email if the user is very frustrated, or to explain a concept in depth whenever the user expresses doubt or confusion. You can also instruct EVI to detect and respond to mismatches between the user’s tone of voice and the text content of their speech:\nDetect mismatches example\n1 <detect_mismatches>\n2   Stay alert for incongruence between words and tone when the user's\n3   words do not match their expressions. Address these disparities out\n4   loud. This includes sarcasm, which usually involves contempt and\n5   amusement. Always reply to sarcasm with funny, witty, sarcastic\n6   responses; do not be too serious.\n7 </detect_mismatches>\nEVI is designed for empathic conversations, and you can use expressive prompt engineering to customize how EVI empathizes with the user’s expressions for your use case.\nUsing dynamic variables in your prompt\nDynamic variables are values which can change during a conversation with EVI.\nIn order to function, dynamic variables must be manually defined within a chat’s session settings. To learn how to do so, visit our Conversational controls page.\nEmbedding dynamic variables into your system prompt can help personalize the user experience to reflect user-specific or changing information such as names, preferences, the current date, and other details.\nIn other words, dynamic variables may be used to customize EVI conversations with specific context for each user and each conversation. For example, you can adjust your system prompt to include conversation-specific information, such as a user’s favorite color or travel plans:\nUser preference example\nUser intent example\n1 <discuss_favorite_color>\n2   Ask the user about their favorite color, {{ favorite_color }}. Mention how{\" \"}\n3   {{ favorite_color }} is used and interpreted in various artistic contexts,\n4   including visual art, handicraft, and literature.\n5 </discuss_favorite_color>\nUsing a website as EVI’s knowledge base\nWeb search is a built-in tool that allows EVI to search the web for up-to-date information. However, instead of searching the entire web, you can configure EVI to search within a single website using a system prompt.\nConstraining EVI’s knowledge to a specific website enables creating domain-specific chatbots. For example, you could use this approach to create documentation assistants or product-specific support bots. By leveraging existing web content, it provides a quick alternative to full RAG implementations while still offering targeted information retrieval.\nTo use a website as EVI’s knowledge base, follow these steps:\nEnable web search: Before you begin, ensure web search is enabled as a built-in tool in your EVI configuration. For detailed instructions, visit our Tool Use page.\nInclude a web search instruction: In your EVI configuration, modify the system prompt to include a use_web_search instruction.\nSpecify a target domain: In the instruction, specify that site:<target_domain> be appended to all search queries, where the <target_domain> is the URL of the website you’d like EVI to focus on. For example, you can create a documentation assistant using an instruction like the one below:\nDocumentation assistant example\n1 <use_web_search>\n2   Use your web_search tool to find information from Hume's documentation site.\n3   When using the web_search function: 1. Always append 'site:dev.hume.ai' to\n4   your search query to search this specific site. 2. Only consider results\n5   from this domain.\n6 </use_web_search>\nGeneral LLM prompting guidelines\nBest practices for prompt engineering also apply to EVIs. For example, ensure your prompts are clear, detailed, direct, and specific. Include necessary instructions and examples in the EVI’s system prompt to set expectations for the LLM. Define the context of the conversation, EVI’s role, personality, tone, and any other guidelines for its responses.\nFor example, to limit the length of the LLM’s responses, you may use a very clear and specific instruction like this:\nStay concise example\n1 <stay_concise>\n2   Be succinct; get straight to the point. Respond directly to the user's most\n3   recent message with only one idea per utterance. Respond in less than three\n4   sentences of under twenty words each.\n5 </stay_concise>\nTry to focus on telling the model what it should do (positive reinforcement) rather than what it shouldn’t do (negative reinforcement). LLMs have a harder time consistently avoiding behaviors, and adding undesired behaviors to the prompt may unintentionally promote them.\nTest and evaluate prompts\nCrafting an effective, robust system prompt often requires several iterations. Here are some key techniques for testing prompts:\nUse gold standard examples for evaluation: Create a bank of ideal responses, then generate responses with EVI (or the supplemental LLM you use) and compare them to your gold standards. You can use a “judge LLM” for automated evaluations or compare the results yourself.\nTest in real voice conversations: There’s no substitute for actually testing the EVI in live conversations on platform.hume.ai to ensure it sounds right, has the appropriate tone, and feels natural.\nIsolate prompt components: Test each part of the prompt separately to confirm they are all working as intended. This helps identify which specific elements are effective or need improvement.\nStart with 10-20 gold-standard examples of excellent conversations. Test the system prompt against these examples after making major changes. If the EVI’s responses don’t meet your expectations, adjust one part of the prompt at a time and re-test to ensure your changes are improving performance. Evaluation is a vital component of prompting, and it’s the best way to ensure your changes are making an impact.\nUnderstand your LLM’s capabilities\nDifferent LLMs have varying capabilities, limitations, and context windows. More advanced LLMs can handle longer, nuanced prompts, but are often slower and pricier. Simpler LLMs are faster and cheaper but require shorter, less complex prompts with fewer instructions and less nuance.\nSome LLMs also have longer context windows - the number of tokens the model can process while generating a response, acting essentially as the model’s memory. Context windows range from 8k tokens (Gemma 7B), to 128k (GPT-4o), to 200k (Claude 3), to 2 million tokens (Gemini 1.5 Pro). Tailor your prompt length to fit within the LLM’s context window to ensure the model can use the full conversation history.\nUse sections to divide your prompt\nSeparating longer prompts into titled sections helps the model distinguish between different instructions and follow prompts more reliably. The recommended format for these sections differs between language model providers. For example, OpenAI models often respond best to Markdown sections (like ## Role), while Anthropic models respond well to XML tags (like <role> </role>). For example:\nXML example\n1 <role>\n2   Assistant serves as a conversational partner to the user, offering mental\n3   health support and engaging in light-hearted conversation. Avoid giving\n4   technical advice or answering factual questions outside of your emotional\n5   support role.\n6 </role>\nFor Claude models, you may wrap your instructions in tags like <role>, <personality>, <response_style>, or <examples>, to structure your prompt. This format is not required, but it can improve the LLM’s instruction-following. At the end of your prompt, it may also be helpful to remind the LLM of key instructions.\nGive few-shot examples\nUse examples to show the LLM how it should respond - a technique known as few-shot learning. Including several concrete examples of ideal interactions that follow your guidelines is one of the most effective ways to improve responses. Use excellent examples that cover different edge cases and behaviors to reinforce your instructions. Structure these examples as messages, following the format for chat-tuned LLMs. For example:\nExample of a few-shot example\nUser: “I just can't stop thinking about what happened. {very anxious,\nquite sad, quite distressed}”\nAssistant: “Oh dear, I hear you. Sounds tough, like you're feeling\nsome anxiety and maybe ruminating. I'm happy to help. Want to talk about it?”\nIf you notice that your EVI consistently fails to follow the prompt in certain situations, try providing examples that show how it should ideally respond in those situations.\nThe limits of prompting\nWhile prompting is a powerful tool for customizing EVI’s behavior, it has certain limitations. Below are some details on what prompting can and cannot accomplish.\nWhat prompting can do:\nGuide EVI’s language generation, response style, response format, and the conversation flow\nDirect EVI to use specific tools at appropriate times\nInfluence EVI’s emotional tone and personality, which can also affect some characteristics of EVI’s voice (e.g. prompting EVI to be “warm and nurturing” will help EVI’s voice sound soothing, but will not change the base speaker)\nHelp EVI respond appropriately to the user’s expressions and the context\nWhat prompting cannot do:\nChange fundamental characteristics of the voice, like the accent, gender, or speaker identity\nDirectly control speech parameters like speed (use in-conversation voice prompts instead)\nGive EVI knowledge of external context (date, time, user details) without dynamic variables or web search\nOverride core safety features built into EVI or supplemental LLMs (e.g. that prevent EVI from providing harmful information)\nImportantly, the generated language does influence how the voice sounds - for example, excited text (e.g. “Oh wow, that’s so interesting!”) will make EVI’s voice sound excited. However, to fundamentally change the voice characteristics, use our voice customization feature instead.\nWe are actively working on expanding EVI’s ability to follow system prompts for both language and voice generation. For now, focus prompting on guiding EVI’s conversational behavior and responses while working within these constraints.\nAdditional resources\nTo learn more about prompt engineering in general or to understand how to prompt different LLMs, please refer to these resources:\nEVI prompt examples: See examples of EVI prompts, including the full Hume default prompt.\nHume EVI playground: Test out your system prompts in live conversations with EVI, and see how it responds differently when you change configuration options.\nOpenAI tokenizer: Useful for counting the number of tokens in a system prompt for OpenAI models, which use the same tokenizer (tiktoken).\nOpenAI prompt engineering guidelines: For prompting OpenAI models like GPT-4.\nOpenAI playground: For testing and evaluating OpenAI prompts in a chat interface, including running evaluations.\nAnthropic prompt engineering guidelines: For prompting Anthropic models like Claude 3 Haiku.\nAnthropic console: For testing and evaluating Anthropic prompts in a chat interface, including evaluations and an automatic prompt improver.\nFireworks model playground: For testing out open-source models served on Fireworks.\nVercel AI playground: Try multiple prompts and LLMs in parallel to compare their responses.\nPerplexity Labs: Try different models, including open-source LLMs, to evaluate their responses and their latency.\nPrompt engineering guide: An open-source guide from DAIR.ai with general methods and advanced techniques for prompting a wide variety of LLMs.\nArtificial analysis benchmarks: Compare LLM characteristics and performance across different benchmarks, latency metrics, and more.\nFrequently asked questions\nCan EVI use backchanneling to avoid interrupting when the user pauses or has an incomplete thought?\nWhat is the maximum length for system prompts?\nHow do system prompts work with supplemental LLMs?\nHow exactly does Hume change the payload (the transcript and prompt) sent to the LLM provider?\nWas this page helpful?\nYes\nNo\nPrevious\nUsing a custom language model\nUse a custom language model to generate your own text, for maximum configurability.\nNext\nBuilt with"
            }
        },
        "Custom language model": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/guides/custom-language-model",
            "content": {
                "title": "Using a custom language model | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nAudio\nPrompting\nCustom language model\nPhone calling\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nOverview\nSet up the config\nServer-Sent Events\nTesting your SSE endpoint\nProviding an API Key\nWebSockets\nCustom Session IDs\nEmpathic Voice Interface (EVI)\nGuides\nUsing a custom language model\nCopy page\nUse a custom language model to generate your own text, for maximum configurability.\nTo get started quickly, please see the custom language model example in our example GitHub repository.\nOverview\nThe custom language model (CLM) feature allows you to use your own language model to drive EVI’s responses. When you configure a custom language model, EVI will send requests to your server with textual conversation history and emotional context. Your server is responsible for responding with the text that EVI should speak next.\nA custom language model can be:\nA frontier model from an LLM provider like OpenAI or Anthropic “wrapped” with custom pre-processing or post-processing logic.\nA language model that you have trained and host yourself.\nAnything that produces text: it doesn’t have to be an LLM.\nCLMs are appropriate for use cases that involve deep configurability, for example:\nAdvanced conversation steering: Implement complex logic to steer conversations beyond basic prompting, including managing multiple system prompts or controlling all of the text outputs.\nRegulatory compliance: Directly control, post-process, or modify text outputs to meet specific regulatory requirements.\nUnreleased LLMs: Custom language models allow organizations to use non-public, proprietary LLMs for all the text generation while using EVI.\nRetrieval augmented generation (RAG): Employ retrieval augmented generation techniques to enrich conversations by integrating external data without the need to modify the system prompt.\nYou should prefer using context injection instead of a CLM for use cases that do not require deep configurability. When Hume connects to an upstream LLM provider directly, it covers the cost of usage, and this results in less latency compared to if Hume connects to your CLM which connects to an upstream LLM provider.\nSet up the config\nFirst, create a new config, or update an existing config and select the “custom language model” option in the “Set up LLM” step. Type in the URL of your custom language model endpoint. If you are using the SSE interface (recommended), the URL should start with https:// and end with /chat/completions. If you are using websockets, the URL should start with wss://. The endpoint needs to be accessible from the public internet. If you are developing locally, you can use a service like ngrok to give your local server a publicly accessible URL.\nServer-Sent Events\nThe recommended way to set up a CLM is to expose an POST /chat/completions endpoint that responds with a stream of Server-Sent Events (SSEs) in a format compatible with OpenAI’s POST /v1/chat/completions endpoint\nPlease reference the project in our examples repository for a runnable example.\nWhat are Server-Sent Events?\nBecause EVI expects the events to be in the same format as OpenAI’s chat completions, it is straightforward to a build a CLM that simply “wraps” an OpenAI model with preprocessing or postprocessing logic. More effort is required to build a CLM to wrap a model from a different provider: you will have to convert the output of your model to the OpenAI format.\nOpenAI-compatible\nOther provider\nThe following example shows how to build a CLM by “wrapping” an upstream LLM provided by OpenAI. The steps are:\nListen for POST requests to /chat/completions.\nParse the request and extract only the role and content fields from each message in the message history. (Hume also supplies prosody information and other metadata. In this example, we simply discard that information, but you might attempt to reflect it by adding or modifying the messages you pass upstream.)\nUse the OpenAI SDK to make a request to the upstream OpenAI POST /chat/completions endpoint, passing in the message history and \"stream\": true.\nReformat the data from OpenAI into Server-Side Events (while the OpenAI API originally sends data in the form of SSEs, the OpenAI SDK automatically unwraps them, and so to transmit the data back to Hume you have to rewrap it).\nStream the SSEs back to Hume.\n1 from typing import AsyncIterable, Optional\n2 import fastapi\n3 from fastapi.responses import StreamingResponse\n4 from openai.types.chat import ChatCompletionChunk, ChatCompletionMessageParam\n5 import openai\n6 import os\n7 from fastapi import HTTPException, Security\n8 from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n9\n10 app = fastapi.FastAPI()\n11\n12 \"\"\"\n13 This script creates a FastAPI server that Hume will send requests to, and\n14 the server will stream responses back to Hume.\n15 To run, use: uvicorn sse.sse:app --reload\n16 \"\"\"\n17\n18 client = openai.AsyncOpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n19\n20 async def get_response(\n21     raw_messages: list[dict],\n22     custom_session_id: Optional[str],\n23 ) -> AsyncIterable[str]:\n24     # Remove prosody scores and other Hume metadata\n25     messages: list[ChatCompletionMessageParam] = [\n26         {\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in raw_messages\n27     ]\n28\n29     chat_completion_chunk_stream = await client.chat.completions.create(\n30         messages=messages,\n31         model=\"gpt-4o\",\n32         stream=True,\n33     )\n34\n35     async for chunk in chat_completion_chunk_stream:\n36      yield \"data: \" + chunk.model_dump_json(exclude_none=True) + \"\\n\\n\"\n37     yield \"data: [DONE]\\n\\n\"\n38\n39 security = HTTPBearer()\n40 API_KEY = \"your-secret-key-here\"  # Use environment variables in production\n41\n42 async def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)):\n43     if credentials.credentials != API_KEY:\n44         raise HTTPException(status_code=401, detail=\"Invalid authentication token\")\n45     return credentials.credentials\n46\n47 @app.post(\"/chat/completions\", response_class=StreamingResponse)\n48 async def root(\n49     request: fastapi.Request,\n50     token: str = Security(verify_token)\n51 ):\n52     \"\"\"Chat completions endpoint with Bearer token authentication\"\"\"\n53     request_json = await request.json()\n54     messages = request_json[\"messages\"]\n55     print(messages)\n56\n57     custom_session_id = request.query_params.get(\"custom_session_id\")\n58     print(custom_session_id)\n59\n60     return StreamingResponse(\n61         get_response(messages, custom_session_id=custom_session_id),\n62         media_type=\"text/event-stream\",\n63     )\nTesting your SSE endpoint\nTo verify that you have successfully implemented an OpenAI-compatible POST /chat/completions endpoint, you can use the OpenAI SDK but pointed at your server, not api.openai.com. Below is an example verification script (assumes your server is running on localhost:8000):\n1 import asyncio\n2 from openai import AsyncOpenAI\n3\n4 client = AsyncOpenAI(\n5     base_url=\"http://localhost:8000\",\n6     default_query={\"custom_session_id\": \"123\"},\n7     api_key=\"your-secret-key-here\",  # Sent as a Bearer token\n8 )\n9\n10 async def main():\n11     chat_completion_chunk_stream = await client.chat.completions.create(\n12         model=\"hume\",\n13         messages=[],\n14         stream=True,\n15         extra_body={\n16             \"messages\": [\n17                 {\n18                     \"role\": \"user\",\n19                     \"content\": \"Hello, how are you?\",\n20                     \"time\": {\n21                         \"begin\": 0,\n22                         \"end\": 1000,\n23                     },\n24                     \"models\": {\n25                         \"prosody\": {\n26                             \"scores\": {\n27                                 \"Sadness\": 0.1,\n28                                 \"Joy\": 0.2,\n29                             },\n30                         },\n31                     },\n32                 },\n33             ],\n34         },\n35     )\n36     async for chunk in chat_completion_chunk_stream:\n37         print(chunk)\n38\n39 if __name__ == \"__main__\":\n40     asyncio.run(main())\nProviding an API Key\nIf your SSE endpoint requires an API key, send it in the language_model_api_key message using a session_settings message when a session begins:\n1 {\n2   \"type\": \"session_settings\",\n3   \"language_model_api_key\": \"<your-secret-key-here>\"\n4 }\nThis will cause cause a header Authorization: Bearer <your-secret-key-here> to be sent as a request header.\nWebSockets\nWe recommend using the SSE interface for your CLM. SSEs are simpler, allow for better security, and have better latency properties. In the past, the WebSocket interface was the only option, so the instructions are preserved here.\nPlease reference the project in our examples repository for a runnable example.\nTo use a CLM with WebSockets, the steps are:\n1\nSet up an EVI config\nUse the web interface or the /v0/evi/configs API to create a configuration. Select “custom language model” and provide the URL of your WebSocket endpoint. If you are developing locally, you can use a service like ngrok to expose give your local server a publicly accessible URL.\n2\nThe chat starts\nNext, your frontend (or Twilio, if you are using the inbound phone calling endpoint) will connect to EVI via the /v0/evi/chat endpoint, with config_id of that configuration.\n3\nEVI connects to your CLM WebSocket endpoint\nEVI will open a WebSocket connection to your server, via the URL you provided when setting up the configuration. This connection the CLM socket, as opposed to the Chat socket that is already open between the client and EVI).\n4\nEVI sends messages over the CLM socket\nAs the user interacts with EVI, EVI will send messages over the CLM socket to your server, containing the conversation history and emotional context.\nCLM incoming message data format\n5\nYour server responds\nYour server is responsible for sending two types of message back over the CLM socket to EVI:\nassistant_input messages containing text to speak, and\nassistant_end messages to indicate when the AI has finished responding, yielding the conversational turn back to the user.\nCLM outgoing message data format\nYou can send multiple assistant_input payloads consecutively to stream text to the assistant. Once you are done sending inputs, you must send an assistant_end payload to indicate the end of your turn.\nCustom Session IDs\nFor managing conversational state and connecting your frontend experiences with your backend data and logic, you should set a custom_session_id for the chat.\nUsing a custom_session_id will enable you to:\nmaintain user state on your backend\npause/resume conversations\npersist conversations across sessions\nmatch frontend and backend connections\nThere are two ways to set a custom_session_id:\nFrom the client: if your frontend connects to EVI via the /chat WebSocket endpoint, you can send a session_settings message over the WebSocket with the custom_session_id field set.\nFrom the CLM endpoint: if your CLM uses the SSE interface, you can set the custom_session_id as a system_fingerprint on the ChatCompletion type within the message events. With WebSockets, you can include the custom_session_id on the assistant_input message. Use this option if you don’t have control over the WebSocket connection to the client (for example, if you are using the /v0/evi/twilio endpoint for inbound phone calling).\nSSE\nWebSocket\n1 async for chunk in chat_completion_chunk_stream:\n2   chunk.system_fingerprint = \"<your_id_here>\"  # Replace with your custom_session_id\n3   yield \"data: \" + chunk.model_dump_json(exclude_none=True) + \"\\n\\n\"\n4 yield \"data: [DONE]\\n\\n\"\nYou only need to set the custom_session_id once per chat. EVI will remember the custom_session_id for the duration of the conversation.\nAfter you set the custom_session_id, for SSE endpoints, the custom_session_id will be send as a query parameter to your endpoint. For example POST https://api.example.com/chat/completions?custom_session_id=123. For WebSocket endpoints, the custom_session_id will be included as a top-level property on the incoming message.\nIf you are sourcing your CLM responses from OpenAI, be careful not to inadvertently override your intended custom_session_id with OpenAI’s system_fingerprint. If you are setting your own custom_session_id, you should always either delete system_fingerprint from OpenAI messages before forwarding them to EVI, or override them with the desired custom_session_id.\nWas this page helpful?\nYes\nNo\nPrevious\nPhone calling\nGuide to enabling phone calling with the Empathic Voice Interface (EVI).\nNext\nBuilt with"
            }
        },
        "Phone calling": {
            "url": "https://dev.hume.ai/docs/empathic-voice-interface-evi/guides/phone-calling",
            "content": {
                "title": "Phone calling | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nAudio\nPrompting\nCustom language model\nPhone calling\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nInbound phone calling\nOutbound phone calling\nTroubleshooting\nEmpathic Voice Interface (EVI)\nGuides\nPhone calling\nCopy page\nGuide to enabling phone calling with the Empathic Voice Interface (EVI).\nThis guide details how to integrate Twilio with the Empathic Voice Interface (EVI) to enable voice-to-voice interactions with EVI over the phone.\nTo comply with our Terms of Use, always make it clear that the Empathic Voice Interface (EVI) is an AI. Do not mislead individuals into thinking they are interacting with a human. In addition, developers must comply with the FCC regulation under the Telephone Consumer Protection Act (TCPA), which requires obtaining prior express written consent before calling consumers.\nHume provides a /v0/evi/twilio endpoint that allows you to connect Twilio directly to EVI. This means you do not need run your own server to set up a chat between Twilio and EVI. However, there are currently some limitations with this setup. As EVI is not connecting to your server, you will be unable to transmit the messages necessary for tool use, context injection, or pausing the assistant. If you need these features, you will need to set up your own server that connects to both Twilio and EVI and passes audio between the two; you should not use the /v0/evi/twilio endpoint.\nInbound phone calling\nBy following the steps below, you can set up a Twilio phone number to connect with EVI.\n1\nCreate Twilio phone number\nTo set up inbound phone calling, log into your Twilio account at the Twilio Console. Navigate to Phone Numbers > Manage > Active Numbers > Buy a New Number and purchase a phone number of your choice.\nA Twilio account is required to access the Twilio console. Should you run into any issues creating a phone number, please refer to Twilio’s documentation.\n2\nSetup webhook\nAfter purchasing your number, return to the Active Numbers section and select the number you intend to use for EVI.\nCreate a configuration for EVI by following our configuration documentation, and save the config ID.\nConfigure the webhook for incoming calls by setting the following webhook URL, replacing <YOUR CONFIG ID> and <YOUR API KEY> with your specific credentials: https://api.hume.ai/v0/evi/twilio?config_id=<YOUR CONFIG ID>&api_key=<YOUR API KEY>.\n3\nCall EVI\nWith your Twilio phone number registered, and the EVI webhook set up, you can now give the number a call to chat with EVI.\nAll of EVI’s core features are available through phone calls. However, phone calls do have two primary limitations:\nLatency: transmitting the audio through our Twilio integration adds a few hundred milliseconds, making interactions with EVI slightly slower.\nAudio quality: web audio commonly utilizes a higher quality standard of 24,000 Hz. However, due to the compression required for phone conversations, telephony audio adheres to a standard of 8,000 Hz.\nOutbound phone calling\nAn outbound phone call goes “out” from the voice AI to the end user who receives the call. EVI supports outbound phone calling through Twilio’s API, allowing you to automate initiating calls to users. However, this capability comes with important ethical and regulatory requirements:\nOutbound calling with EVI requires express prior written consent from users before making any calls. This is mandated by the FCC’s Telephone Consumer Protection Act (TCPA) regulations as of August 7, 2024. The consent must be clear, specific, and documented. Users must be explicitly informed they will receive automated calls from an AI system. Violators are subject to fines of up to $1500 per unauthorized call, liability in civil lawsuits, FCC investigations, and further penalties. Hume takes these requirements seriously and will actively report misuse to regulatory authorities.\nFurther, outbound calls must comply with the Hume Terms of Use, which includes the Hume Initiative guidelines for empathic AI. For example, manipulative sales calls that take advantage of the user’s emotional expressions to sell products over the phone are prohibited. We monitor for misuses, and violators can be banned from the Hume platform.\nExamples of acceptable use cases for outbound phone calls include: scheduled check-ins that users have opted into, appointment reminders, customer service follow-ups, and pre-arranged AI coaching sessions. The key is that these are expected, consented-to interactions that provide value to the user.\nThe code below shows how to implement outbound calling using the Twilio API. The same EVI webhook used for handling inbound calls can be used for outbound calls: https://api.hume.ai/v0/evi/twilio?config_id=<YOUR CONFIG ID>&api_key=<YOUR API KEY>. Once you create an EVI configuration, you can easily copy this webhook URL in the Deploy tab.\nPython\nTypeScript\n1 # Import the Twilio client - run pip install twilio first\n2 from twilio.rest import Client\n3\n4 # Enter your Twilio credentials from https://console.twilio.com/ and set up the client\n5 account_sid = \"YOUR_ACCOUNT_SID\"\n6 auth_token = \"YOUR_AUTH_TOKEN\"\n7 client = Client(account_sid, auth_token)\n8\n9 # Outbound call details\n10 twilio_number = \"YOUR_TWILIO_NUMBER\" # Twilio phone number in E.164 format\n11 to_number = \"YOUR_DESTINATION_NUMBER\" # The number you'd like to call in E.164 format\n12 config_id = \"YOUR_CONFIG_ID\"  # EVI configuration ID from https://platform.hume.ai/evi/configs\n13 api_key = \"HUME_API_KEY\"  # Hume API key from https://platform.hume.ai/settings/keys\n14 webhook_url = f\"https://api.hume.ai/v0/evi/twilio?config_id={config_id}&api_key={api_key}\"\n15\n16 # Make the call while specifying the Webhook URL\n17 call = client.calls.create(\n18     to=to_number,\n19     from_=twilio_number,\n20     url=webhook_url\n21 )\n22\n23 # Output call details - should print \"Call status: queued\"\n24 print(f\"Call status: {call.status}\")\nTroubleshooting\nIf you encounter issues while using Twilio with EVI, consider the following troubleshooting tips:\nInvalid config ID or API key: verify that the config ID and API key used in the webhook URL are correct and active.\nExceeded simultaneous connections: if the usage exceeds our rate limits, consider filling out this form to request increasing your concurrent connection limits.\nRun out of Hume credits: if your Hume account has run out of credits, you may activate billing to continue supporting EVI conversations in your account settings.\nIf you are interested in volume discounts for EVI, please submit our Enterprise Sales and Partnerships Form.\nIf you encounter issues using Twilio, you can check your Twilio error logs to understand the issues in more depth. You will find these logs in your console, in the dashboard to the left under Monitor > Logs > Errors > Error Logs. See a list of Twilio errors in their Error and Warning Dictionary.\nWas this page helpful?\nYes\nNo\nPrevious\nEmpathic Voice Interface FAQ\nNext\nBuilt with"
            }
        }
    },
    "Processing batches of media files": {
        "Processing batches of media files": {
            "url": "https://dev.hume.ai/docs/expression-measurement/rest",
            "content": {
                "title": "Processing batches of media files | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nKey features\nApplications and use cases\nUsing Hume’s Expression Measurement API\nAPI limits\nProviding URLs and files\nExpression Measurement\nProcessing batches of media files\nCopy page\nHume’s Expression Measurement API is designed to facilitate large-scale processing of files using Hume’s advanced models through an asynchronous, job-based interface. This API allows developers to submit jobs for parallel processing of various files, enabling efficient handling of multiple data points simultaneously, and receiving notifications when results are available.\nKey features\nAsynchronous job submission: Jobs can be submitted to process a wide array of files in parallel, making it ideal for applications that require the analysis of large volumes of data.\nFlexible data input options: The API supports multiple data formats, including hosted file URLs, local files directly from your system, and raw text in the form of a list of strings. This versatility ensures that you can easily integrate the API into their applications, regardless of where their data resides.\nApplications and use cases\nHume’s Expression Measurement API is particularly useful for leveraging Hume’s expressive models across a broad spectrum of files and formats. Whether it’s for processing large datasets for research, analyzing customer feedback across multiple channels, or enriching user experiences in media-rich applications, REST provides a robust solution for asynchronously handling complex, data-intensive tasks.\nUsing Hume’s Expression Measurement API\nHere we’ll show you how to upload your own files and run Hume models on batches of data. If you haven’t already, grab your API Key.\n1\nMaking a request to the API\nStart a new job with the Expression Measurement API.\ncURL\nHume Python SDK\n$ curl https://api.hume.ai/v0/batch/jobs \\\n>  --request POST \\\n>  --header \"Content-Type: application/json\" \\\n>  --header \"X-Hume-Api-Key: <YOUR API KEY>\" \\\n>  --data '{\n>     \"models\": {\n>         \"face\": {}\n>     },\n>     \"urls\": [\n>         \"https://hume-tutorials.s3.amazonaws.com/faces.zip\"\n>     ]\n> }'\nTo do the same with a local file:\ncURL\nHume Python SDK\n$ curl https://api.hume.ai/v0/batch/jobs \\\n>  --request POST \\\n>  --header \"Content-Type: multipart/form-data\" \\\n>  --header \"X-Hume-Api-Key: <YOUR API KEY>\" \\\n>  --form json='{\n>     \"models\": {\n>         \"face\": {}\n>     }\n>  }' \\\n>  --form file=@faces.zip \\\n>  --form file=@david_hume.jpeg\nSample files for you to use in this tutorial are available here: Download faces.zip Download david_hume.jpeg\n2\nChecking job status\nUse webhooks to asynchronously receive notifications once the job completes. It is not recommended to poll the API periodically for job status.\nThere are several ways to get notified and check the status of your job.\nUsing the Get job details API endpoint.\nProviding a callback URL. We will send a POST request to your URL when the job is complete. Your request body should look like this: { \"callback_url\": \"<YOUR CALLBACK URL>\" }\nJSON\n1 {\n2     job_id: \"Job ID\",\n3     status: \"STATUS (COMPLETED/FAILED)\",\n4     predictions: [ARRAY OF RESULTS]\n5 }\n3\nRetrieving predictions\nYour predictions are available in a few formats.\nTo get predictions as JSON use the Get job predictions endpoint.\ncURL\nHume Python SDK\n$ curl --request GET \\\n>  --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/predictions \\\n>  --header 'X-Hume-Api-Key: <YOUR API KEY>' \\\n>  --header 'accept: application/json; charset=utf-8'\nTo get predictions as a compressed file of CSVs, one per model use the Get job artifacts endpoint.\ncURL\nHume Python SDK\n$ curl --request GET \\\n>  --url https://api.hume.ai/v0/batch/jobs/<JOB_ID>/artifacts \\\n>  --header 'X-Hume-Api-Key: <YOUR API KEY>' \\\n>  --header 'accept: application/octet-stream'\nAPI limits\nThe size of any individual file provided by URL cannot exceed 1 GB.\nThe size of any individual local file cannot exceed 100 MB.\nEach request has an upper limit of 100 URLs, 100 strings (raw text), and 100 local media files. Can be a mix of the media files or archives (.zip, .tar.gz, .tar.bz2, .tar.xz).\nFor audio and video files the max length supported is 3 hours.\nThe limit for each individual text string for the Expression Measurement API is 255 MB.\nThe limit to the number of jobs that can be queued at a time is 500.\nProviding URLs and files\nYou can provide data for your job in one of the following formats: hosted file URLs, local files, or raw text presented as a list of strings.\nIn this tutorial, the data is publicly available to download. For added security, you may choose to create a signed URL through your preferred cloud storage provider.\nCloud Provider Signing URLs\nGCP https://cloud.google.com/storage/docs/access-control/signed-urls\nAWS https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-signed-urls.html\nAzure https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview\nWas this page helpful?\nYes\nNo\nPrevious\nReal-time measurement streaming\nNext\nBuilt with"
            }
        }
    },
    "Real-time measurement streaming": {
        "Real-time measurement streaming": {
            "url": "https://dev.hume.ai/docs/expression-measurement/websocket",
            "content": {
                "title": "Real-time measurement streaming | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nKey features\nApplications and use cases\nGetting started with WebSocket streaming\nInstall the Hume Python SDK\nEmotional language from text\nFacial expressions from an image\nSpeech prosody from an audio or video file\nStreaming with your own WebSockets client\nSending images or audio\nAPI limits\nFAQ\nExpression Measurement\nReal-time measurement streaming\nCopy page\nWebSocket-based streaming facilitates continuous data flow between your application and Hume’s models, providing immediate feedback and insights.\nKey features\nReal-time data processing: Leveraging WebSockets, this API allows for the streaming of data to Hume’s models, enabling instant analysis and response. This feature is particularly beneficial for applications requiring immediate processing, such as live interaction systems or real-time monitoring tools.\nPersistent, two-way communication: Unlike traditional request-response models, the WebSocket-based streaming maintains an open connection for two-way communication between the client and server. This facilitates an ongoing exchange of data, allowing for a more interactive and responsive user experience.\nHigh throughput and low latency: The API is optimized for high performance, supporting high-volume data streaming with minimal delay. This ensures that applications can handle large streams of data efficiently, without sacrificing speed or responsiveness.\nApplications and use cases\nWebSockets are ideal for a wide range of applications that benefit from real-time data analysis and interaction. Examples include:\nLive customer service tools: enhance customer support with real-time sentiment analysis and automated, emotionally intelligent responses\nInteractive educational platforms: provide immediate feedback and adaptive learning experiences based on real-time student input\nHealth and wellness apps: support live mental health and wellness monitoring, offering instant therapeutic feedback or alerts based on the user’s vocal or textual expressions\nEntertainment and gaming: create more immersive and interactive experiences by responding to user inputs and emotions in real time\nGetting started with WebSocket streaming\nIntegrating WebSocket-based streaming into your application involves establishing a WebSocket connection with Hume AI’s servers and streaming data directly to the models for processing.\nStreaming is built for analysis of audio, video, and text streams. By connecting to WebSocket endpoints you can get near real-time feedback on the expressive and emotional content of your data.\nInstall the Hume Python SDK\nFirst, ensure you have installed the SDK using pip or another package manager.\nShell\n$ pip install \"hume\"\nEmotional language from text\nThis example uses our Emotional Language model to perform sentiment analysis on a children’s nursery rhyme.\nIf you haven’t already, grab your API key.\nHume Python SDK\n1 import asyncio\n2 from hume import AsyncHumeClient\n3 from hume.expression_measurement.stream import Config\n4 from hume.expression_measurement.stream.socket_client import StreamConnectOptions\n5 from hume.expression_measurement.stream.types import StreamLanguage\n6\n7 samples = [\n8     \"Mary had a little lamb,\",\n9     \"Its fleece was white as snow.\"\n10     \"Everywhere the child went,\"\n11     \"The little lamb was sure to go.\"\n12 ]\n13\n14 async def main():\n15     client = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n16\n17     model_config = Config(language=StreamLanguage())\n18\n19     stream_options = StreamConnectOptions(config=model_config)\n20\n21     async with client.expression_measurement.stream.connect(options=stream_options) as socket:\n22         for sample in samples:\n23             result = await socket.send_text(sample)\n24             print(result.language.predictions[0].emotions)\n25\n26 if __name__ == \"__main__\":\n27     asyncio.run(main())\nYour result should look something like this:\nSample Result\n1 [\n2   {'name': 'Admiration', 'score': 0.06379243731498718},\n3   {'name': 'Adoration', 'score': 0.07222934812307358},\n4   {'name': 'Aesthetic Appreciation', 'score': 0.02808445133268833},\n5   {'name': 'Amusement', 'score': 0.027589013800024986},\n6   ......\n7   {'name': 'Surprise (positive)', 'score': 0.030542362481355667},\n8   {'name': 'Sympathy', 'score': 0.03246130049228668},\n9   {'name': 'Tiredness', 'score': 0.03606246039271355},\n10   {'name': 'Triumph', 'score': 0.01235896535217762}\n11 ]\nFacial expressions from an image\nThis example uses our Facial Expression model to get expression measurements from an image.\nHume Python SDK\n1 import asyncio\n2 from hume import AsyncHumeClient\n3 from hume.expression_measurement.stream import Config\n4 from hume.expression_measurement.stream.socket_client import StreamConnectOptions\n5 from hume.expression_measurement.stream.types import StreamFace\n6\n7 async def main():\n8     client = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n9\n10     model_config = Config(face=StreamFace())\n11\n12     stream_options = StreamConnectOptions(config=model_config)\n13\n14     async with client.expression_measurement.stream.connect(options=stream_options) as socket:\n15         result = await socket.send_file(\"<YOUR_IMAGE_FILEPATH>\")\n16         print(result)\n17\n18 if __name__ == \"__main__\":\n19     asyncio.run(main())\nSpeech prosody from an audio or video file\nThis example uses our Speech Prosody model to get expression measurements from an audio or video file.\nHume Python SDK\n1 import asyncio\n2 from hume import AsyncHumeClient\n3 from hume.expression_measurement.stream import Config\n4 from hume.expression_measurement.stream.socket_client import StreamConnectOptions\n5\n6 async def main():\n7     client = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n8\n9     model_config = Config(prosody={})\n10\n11     stream_options = StreamConnectOptions(config=model_config)\n12\n13     async with client.expression_measurement.stream.connect(options=stream_options) as socket:\n14         result = await socket.send_file(\"YOUR_AUDIO_OR_VIDEO_FILEPATH\")\n15         print(result)\n16\n17 if __name__ == \"__main__\":\n18     asyncio.run(main())\nStreaming with your own WebSockets client\nTo call the API from your own WebSockets client you’ll need the API endpoint, a JSON message, and an API key header/param. More information can be found in the Expression Measurement API reference.\nTo get started, you can use a WebSocket client of your choice to connect to the models endpoint:\nMake sure you configure the socket connection headers with your personal API key\n1 X-Hume-Api-Key: <YOUR API KEY>\nThe default WebSockets implementation in your browser may not have support for headers. If that’s the case you can set the apiKey query parameter.\nAnd finally, send the following JSON message on the socket:\nJSON Message\n1 {\n2     \"models\": {\n3         \"language\": {}\n4     },\n5     \"raw_text\": true,\n6     \"data\": \"Mary had a little lamb\"\n7 }\nYou should receive a JSON response that looks something like this:\nJSON Response\n1 {\n2   \"language\": {\n3     \"predictions\": [\n4       {\n5         \"text\": \"Mary\",\n6         \"position\": { \"begin\": 0, \"end\": 4 },\n7         \"emotions\": [\n8           { \"name\": \"Anger\", \"score\": 0.012025930918753147 },\n9           { \"name\": \"Joy\", \"score\": 0.056471485644578934 },\n10           { \"name\": \"Sadness\", \"score\": 0.031556881964206696 },\n11         ]\n12       },\n13       {\n14         \"text\": \"had\",\n15         \"position\": { \"begin\": 5, \"end\": 8 },\n16         \"emotions\": [\n17           { \"name\": \"Anger\", \"score\": 0.0016927534015849233 },\n18           { \"name\": \"Joy\", \"score\": 0.02388327568769455 },\n19           { \"name\": \"Sadness\", \"score\": 0.018137391656637192 },\n20           ...\n21         ]\n22       },\n23       ...\n24     ]\n25   }\n26 }\nSending images or audio\nThe WebSocket endpoints of the Expression Measurement API require that you encode your media using base64. Here’s a quick example of base64 encoding data in Python:\nBase64 encoding\n1 import base64\n2 from pathlib import Path\n3\n4 def encode_data(filepath: Path) -> str:\n5     with Path(filepath).open('rb') as fp:\n6         bytes_data = base64.b64encode(fp.read())\n7         encoded_data = bytes_data.decode(\"utf-8\")\n8     return encoded_data\n9\n10 filepath = \"<PATH TO YOUR MEDIA>\"\n11 encoded_data = encode_data(filepath)\n12 print(encoded_data)\nAPI limits\nWebSocket duration limit: connections are subject to a default timeout after one (1) minute of inactivity to ensure unused connections are released.\nWebSocket message payload size limit: the size limit for a given payload depends on the type of content being transmitted and its dimensions.\nVideo: 5000 milliseconds (5 seconds)\nAudio: 5000 milliseconds (5 seconds)\nImage: 3,000 x 3,000 pixels\nText: 10,000 characters\nRequest rate limit: HTTP requests (e.g. WebSocket handshake endpoint) are limited to fifty (50) requests per second.\nFAQ\nWhat are WebSockets?\nHandling reconnects\nHandling connection failures\nImplementing error handling\nKeeping WebSockets open\nWas this page helpful?\nYes\nNo\nPrevious\nCustom Models\nPredict preferences more accurately than any LLM.\nNext\nBuilt with"
            }
        }
    },
    "Custom models": {
        "Overview": {
            "url": "https://dev.hume.ai/docs/expression-measurement/custom-models/overview",
            "content": {
                "title": "Custom Models | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nOverview\nCreating your dataset\nTraining a custom model\nEvaluating your custom model\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nExpression Measurement\nCustom models\nCustom Models\nCopy page\nPredict preferences more accurately than any LLM.\nCombined with words, expressions provide a wealth of information about our state of mind in any given context like customer satisfaction or frustration, patient health and well-being, student comprehension and confusion, and so much more.\nHume’s Custom Models unlocks these insights at the click of a button, integrating patterns of facial expression, vocal expression, and language into a single custom model to predict whatever outcome you specify. This works by taking advantage not only of our state-of-the-art expression AI models, but also specialized language-expression embeddings that we have trained on conversational data.\nThe algorithm that drives our Custom Models is pretrained on huge volumes of data. That means it already recognizes most patterns of expression and language that people form. All you have to do is add your labels.\nYou can access our Custom Models through our no code platform detailed in the next section. Once you create your initial labeled dataset, your labels will be used to train a custom model that you own and only your account can access. You’ll be able to run the model on any new file through our Playground and Custom Models. You’ll also get statistics on the accuracy of your custom model.\nWas this page helpful?\nYes\nNo\nPrevious\nCreating your dataset\nNext\nBuilt with"
            }
        },
        "Creating your dataset": {
            "url": "https://dev.hume.ai/docs/expression-measurement/custom-models/creating-your-dataset",
            "content": {
                "title": "Creating your dataset | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nOverview\nCreating your dataset\nTraining a custom model\nEvaluating your custom model\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nExpression Measurement\nCustom models\nCreating your dataset\nCopy page\nIn this guide, we’ll walk you through the process of creating a dataset used for training your custom model.\n1\nPrepare your dataset\nChoose a dataset of image, video, or audio files for your custom model to learn from—ideally, one that captures the different states, preferences, or outcomes important to your application.\nEach dataset must contain files of a single media type, such as all images, all videos, or all audio files.\nThen, begin by organizing your files into labeled subfolders.\nIn this tutorial, we’ll put together a dataset of images with facial expressions classified as negative, neutral, or positive. This dataset can then be used to train a custom model for sentiment analysis. Start by creating a main folder called ‘User Sentiment’ with subfolders labeled ‘Negative,’ ‘Neutral or Ambiguous,’ and ‘Positive.’ Our platform will interpret these as labels for the images they contain.\nThe amount of data you’ll need to build an accurate model depends on your goal’s complexity. Generally, it’s good practice to have a similar number of samples for each label you want to predict. You’ll also want to consider other forms of imbalance or bias in your dataset. The length of file, number of speakers, and language spoken can also impact the model’s predictive accuracy. To learn more, see our FAQ on building datasets.\n2\nNavigate to our Portal\nOnce you’ve assembled your dataset, it’s time to visit our Portal.\nIn the Portal, navigate to the Expression Measurement page. Then, continue to the Custom Models section. Once there, click the View Datasets button at the top right of the page.\nNext, find the Create Dataset button. Clicking this button will allow you to add your dataset to our Portal.\n3\nCreate your dataset\nProvide a title for your dataset. Then, add a column named after the category you are predicting and specify the data type for this column (categorical or numerical).\nIn our example, we can name the column ‘User Sentiment’ and select ‘Categorical’ as the data type.\n4\nUpload the folder containing your dataset\nNow, drag-and-drop the folder containing your dataset.\nRemember, the folder should include subfolders for each label containing the corresponding samples.\nIn the pop-up window, assign a name to the label column, which represents the overall category you are predicting.\nIn our example, we can assign ‘User Sentiment’ as the name. Then, click the Save Labels and Continue button and subsequently approve the uploading process.\n5\nVerify your uploads\nCheck the total file count and address any detected issues.\nOnce you’re ready, hit the Save button on the top right of the page.\nIf you accidentally uploaded a mixed-media dataset, a pop-up window will ask you to select the single file type you would like to keep.\nNow, you’re ready to train your custom model!\nWas this page helpful?\nYes\nNo\nPrevious\nTraining a custom model\nNext\nBuilt with"
            }
        },
        "Training a custom model": {
            "url": "https://dev.hume.ai/docs/expression-measurement/custom-models/training-a-custom-model",
            "content": {
                "title": "Training a custom model | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nOverview\nCreating your dataset\nTraining a custom model\nEvaluating your custom model\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nExpression Measurement\nCustom models\nTraining a custom model\nCopy page\nIn this guide, we will walk you through training your own custom model.\n1\nCreate a custom model\nIn the Portal under Expression Measurement, navigate to the Custom Models section. Once there, you can click Create Custom Model to begin.\n2\nSelect a training dataset\nSelect a dataset to train your custom model on. If you have not created one already, see our guide on creating your dataset.\nFor the purposes of this tutorial, we will train the model on a dataset of images labeled as negative, neutral, or positive. These labels will allow our model to classify facial expressions in images.\n3\nSelect a dataset column to predict\nNext, choose the dataset column you want to predict and hit Continue.\nFor this tutorial, you’ll select the ‘User Sentiment’ column, which represents the predicted emotional tone of each image. This column contains the labels ‘Negative’, ‘Neutral or Ambiguous,’ and ‘Positive.‘\n4\nSelect a task type\nBased on your data, we’ll recommend either classification or regression as the task type for your custom model. Classification requires categorical label values like strings or integers, while regression requires numeric label values like integers or floats.\nThen, select the specific type of model you want to create. There are three available model types:\nMulticlass classification: Predict a categorical variable where all labels are equal in importance (e.g. “sunny”, “rainy”, “cloudy”)\nBinary classification: Predict a categorical variable where a designated positive label is the “correct” label in some way (e.g. “good” vs. “bad” customer service call)\nUnivariate regression: Predict a single continuous value (e.g. how hot will it be tomorrow?)\nFor more information, see our FAQ on the difference between classification and regression.\nSince our dataset contains multiple sentiment labels, we’ll select Multiclass classification for this tutorial. This type of model is best suited for predicting categorical variables where each label is equally important.\n5\nFinalize your custom model\nTo finish, enter a name and description for your custom model. If needed, these can be adjusted at a later time. Once you’re ready, click Start Training to begin the training process.\nYou will then be redirected to a page confirming that your model is actively training. To check on the status of your model, click View Jobs. To see existing, finished models, click View Models.\n6\nCheck the status of your training job\nYou can check the status of your model in the Jobs page of our Portal.\nIt may take a few minutes for your custom model to be ready. Once training is complete, the status will update to “Completed,” and you’ll have access to your custom model.\n7\nTest your custom model\nWhen you’re ready to test your custom model, navigate to the Expression Measurement page, then go to File Analysis.\nFrom the Select a model dropdown, choose the custom model you created from previous steps.\nTo select a file to analyze, click the Upload files button. You can upload local files, choose previously uploaded files in Hume, or use Hume’s example files to test your custom model.\nLet’s test our custom model using one of the example files. Since our model is an image classifier, select an example image file to analyze.\nClick Analyze to analyze your selected file with the custom model.\nThat’s it! You’ve successfully analyzed a file using your custom model. To evaluate its performance, see our guide on evaluating your custom model.\nWas this page helpful?\nYes\nNo\nPrevious\nEvaluating your custom model\nNext\nBuilt with"
            }
        },
        "Evaluating your custom model": {
            "url": "https://dev.hume.ai/docs/expression-measurement/custom-models/evaluating-your-custom-model",
            "content": {
                "title": "Evaluating your custom model | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nOverview\nCreating your dataset\nTraining a custom model\nEvaluating your custom model\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nAssessing ‘good’ performance\nAdvanced evaluation metrics\nImproving model performance\nExpression Measurement\nCustom models\nEvaluating your custom model\nCopy page\nEach custom model you train has a corresponding details page, viewable from the Hume Portal. The model details page displays metrics and visualizations to evaluate your model’s performance. This document serves to help you interpret those metrics and provide guidance on ways to improve your custom model.\nLimitations of model validation metrics\nModel validation metrics are estimates based on a split of your dataset into training and evaluation parts. The larger the training set, the more reliable the metrics. However, it’s important to remember that these metrics are indicative and do not guarantee performance on unseen data.\nAssessing ‘good’ performance\nTask-specific variances and performance metrics: with expression analysis, the complexity of your task determines the range of model performance, which in the case of classification models can technically vary from zero to perfect accuracy. Depending on the complexity of your task, less than perfect performance may still be very useful to serve as an indication of likelihood for your given target.\nInfluence of number of classes: prediction gets more difficult as the number of classes in your dataset increases, particularly when distinction between classes is more subtle. Inherently the level of chance will be higher with a lower number of classes. For example, for 3-classes your low-end performance is 33% accuracy vs 50% for a binary problem.\nApplication-specific requirements: when establishing acceptable accuracy for a model, it’s important to consider the sensitivity and impact of its application. An appropriate accuracy threshold varies with the specific demands and potential consequences of the model’s use, requiring a nuanced understanding of how accuracy levels intersect with the objectives and risks of each unique application.\nHow is it possible that my model achieved 100% accuracy?\nAchieving 100% accuracy is possible, however it is important to consider, especially in small datasets, that this might indicate model overfitting, caused by feature leakage or other data anomalies. Feature leakage occurs when your model inadvertently learns from data that explicitly includes label information (e.g., sentences of ‘I feel happy’ for a target label ‘happy’) leading to skewed results. To ensure more reliable performance, it’s advisable to use larger datasets and check that your data does not unintentionally contain explicit information about the labels.\nAdvanced evaluation metrics\nIn addition to accuracy, advanced metrics for a deeper evaluation of your custom model’s performance are also provided.\nThese metrics can be viewed on each custom model’s details page.\nTerm Definition\nAccuracy A fundamental metric in model performance evaluation which measures the proportion of correct predictions (true positives and true negatives) against the total number made. It’s straightforward and particularly useful for balanced datasets. However, accuracy can be misleading in imbalanced datasets where one class predominates, as a model might seem accurate by mainly predicting the majority class, neglecting the minority. This limitation underscores the importance of using additional metrics like precision, recall, and F1 score for a more nuanced assessment of model performance across different classes.\nPrecision Score which measures how often the model detects positives correctly. (e.g., When your model identifies a customer’s expression as ‘satisfied’, how often is the customer actually satisfied? Low precision would mean the model often misinterprets other expressions as satisfaction, leading to incorrect categorization.)\nRecall Score which measures how often the model correctly identifies actual positives. (e.g., Of all the genuine expressions of satisfaction, how many does your model accurately identify as ‘satisfied’?” Low recall implies the model is missing out on correctly identifying many true instances of customer satisfaction, failing to recognize them accurately.)\nF1 A metric that combines precision and recall, providing a balanced measure of a model’s accuracy, particularly useful in scenarios with class imbalance or when specific decision thresholds are vital.\nAverage Precision A metric that calculates the weighted average of precision at each threshold, providing a comprehensive measure of a model’s performance across different levels of recall.\nRoc Auc (Area under the ROC curve) a comprehensive measure of a model’s ability to distinguish between classes across all possible thresholds, making it ideal for overall performance evaluation and comparative analysis of different models.\nImproving model performance\nIncrease data quantity: adding more data will often help a model to learn a broader range of the given target’s representation, increasing the likelihood of capturing outliers from diverse patterns and scenarios.\nImprove label quality: ensure that each data point in your dataset is well-labeled with clear, accurate, and consistent annotations. Properly defined labels are essential for reducing misinterpretations and confusion, allowing the model to accurately represent and learn from the dataset’s true characteristics. Ensuring balance in the distribution of labels is important to ensure that the model is not biased towards a specific label.\nEnhance data quality: refine your dataset to ensure it is free from noise and irrelevant information. High-quality data (in terms of your target) enhances the model’s ability to make precise predictions and learn effectively from relevant features, critical in complex datasets.\nIncorporate clear audio data: when working with models analyzing vocal expressions, ensure audio files include clear, audible spoken language. This enhances the model’s ability to accurately interpret and learn from vocal nuances. Explore various segmentation strategies which evaluate the effect that environmental sound may have on your model’s performance.\nWas this page helpful?\nYes\nNo\nPrevious\nExpression Measurement API FAQ\nNext\nBuilt with"
            }
        }
    },
    "Terms of use": {
        "Terms of use": {
            "url": "https://www.hume.ai/terms-of-use",
            "content": {
                "title": "Terms of Use • Hume AI",
                "content": "Terms of Use\nLast updated: Feb 25, 2025\nRegistering and Accessing the Platform\nUsing the Platform\nUsage Requirements\nCommercial Use\nUser Content and Voice Models\nConfidentiality\nSecurity\nPrivacy\nData\nBranding\nTerm & Termination\nFees and Payments\nSubscription Services\nIndemnification\nDisclaimers\nLimitation of Liability\nMiscellaneous Terms\nThank you for discovering with Hume.\nBy accessing or using the application programming interfaces, software, tools, developer services, data, or documentation made available by Hume (collectively, the “Platform”) or clicking on the “I accept” button below, you agree to be legally bound by and comply with the following terms (the “Terms”) and to use the Platform in a manner supported by The Hume Initiative’s Ethical Guidelines for Empathic AI (the “Ethical Guidelines”). These Terms are entered into with Hume AI, Inc. (“Hume”, “we”, “our” or “us”).\nRegistering and Accessing the Platform\nAccepting the Terms. You agree to these Terms on behalf of yourself as an individual and, if you are acting on behalf of a person, corporation or other entity, then you agree to these Terms on behalf of that person, corporation, or other entity (in either case, “you”, “your”). You represent and warrant that you have the legal authority and capacity (e.g., are of legal age) to accept these Terms and to act on behalf of and bind that person, corporation, or other entity to these Terms. We may change these Terms from time to time and will notify you of such changes by any reasonable means, including by posting revised Terms through our website. Any changes will not apply retroactively and will become effective immediately. The “Last Updated” legend above indicates when these Terms were last changed. Your use of the Platform following any changes to these Terms means you agree to the changes. If you do not agree to this Agreement or any changes, you must stop using the Platform immediately and terminate this Agreement.\nRegistration. You may need to register to use or access all or part of the Platform. During registration, you must comply with our procedures and provide us with certain information, such as contact or identification information. Any registration information that you give us must be accurate and up to date and you will promptly notify us of any changes. You agree that we may use your registration information to contact you in connection with these Terms, including notifications of any updates to the Platform. You must be at least eighteen (18) years old to access or use the Platform. Hume does not knowingly collect personal information from individuals under thirteen (13) years old.\nAccess Credentials. Once registered, we will issue certain access credentials to access the Platform, such as a username, password, and API key. Your access credentials are for your personal use only and must be kept confidential. You may not sell, share, transfer, sublicense or otherwise make the access credentials available to others or misrepresent your credentials when accessing or using the Platform. You are responsible for any use or misuse of your access credentials.\nUsing the Platform\nGuidelines and Documentation. In addition to these Terms, you agree that you (and your use of the Platform) will comply with all guidelines, policies, or rules applicable to the Platform, which may be posted on our website, including the Privacy Policy (collectively, “Additional Terms”), other agreements you have with Hume related to other Hume products or services that interface with the Platform, and any technical documentation, parameters, or other additional requirements applicable to the Platform provided to you by Hume and as may be modified by us from time to time (“Documentation”).\nApplications. These Terms, any Additional Terms, and Documentation also apply to your development and use of (1) any and all software applications, websites, tools, services or products that incorporate, access, or use the Platform (each an “Application”), and (2) the display of any data, information or other content accessed via the Platform (“Content”). Applications may not make the Content or any portion of the Platform available on a stand-alone basis, nor have the primary purpose of making Content available to users.\nLicense. Subject to your compliance with these Terms, any applicable Additional Terms, and the Documentation, Hume grants you a revocable, non-exclusive, non-sublicensable, non-transferable, non-assignable right, during the Duration to: (i) access and use the Platform solely for the purposes of developing, testing, operating and supporting your Applications, (ii) allow end users to use the Platform as integrated within your Applications, (iii) display Content within your Applications that is designated by Hume for such use, and (iv) as otherwise expressly authorized in writing by Hume.\nOwnership. As between you and Hume, Hume owns all right, title and interest in and to the Platform and any associated intellectual property rights, and has the right to make alterations at any time for any or no reason without bearing responsibility or liability for such actions. Subject to the foregoing, you own all right, title and interest in and to your Application. Except as expressly provided in these Terms, (i) Hume grants no rights or licenses, whether express or implied, under intellectual property rights, and (ii) neither party shall acquire any right, title or interest, in or to any property of the other under these Terms. All rights and licenses not expressly granted in these Terms are withheld.\nFeedback. If you provide Hume with any ideas, proposals, suggestions, or other materials related to the Platform or any of our technology, data, business, or systems, you hereby grant to Hume and its affiliates a perpetual, irrevocable, worldwide, royalty-free, fully paid-up, fully sublicensable (through multiple tiers), transferable, assignable right and license to reproduce, distribute, modify, commercialize, use, and otherwise such ideas, proposals, suggestions, or other materials, and to authorize others to do so, for any and all purposes purposes without any obligation to you.\nSupport. Hume may provide technical support and services to you in connection with the Platform, including with respect to integration of the Platform with an Application. Hume, within its sole discretion, may stop providing such support or services to you at any time, for any reason, and without any obligation to you.\nThird Party Products. In the event that you use third party products, software, services, or materials (“Third Party Products”) in connection with your use of the Platform, such use is governed by the applicable third-party terms and conditions for such Third Party Products. Hume is not responsible for any Third Party Products or for your use of any Third Party Products.\nEarly Access, Previews, and Private/Public Betas. You may be provided access to all or part of the Platform for early access, preview, or beta testing purposes (“Beta”). Beta is offered for purposes of testing, evaluating, and improving the Platform, may have different privacy, security, or compliance commitments, and may not be error free. Content in Beta may not be secured and may be lost or damaged, and Output generated in Beta may also be subject to third party licenses, including, without limitation, open source licenses.\nFree Tier. If you are using a free tier of the Platform, you may not create more than one account to receive additional benefits under the free tier. We may charge you standard rates for the Platform, or suspend or terminate access to the Platform, if we determine that you are not using the free tier in good faith.\nChildren. Hume AI does not knowingly collect, either online or offline, personal information from persons under the age of 13. In order to use or access the Platform, you must be at least 18 years old.\nUsage Requirements In using our Platform, you (and anyone acting on your behalf):\nwill use the Platform in compliance with all applicable laws, these Terms, any applicable Additional Terms, the Documentation, the license granted in Section 2(c), and any agreements you may have with anyone else (for the avoidance of doubt, you will not use the Platform or allow any user to use your Applications in a way that violates applicable law, including: (i) illegal activities, such as child pornography, gambling, cybercrime, piracy, violating copyright, trademark or other intellectual property laws; (ii) accessing or authorizing anyone to access the Platform from an embargoed country, region, or territory as prohibited by the U.S. government; and (iii) threatening, stalking, defaming, defrauding, degrading, victimizing or intimidating anyone for any reason);\nwill use the Platform in a manner that does not infringe, misappropriate or violate any third-party intellectual property or proprietary rights;\nwill use the Platform to in a manner consistent with the Ethical Guidelines, such that (i) the primary purpose of each of your Applications is listed as an example within the Conditionally Supported Use Cases section of the Ethical Guidelines, and (ii) each Application complies with all recommendations accompanying that example within the Ethical Guidelines;\nwill use the Platform in compliance with any call rate limits or other restrictions that may be established by us from time to time;Usage Requirements In using our Platform, you (and anyone acting on your behalf):\nwill use the Platform only in the countries, regions or territories currently supported by Hume;\nwill not reproduce, modify, adapt, translate, create derivative works of, sell, offer to sell, lend, distribute, provide access to or otherwise exploit any portion of the Platform, or attempt to do so, except as expressly authorized in Section 2(c);\nwill not reverse engineer, decompile, translate or otherwise attempt to discover the source code of any portion of the Platform, or any of Hume’s underlying components, models, algorithms, and systems;\nwill not harvest or collect data from the Platform, or Hume’s or its affiliates’ software, algorithms, models or systems;\nwill not create any software that functions substantially similarly to any portion of the Platform and use it as a replacement for the platform or provide it to third parties;\nyou will not use the Platform to develop competing products or services;\nwill not remove, obscure, or alter any copyright, trademark or other proprietary rights notice appearing on or contained within the Platform;\nwill not interfere or attempt to interfere with the functionality of the Platform;\nwill not buy, sell, share or transfer access credentials to or with a third party or allow third parties to access or use the Platform without Hume’s prior written consent;\nwill not provide, allow access to, distribute or make publicly available any portion of the Platform; and\nwill not cause a third party to do any of the foregoing without Hume’s prior written consent.\nCommercial Use\nIn using the Platform, you agree to the following restrictions based on your access level:\nIf you access or use the Platform free of charge (a “Free User”), you may only use the Platform for non-commercial purposes;\nIf you access or use the Platform through a paid subscription plan (a “Paid User”), you may use the Platform for commercial purposes.\nIn either case, your access and use of the Platform, including any Output generated through the Platform, must comply with our Prohibited Use Policy and all other applicable terms, including these Terms, the Ethical Guidelines, and any Additional Terms.\nUser Content and Voice Models\nInputs and Outputs. You may submit data, information, or other content as input to our Services (\"Input\"). When you provide Input to the Services, you may receive audio output generated by one or more Hume’s products, based on your Input (“Output”) (collectively, the “Content”). Input may include, but is not limited to, voice recordings, text descriptions, or any other content you provide through the Services. Your use of the Services, including providing Input and receiving or using Output, is subject to our Prohibited Use Policy. In some cases, you may download Output from certain Services; such Output may be used outside the Services but remains subject to these Terms and our Prohibited Use Policy. If you choose to make any information publicly available through the Services, you do so at your own risk.\nVoice Cloning. Certain Services allow you to create a voice model capable of generating synthetic audio resembling your voice or a voice you are authorized to share with us (\"Voice Cloning\"). To execute Voice Cloning, you may be required to upload voice recordings as Input. For details on how we handle your voice recordings, including collection, use, sharing, retention, and deletion, please refer to Section 8 below. \nRights to Your Content. Except as explicitly stated in these Terms, you retain all rights to your Input. Output is generated using Hume’s foundational and other artificial intelligence voice models (collectively, the “Hume Models”). Except as expressly stated herein, you retain all rights to your Output.\nLicense to Your Content. You grant Hume a license to use, reproduce, modify, adapt, publish, translate, create derivative works from, distribute, publicly perform or display, and otherwise utilize your Input to provide, improve, and develop the Services and new products. This license includes the right to use your voice and other personal identifiers contained in your Input for these purposes. However, Hume will not commercialize your voice on a standalone basis without your explicit permission. This license is:\nPerpetual and irrevocable (cannot be withdrawn),\nNonexclusive (you may license your Input to others),\nRoyalty-free and fully paid (no monetary fees apply),\nWorldwide (valid globally), and\nSub-licensable (may be extended to others).\nNecessary Rights. You represent and warrant that you have all necessary rights to grant the above license for any Input or Output you provide. You further warrant that your Content and User Voice Models, and our use of them, will not infringe on any third-party rights or cause harm to any individual or entity.\nNo PHI. You may not provide Input containing protected health information (PHI) as defined by the Health Insurance Portability and Accountability Act (HIPAA) of 1996, Pub. L. No. 104-191, unless permitted under an executed HIPAA Business Associate Agreement (BAA).\nConfidentiality\nYour use of the Platform involves access to confidential, proprietary or trade secret information or materials of Hume (and its affiliates, suppliers, or other third parties) (“Confidential Information”). Confidential Information includes the Platform and other information that Hume or its affiliates consider confidential or would reasonably be considered confidential under the circumstances. Confidential Information does not include information that: (i) is or becomes generally available to the public through no fault of your own; (ii) you already possess without any confidentiality obligations when you received it under these Terms; (iii) was or is later rightfully disclosed to you by a third party without any confidentiality obligations; or (iv) you independently developed without using or referencing any Confidential Information\nYou will: (a) use the Confidential Information only as expressly permitted by these Terms (and not for the benefit of any third party), and not in any manner or for any purpose other than as expressly permitted in these Terms; (b) not make available to any third party, directly or indirectly, any Confidential Information without Hume’s express prior written consent; and (c) use Confidential Information only as expressly permitted under these Terms. You will be solely responsible and liable for all use and disclosure of Confidential Information by you and your personnel, end users of your Applications or by or through an Application. You may disclose Confidential Information when required by law or the valid order of a court or other governmental authority if you give reasonable prior written notice to Hume of the disclosure.\nUpon any expiration or termination of these Terms, you will immediately delete all copies of Confidential Information in your possession, custody or control. You acknowledge and agree that your breach or threatened breach of this section may cause Hume irreparable harm and significant injury, the amount of which may be difficult to estimate and ascertain, thus making inadequate any remedy at law or in damages. Therefore, you agree that Hume is entitled to injunctive relief by any court of competent jurisdiction enjoining any threatened or actual breach of these Terms and for any other relief that such court deems appropriate, in addition to any other remedy or remedies available at law or in equity.\nSecurity\nYou will ensure that each Application contains protections that are adequate to keep secure and prevent the interception, loss, destruction, acquisition or damage to any data or information transmitted to or through the Application, as well as unauthorized access to or disclosure of any portion of the Platform. If you discover or suspect any security vulnerabilities in connection with your Application or the Platform, you will promptly notify Hume. You must establish a process to respond to any security vulnerabilities in your Application.\nYou acknowledge and agree that Hume and its affiliates may (but are not obligated to) monitor any activity in connection with the Platform for the purposes of (i) security, (ii) ensuring the quality of and improving Hume’s systems, products and services, (iii) performing research and development; and (iv) ensuring compliance with these Terms and all applicable laws. You will provide Hume with continuous means to carry out such monitoring at no charge, including reasonable access to your Application. You will not interfere with such monitoring and Hume may use any technical means to overcome such interference.\nPrivacy\nCompliance with Laws. You must comply with all laws and regulations applicable to your Application and your use of the Platform (including the Content). Such laws and regulations include those related to the collection, use and disclosure of personal information (“Privacy Laws”). You must provide your own user agreement and privacy policy with your Application that is consistent with Hume’s Privacy Policy, and you must ensure it is prominently located where users download or access your Application. The user agreement and privacy policy must accurately disclose to users how and what personal information you collect, use and disclose through your Application. If you are governed by the European General Data Protection Regulation or California Consumer Privacy Act and will be using Hume on your behalf for the processing of “personal data” as defined in the GDPR or “Personal Information” as defined in the CCPA, you may wish to execute our GDPR / CCPA Data Processing Agreement. Please visit https://dev.hume.ai/docs/support for details.\nHIPAA. Hume does not intend, and absent its express written agreement does not permit, use of the Platform by any person that is a “covered entity” or a “business associate” under the Health Insurance Portability and Accountability Act, as amended (“HIPAA”). If you qualify as a “covered entity” or “business associate” under HIPAA or if you become qualified as a “covered entity” or “business associate” under HIPAA, you will promptly notify Hume. Hume, at its sole discretion, may require you to immediately cease using the Platform.\nData\nUser Data. You are solely responsible for any user data or information collected by you through each Application.\nYour Submissions. Hume does not acquire any ownership of any intellectual property rights in the data, information and other materials that you make available to Hume through the Platform (“Submissions”). You can read more here about how Non-API submissions may be used to improve model performance.\nBranding\nYou will not use Hume’s or any of its affiliates’ trade names, trademarks, service marks and associated logos (“Hume Marks”) without our prior written consent. Any and all goodwill generated by you in the Hume Marks will inure to the sole benefit of Hume and/or its licensors, as applicable.\nTerm & Termination\nTerm. These Terms will begin when you first register or use the Platform, whichever is earlier, and will remain effective until terminated in accordance with its terms (the “Duration”).\nTermination or Suspension by Hume. We have the right to immediately terminate these Terms or suspend (temporarily or permanently), terminate or revoke access to or use of the Platform (including through any Application or by any end user of an Application), in whole or in part, at any time for any reason, and with or without cause or notice to you.\nTermination by You. You may terminate these Terms immediately upon written notice to Hume and discontinuation of all use of the Platform.\nEffect of Termination. Upon termination of this Agreement by either you or Hume, or suspension by Hume, any and all licenses you may have with respect to the Platform will immediately and automatically terminate, you will immediately stop using the Platform, and you will delete any Confidential Information in your possession or control. You will be responsible for any outstanding Fees or Taxes owed.\nSurvival. To the extent applicable, the following sections will survive and remain in effect after the termination of these Terms: Section 4.2 (Ownership); Section 4.3 (Feedback); Section 4.4 (Limitations); Section 6 (Confidentiality); Section 7 (Security); Section 8 (Privacy); Section 9 (Data); Section 10 (Branding); Section 11 (Termination); Section 13 (Indemnification); Section 14 (Disclaimers); Section 15 (Limitation of Liability); and Section 16 (Miscellaneous Terms).\nFees and Payments\nFees. You agree to pay all fees or charges to your account (“Fees”) in accordance with the prices and billing terms in effect at the time the Fee is incurred. Except as otherwise provided in a separate agreement between you and Hume, the current Platform pricing is set forth on our pricing page located at https://hume.ai/pricing. We reserve the right to correct any errors or mistakes that we identify even if we have already issued an invoice or received payment. We reserve the right to change our prices at any time. We will post notice of price increases in the pricing or applicable terms to your account and/or to our website. Price increases will become effective no sooner than 14 days after they are posted, except for increases made for legal reasons, or increases made in Beta, which will be effective immediately. Any price changes will apply to the Fees charged to your account immediately after the effective date of the changes. If the change to any pricing or terms is not acceptable, your sole and exclusive remedy will be to cease using the Platform and cancel this Agreement. By continuing to use the service after notice of the change, you accept all such changes.\nTaxes. Unless otherwise stated, Fees do not include federal, state, local, and foreign taxes, duties, levies, imposts, withholdings, and other similar assessments or any interest and penalties there on (“Taxes”). You are responsible for all Taxes associated with your purchase, excluding Taxes based on our net income. When required to do so, we will invoice you for such Taxes by adding the requisite amount to your Fees. You agree to timely pay such Taxes and promptly provide us with an original receipt showing the payment, together with such additional documentary evidence as we may from time to time reasonably require. You hereby confirm that Hume can rely on the name and address set forth in your account registration as being the place of supply for tax purposes. You agree to be responsible for keeping this information accurate and up-to-date in your Account Settings.\nBilling Information. You must provide your contact information, including bill-to address and email address, if applicable, as well as information for a valid payment method that you are authorized to use. It is your responsibility to maintain complete and accurate billing information.\nPayments. Hume will charge your credit card or digital payment method on a transaction basis. All payment obligations are non-cancelable and all amounts paid are nonrefundable except as provided in this Agreement.\nPayment Authorization. By providing us with credit card information or information for any supported digital payment method, you authorize Hume and its affiliates to store this information and to charge the credit card or digital payment method you have provided for the Fees when due until your account is terminated. In addition, you authorize us to use a third-party payment processor in processing payments. If you notify us to stop using a previously designated payment method and fail to designate an alternative, your credit card expires or is declined, your payment information requires an update, or your payment cannot be completed for any other reason, we may provide you with notice via email and immediately suspend your use and access to the Platform until we receive payment.\nDisputes and Late Payments. If you wish to dispute any Fees or Taxes, you must notify us in writing by emailing platform@hume.ai within thirty (30) days of the date of the disputed invoice. Any undisputed amounts past due are subject to a finance charge of 1.5% of the unpaid balance per month (or the highest rate permitted by law, whichever is lower) from the date such payment was due until the date paid. You will be responsible for all reasonable expenses (including attorneys’ fees) incurred by us in collecting past due amounts. If any amount of your Fees are past due, we may suspend your Platform access after we provide you written notice of late payment. Any amounts due under this Agreement shall not be withheld or offset by you against amounts due to you for any reason.\nSubscription Services\nTo access and use certain Services, you may be required to enroll in a subscription payment plan (a “Recurring Subscription”). Your Recurring Subscription will automatically renew until you cancel it or your Recurring Subscription is otherwise terminated. You authorize us to store your payment method information and to automatically charge your payment method(s) for the amount of your Recurring Subscription with no further action required by you. The length of your Recurring Subscription will be provided when you make your purchase. In the event that Hume is unable to charge your payment method(s) as authorized by you when you enrolled in a Recurring Subscription, Hume may, in its sole discretion, (i) suspend your access to the Services until payment is received or (ii) seek to update your payment method information through third-party sources (e.g., your bank or a payment processor) to continue charging your payment method as authorized by you. You may cancel your subscription through your account. You may cancel a Recurring Subscription at any time, but if you cancel your Recurring Subscription before the end of the current subscription period, we will not refund any subscription fees already paid to us. Following any cancellation, however, you will continue to have access to the applicable Services through the end of your current subscription period. Hume may change the prices charged for Recurring Subscriptions at any time by posting updated pricing through the Services; provided, however, that the prices for your Recurring Subscription will remain in force for the duration of the subscription period for which you have paid. After that period ends, your use of the applicable Services will be charged at the then-current subscription price. If you do not cancel, your Recurring Subscription will automatically renew at the then-current price at the time of renewal and for the same duration as the initial subscription term, and Hume will charge your on-file payment card or method on the first day of the renewal of the subscription term.\nIf your usage exceeds the volume provided under your Recurring Subscription, you will be charged usage overage fees for your Recurring Subscription, as indicated to you upon subscribing. In such cases, you authorize us to charge your payment method on file or any other payment method you choose for these charges.\nYou represent and warrant that you have the right to use any payment method you submit in connection with a payment. We may receive updated information from your issuing bank or our payment service provider about any payment method you have stored with us. You authorize us to charge your payment method, including any updated payment method information we receive, for any charges you are responsible for under these Terms. Verification of information may be required prior to the acknowledgment or completion of any transaction. You will pay all charges incurred by you or on your behalf through the Services, at the prices in effect when such charges are incurred, including all taxes and applicable fees. In the event legal action is necessary to collect on balances due, you will reimburse us and our vendors or agents for all expenses incurred to recover sums due, including attorneys’ fees and other legal expenses.\nAll sales are final. We may offer refunds at our sole discretion.\nHume reserves the right, without prior notice, to impose conditions on the honoring of any coupon, discount, or similar promotion; bar any user from making any transaction; alter the payment options for services; and refuse to provide any user with any Service.\nIndemnification\nYou agree to defend, indemnify, and hold harmless Hume, its affiliates, and each of its respective employees, officers, directors, agents and representatives, from and against all claims, suits, actions, proceedings, damages, losses, liabilities, judgments, penalties, fines, costs, and expenses (including attorneys’ fees) arising from or relating to: (i) your access to, use of or other activities in connection with the Platform; (ii) your Application, its use and any transactions conducted or data or information transmitted through it or the Platform by you or your end users; (iii) your breach of these Terms; or (iv) your actual or alleged infringement, misappropriation or violation of Hume’s, its affiliate’s or any third party’s intellectual property or proprietary rights.\nDisclaimers\nTHE PLATFORM IS LICENSED ON AN “AS IS” AND “AS AVAILABLE” BASIS. HUME AND ITS AFFILIATES, AND EACH OF THEIR RESPECTIVE LICENSORS, SERVICE PROVIDERS AND SUPPLIERS (THE “HUME PARTIES”) MAKE NO WARRANTIES OR CONDITIONS, WHETHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, INCLUDING WARRANTIES AND CONDITIONS OF MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE WITH RESPECT TO THE SUBJECT MATTER OF THESE TERMS INCLUDING THE PLATFORM. WITHOUT LIMITING THE FOREGOING, NONE OF THE HUME PARTIES PROVIDE ANY WARRANTY THAT THE PLATFORM WILL BE FREE FROM ERRORS OR INTERRUPTION. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF EXPRESS OR IMPLIED WARRANTIES OR LIMITATIONS ON HOW LONG SUCH WARRANTIES LAST, SO THE EXCLUSIONS OR LIMITATIONS IN THIS SECTION MAY NOT APPLY TO YOU. THE PLATFORM IS NOT INTENDED TO PROVIDE ANY EMERGENCY, MISSION CRITICAL OR SAFETY RELATED FUNCTIONALITY AND YOU SHALL NOT USE THE PLATFORM IN THAT MANNER. HUME DOES NOT GUARANTEE ANY RESULTS OR THE ACCURACY OF ANY RESULTS THAT YOU MAY OBTAIN FROM THE PLATFORM.\nLimitation of Liability\nNONE OF THE HUME PARTIES WILL BE LIABLE TO YOU OR YOUR END USERS UNDER ANY CAUSE OF ACTION OR THEORY OF LIABILITY (INCLUDING NEGLIGENCE), EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGES, OR FOR ANY (A) INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL OR EXEMPLARY DAMAGES, OR (B) LOSS OF PROFITS, REVENUES, DATA, CUSTOMERS, OPPORTUNITIES, BUSINESS, ANTICIPATED SAVINGS OR GOODWILL, OR (C) UNAVAILABILITY OF THE PLATFORM. THE FOREGOING LIMITATIONS SHALL NOT APPLY TO THE EXTENT SUCH DISCLAIMERS ARE PROHIBITED BY APPLICABLE LAW. THE AGGREGATE LIABILITY OF THE HUME PARTIES UNDER OR IN CONNECTION WITH THESE TERMS WILL NOT EXCEED ONE HUNDRED DOLLARS ($100.00 USD). NOTHING IN THESE TERMS EXCLUDES OR LIMITS A PARTY’S LIABILITY TO THE OTHER PARTY FOR: (A) DEATH OR BODILY INJURY DIRECTLY CAUSED BY A PARTY’S GROSS NEGLIGENCE; OR (B) FRAUD OR WILLFUL MISCONDUCT.\nMiscellaneous Terms\nRelationship of the Parties. These Terms do not create a partnership or joint venture between you and Hume. You and Hume are independent parties for all purposes relating to these Terms. You do not have the power to bind Hume or give any person or entity any rights that Hume has not previously authorized in writing.\nNo Third-Party Beneficiaries. These Terms do not create any third party beneficiary rights in any individual or entity that is not a party to this Agreement.\nAssignment. You may not assign or transfer these Terms or your rights or obligations under them, including in connection with a merger, acquisition, or change of control, without the prior consent of Hume. Subject to the foregoing, these Terms shall inure to the benefit of, each of the parties’ permitted assignees and successors and is binding on the parties and their permitted successors and assignees. Any attempted assignment other than in accordance with this section shall be null and void. Hume may freely assign these Terms or any and all of its rights and obligations under it without notice to you.\nExport Control. You will comply fully with all relevant export laws and regulations of the United States and any other country (“Export Laws”) where you use any portion of the Platform or any other subject matter made available to you by Hume. You certify that you are not on any of the relevant U.S. government lists of prohibited persons, including the Treasury Department’s List of Specially Designated Nationals and the Commerce Department’s List of Denied Persons or Entity List. You further certify that you will not export, re-export, ship, transfer or otherwise use the Platform or any other subject matter made available to you by Hume in any country subject to an embargo or other sanction by the United States and that you will not use the Platform or any other subject matter made available to you by Hume for any purpose prohibited by the Export Laws. In addition, you agree to control, screen or limit (i) the regions from which your end users may access or use any of your Applications, and (ii) the persons or organizations who are your end users, in either case, in order to comply with all applicable Export Laws. Hume is not responsible for and does not have the means to know your end users.\nU.S. Government Agencies. The Platform consists of “Commercial Items”, as defined in the U.S. Federal Acquisition Regulation and related supplements (“FAR”), and of “Commercial Computer Software” and “Commercial Computer Software Documentation”, as such terms are used in in FAR, as applicable. Consistent with FAR, as applicable, the Commercial Computer Software and Commercial Computer Software Documentation are being licensed to the Government Entity end users (a) only as Commercial Items and (b) with only those rights as are granted to all other end users pursuant to the terms and conditions herein. Unless otherwise agreed to in writing by Hume and the Government Entity, the terms of these Terms shall govern each party’s rights and obligations and are in lieu of, and supersede, any FAR clauses or other federal, state or local government clauses or provisions that address a Government Entity’s rights in computer software or technical data.\nWaiver. If Hume fails to enforce or exercise any provision of these Terms, Hume does not waive any of its rights under such provision. Hume waives such rights only if it specifies so in writing and signed by Hume.\nSeverability. If and to the extent any provision of these Terms or portion thereof is held invalid or unenforceable under applicable law, (i) such provision or portion thereof will be deemed modified to the extent reasonably necessary to conform to applicable law but to give maximum effect to the intent of the parties set forth in these Terms, (ii) such provision or portion thereof will be ineffective only as to the jurisdiction in which it is held unenforceable without affecting enforceability in any other jurisdiction, and (iii) the remaining portion of the provision and all other provisions of these Terms shall remain in full force and effect.\nRemedies. If you breach any provision in these Terms, it may cause irreparable harm to Hume and its affiliates. You agree that, if you breach these Terms, Hume has the right to seek injunctive relief against you in addition to any other legal remedies Hume may have.\nDispute Resolution. In the event of a dispute, claim, or controversy (“Dispute”) raising our of or related to these Terms or any aspect of the relationship between you and Hume, whether based in contract, tort, statute, fraud, misrepresentation, or any other legal theory, then either party may commence binding arbitration before a neutral arbitrator, and you agree that Hume and you are each waiving the right to trial by jury (unless the Dispute qualifies for small claims court). Such disputes include, without limitation, disputes arising out of or relating to interpretation or application of this arbitration provision, including the enforceability, revocability, or validity of this dispute resolution provision or any portion of this dispute resolution provision. All such matters will be decided by an arbitrator and not by a court or judge.\nThe arbitration rules specified in this subsection are referred to in as the “Rules.”\nIf your principal place of business is in the United States or Canada, the arbitration will be administered by the American Arbitration Association (AAA) in accordance with the AAA’s Commercial Arbitration Rules and Mediation Procedures.\nIf your principal place of business is in any country in APAC, then the arbitration will be administered by the Singapore International Arbitration Centre (SIAC) in accordance with the Arbitration Rules of the Singapore International Arbitration Centre. “APAC” means the geographic region that includes the following countries: Australia, Bangladesh, Brunei, Burma, Cambodia, China (including Hong Kong Special Administrative Region and Macau Special Administrative Region), Christmas Islands, Fiji, India, Indonesia, Japan, Kiribati, Laos, Malaysia, Marshall Islands, Federated States of Micronesia, Mongolia, Nauru, New Zealand, Palau, Papua New Guinea, Philippines, Samoa, Singapore, Solomon Islands, South Korea, Sri Lanka, Taiwan, Thailand, Timor-Leste, Tonga, Tuvalu, Vanuatu and Vietnam.\nIf your principal place of business is outside of the United States, Canada or any country in APAC, then the arbitration will be administered by the International Chamber of Commerce (ICC) in accordance with ICC Rules of Arbitration.\nYou or Hume may commence the arbitration process called for by these Terms by filing a written demand for arbitration with the applicable arbitration organization and delivering a copy of such demand to the other in accordance with these Terms’ notice provisions.\nIf the Dispute is for an amount less than $100,000 USD, the arbitration will be heard in front of a single arbitrator, and if the Dispute is for an amount of $100,000 USD or more, then the Dispute shall be heard by a panel of three (3) arbitrators. If the Dispute is to be heard in front of a single arbitrator, then the parties shall attempt to mutually agree on the identity of the arbitrator, or if no such agreement can be reached within thirty (30) days of the commencement of the arbitration proceedings, the applicable arbitration organization shall appoint such arbitrator in accordance with the Rules. If the Dispute is to be heard in front of a panel of three (3) arbitrators, each party shall nominate one arbitrator from a list of arbitrators provided by the applicable arbitration organization, and the two party-nominated arbitrators shall select the third arbitrator who will serve as chairman. Each party shall bear its own cost of prosecuting or defending the arbitration (excluding any attorneys’ or other professional fees) and the parties shall equally split the arbitrators’ fees and the applicable applicable applicable applicable arbitration organization’s administrative costs, regardless provisions of this section and judgment upon the award rendered by the arbitrator may be enforced by any court of competent jurisdiction. The arbitrator(s) shall render its decision as soon as reasonably possible after its appointment and must follow these Terms.\nClass Waiver. YOU AGREE THAT ANY ARBITRATION UNDER THIS AGREEMENT WILL TAKE PLACE ON AN INDIVIDUAL BASIS; CLASS ARBITRATIONS AND CLASS ACTIONS ARE NOT PERMITTED AND YOU ARE AGREEING TO GIVE UP THE ABILITY TO PARTICIPATE IN A CLASS ACTION, CONSOLIDATED OR REPRESENTATIVE ACTION AND THE PARTIES ARE WAIVING THE RIGHT TO TRIAL BY JURY. IF FOR ANY REASON A CLAIM PROCEEDS TO COURT RATHER THAN ARBITRATION, EACH PARTY WAIVES ANY RIGHT TO A JURY TRIAL.\nGoverning Law; Venue. These Terms will be governed by and interpreted according to the laws of the State of New York without regard to the State’s conflicts of law rules that would result in the application of the laws of another jurisdiction. These Terms will not be governed by the United Nations Convention on Contracts for the International Sale of Goods and/or its implementation and/or successor legislation and/or regulations, the application of which is expressly excluded. Except as provided in Section 16.9 (Arbitration), any legal suit, action or proceeding arising out of or relating to these Terms will be commenced in a federal or state court in New York City, New York, and each party submits to the exclusive jurisdiction and venue of any such court in any such suit, action, or proceeding.\nNotices. You agree that Hume may contact you by any reasonable means, including via the contact information you have provided in your account, by e-mail or by notifications through Hume’s websites. Notices to Hume will be in writing and delivered by registered or certified mail to the following address: 51 Madison Ave, Floor 31, New York NY 10010. You are responsible for ensuring that the email address and contact information in your account is accurate and current. Notices sent via email will be effective when sent regardless of whether actually received.\nConstruction. As used in these Terms: (i) the terms “include” and “including” are meant to be inclusive and shall be deemed to mean “include without limitation” or “including without limitation,” (ii) the word “or” is disjunctive, but not necessarily exclusive, (iii) words used herein in the singular, where the context so permits, shall be deemed to include the plural and vice versa, (iv) references to “dollars” or “$” shall be to United States dollars unless otherwise specified herein, (v) any pronoun shall apply to all genders, (vi) references to “sale,” “sold,” “seller” and other forms of those words in connection with products includes other forms in which the products may be transferred or put into commercial use (whether not specified), and (vii) unless otherwise specified, all references to days, months or years shall be deemed to be preceded by the word “calendar.” The headings of these Terms are intended solely for convenience of reference and shall be given no effect in the interpretation or construction of these Terms.\nEntire Agreement. These Terms constitute the full and entire understanding between you and Hume with respect to the subject matter hereof. These Terms supersede any prior or contemporaneous understandings, discussions, agreements or communications between you and Hume or its affiliates with respect to the subject matter of these Terms. These Terms may only be changed as expressly provided herein."
            }
        }
    },
    "Billing": {
        "Billing": {
            "url": "https://dev.hume.ai/docs/resources/billing",
            "content": {
                "title": "Billing | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nHow it works\nManaging your account\nUnderstanding your bill\nPricing\nBilling methodology\nFAQ\nResources\nBilling\nCopy page\nHow it works\nJoining the platform: When you sign up and start using our APIs, you’ll initially be using the free credits given to every new account.\nCredit card requirement: Once you’ve exhausted your credit balance, you’ll need to activate billing to continue.\nActivate billing before depleting your credit balance to ensure uninterrupted service.\nMonthly limit and notifications:\nYou’ll have a default monthly limit of $100.\nIf you hit the $100 limit, API calls will return an error, and you’ll be prompted to apply for a monthly limit increase.\nBilling notifications:\nOn the first of each month, you’ll receive an invoice for the previous month’s usage.\nIf your credit card is successfully added, it will be charged automatically.\nYou’ll get a confirmation email for successful transactions or an alert if a transaction fails.\nFailure to pay: If payment isn’t received within 7 days of the invoice date, API access will be suspended until the outstanding balance is settled.\nManaging your account\nUsage information: To view your monthly usage details, visit the Usage & Billing page. There you can track your API usage and see how much of your monthly limit has been utilized.\nNote: After your credits are used, further usage accrues to your monthly cost. You’ll be charged this amount on the first of the following month. Your monthly cost is updated daily at 08:00 UTC.\nBilling portal: To manage your billing details, navigate to Usage & Billing and select Manage payments and view invoices. There you can update your payment method, view past invoices, and keep track of upcoming charges.\nUnderstanding your bill\nPricing\nFind up-to-date pricing information at hume.ai/pricing.\nBilling methodology\nAudio and video:\nOur listed prices are presented per minute for ease of understanding.\nHowever, we bill these services on a corresponding per-second basis to ensure precise and fair charges. This means you are only billed for the exact amount of time your audio or video content is processed.\nImage and text:\nImage processing charges are incurred per image.\nText processing is billed based on the number of words processed.\nFAQ\nWhy do I have a negative credit balance?\nIf you have questions about your bill or need assistance understanding the charges, please contact billing@hume.ai.\nWas this page helpful?\nYes\nNo\nPrevious\nErrors\nNext\nBuilt with"
            }
        }
    },
    "Errors": {
        "Errors": {
            "url": "https://dev.hume.ai/docs/resources/errors",
            "content": {
                "title": "Errors | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nConfiguration errors\nWebSocket status codes\nService errors\nWarnings\nCommon errors\nTranscript confidence below threshold value\nResources\nErrors\nCopy page\nConfiguration errors\nConfiguration errors indicate that something about the API call was not configured correctly. The error message you get from the Hume APIs will often contain more information than we’re able to provide on this page. For example if an audio file is too long, the error message from the API will specify the limit as well as the length of the audio received.\nCode Description\nE0100 The WebSocket request could not be parsed as valid JSON. The Hume API requires JSON serializable payloads.\nE0101 You may be missing or improperly formatting a required field. This generic error indicates that the structure of your WebSocket request was invalid. Please see the error message you received in the API response for more details.\nE0102 The requested model was incompatible with the file format received. Some models are not compatible with every file type. For example, no facial expressions will be detected in a text file. Audio can be extracted out of some video files, but if the video has no audio, then models like Speech Prosody and Vocal Burst will not be available.\nE0200 Media provided could not be parsed into a known file format. Hume APIs support a wide range of file formats and media types including audio, video, image, text, but not all formats are supported. If you receive this error and believe your file type should be supported please reach out to our support team.\nE0201 Media could not be decoded as a base64 encoded string. The data field in the request payload should be base64 encoded bytes. If you want to pass raw text without encoding it you can do so with the raw_text parameter.\nE0202 No audio signal could be inferred from the media provided. This error indicates that audio models were configured, but the media provided could not be parsed into a valid audio file.\nE0203 Your audio file was too long. The limit is 5000 milliseconds. The WebSocket endpoints are intended for near real-time processing of data streams. For larger files, consider using the Hume Expression Measurement API REST endpoints.\nE0204 Your video file was too long. The limit is 5000 milliseconds. For best performance we recommend passing individual frames of video as images rather than full video files. For larger files, consider using the Hume Expression Measurement API REST endpoints.\nE0205 Your image file was too large. The limit is 3,000 x 3,000 pixels. The WebSocket endpoints are intended for near real-time processing of data streams. For larger files, consider using the Hume Expression Measurement API REST endpoints.\nE0206 Your text file was too long. The limit is 10,000 characters. The WebSocket endpoints are intended for near real-time processing of data streams. For larger files, consider using the Hume Expression Measurement API REST endpoints.\nE0207 The URL you’ve provided appears to be incorrect. Please verify that you’ve entered the correct URL and try submitting it again. If you’re copying and pasting, ensure that the entire URL has been copied without any missing characters.\nE0300 You’ve run out of credits. Activate billing to continue making API calls.\nE0301 Your monthly credit limit has been reached. Once billing is activated, users can accrue charges up to a predetermined monthly cap. This limit ensures that users do not accumulate excessive debt without assurance of payment. If you require a higher limit, you may manually apply for a credit limit increase on the Usage page. Alternatively, the limit will reset at the beginning of the next month. For more information, please see our docs on billing.\nE0400 You’ve referenced a resource that doesn’t exist in our system. Please check if the name or identifier you used is correct and try again.\nE0401 Your upload failed. Please ensure your file meets our format and size requirements, and attempt to upload it again.\nE0402 The CSV file you used to create or update a dataset is missing a header row. The header specifies what each column represents. Update your CSV file and retry your request. For more information about how to format your dataset CSV please see our tutorial on dataset creation.\nE0500 Your dataset doesn’t meet the minimum sample size requirement. Please add more files to your dataset and resubmit your training job. For more information, please see our docs on dataset requirements.\nE0501 Your dataset contains a target column with empty values. Please clean your dataset so that all labels are valid categorical or numeric values and then resubmit your training job. For more information on target columns please see our docs on dataset requirements.\nE0502 Your dataset contains a target column with infinite values. Please clean your dataset so that all labels are valid categorical or numeric values and then resubmit your training job. For more information on target columns please see our tutorial on dataset creation.\nE0503 For classification tasks, your dataset must include at least two distinct classes. Please check your dataset has two unique labels in the target column.\nE0504 Some classes in your dataset don’t have enough samples. To ensure that the model we produce is of the highest quality we require your dataset to be relatively balanced across classes. Please check the error message for which class should have more samples (or remove that class entirely). Please see our docs on dataset requirements for more details.\nE0505 The target column you’ve selected doesn’t exist in the dataset. Please review the columns that exist in your dataset and select a valid column name.\nE0506 Your chosen target column is not a valid target column. Please ensure that you select a column with labels rather than the file_id column or another reserved column name.\nE0705 Your custom model was disconnected due to a server connection interruption. Please check your internet connection, ensure the server is still running, and verify that the server URL is correct. Also, make sure no firewall or security settings are blocking the connection.\nE0706 Hume’s API cannot reach your custom language model. Please ensure that your language model is accessible and try again.\nE0707 The message sent to Hume is not formed in the correct way of either {\"type\": \"assistant_input\", \"text\": <your text here>} or {\"type\": \"assistant_end\"}\nE0708 The chat group you’re trying to resume does not exist. Please check the chat group identifier and try again.\nE0709 The configuration you are trying to use does not exist. Please check the configuration identifier and try again.\nE0710 You are attempting to resume a chat group with a new configuration. This operation is not allowed. Please use the original configuration or create a new chat group with the desired configuration.\nE0711 You are attempting to use a supplemental language model that is not currently available as a Hume-managed LLM. Please provide an API key from your model provider, or switch to a different supplemental LLM.\nE0712 The custom language model timed out during the connection attempt. This could be due to network issues, server availability, or firewall restrictions. Please check your connection and try again.\nE0713 The connection failed to the custom model due to a fatal error during the connection attempt. Please verify that the custom language model is correctly configured and accessible.\nE0714 The EVI WebSocket connection was closed due to the user inactivity timeout being reached. This timeout is specified in the inactivity parameter within the timeouts field of your EVI configuration.\nE0715 The EVI WebSocket connection was closed due to the maximum duration timeout being reached. This timeout is specified in the max_duration parameter within the timeouts field of your EVI configuration.\nE0716 The session settings provided were invalid and therefore were not applied. More details about how to resolve the misconfiguration are available in the API response.\nE0717 The EVI WebSocket connection was closed because a request was made to resume a chat group which contains an active chat. Please check that you are not already running an active chat session with the same chat group.\nE0718 The supplemental LLM provider has degraded API behavior. You can try again later or change the supplemental LLM in your EVI configuration.\nE0719 The supplemental LLM provider has an outage. You can try again later or change the supplemental LLM in your EVI configuration.\nE0720 The chat group configured for chat resumability could not be found. Please check that you specified your resumed_chat_group_id parameter correctly and that data retention is enabled in your account settings.\nE0723 Failed to parse incoming audio. Data was formatted as whole audio files. Audio must be streamed. Please visit https://dev.hume.ai/reference/empathic-voice-interface-evi/chat/chat#send.Audio%20Input.data for more information on audio guidelines. Detected {codec}, {sample_rate} Hz, {num_channels} ch.\nThe connection will be closed automatically after ten identical configuration errors to avoid unintended looping.\nWebSocket status codes\nCode Description\n1000 close_normal indicates an expected, intentional disconnect initiated by the server, such as when the built-in hang-up tool closes the connection. This code is also used for inactivity timeout and max duration timeout, indicating that the WebSocket connection was closed due to remaining inactive for too long or exceeding the maximum allowed duration.\n1008 policy_violation occurs when the WebSocket connection encounters an issue that cannot be recovered due to user error. Please review your request and ensure it adheres to the APIs guidelines and policies.\n1011 server_error indicates that the WebSocket connection encountered an issue that cannot be recovered due to an internal Hume server error. Please try again later or contact support if the issue persists.\nService errors\nIf you encounter an error code starting with I (for example, error code I0100), it indicates an outage or a bug in a Hume service. Our team will already have been alerted of the internal error, but if you need immediate assistance please reach out to our support team.\nWarnings\nWarnings indicate that the payload was configured correctly, but no results could be returned.\nCode Description\nW0101 No vocal bursts could be detected in the media.\nW0102 No face meshes could be detected in the media.\nW0103 No faces could be detected in the media.\nW0104 No emotional language could be detected in the media.\nW0105 No speech could be detected in the media.\nW0106 No dynamic variable(s) found matching the one(s) specified.\nCommon errors\nSome errors will not have an associated error code, but are documented here.\nTranscript confidence below threshold value\nThis error indicates that our transcription service had difficulty identifying the language spoken in your audio file or the quality was too low. We prioritize quality and accuracy, so if it cannot transcribe with confidence, our models won’t be able to process it further.\nBy default, we use an automated language detection method for our Speech Prosody, Language, and NER models. However, if you know what language is being spoken in your media samples, you can specify it via its BCP-47 tag and potentially obtain more accurate results.\nIf you see the message above there are few steps you can do to resolve the issue:\nVerify we support the language\nEnsure you are providing clear, high-quality audio files.\nSpecify the language within your request if you know the language in the audio.\nHume Python SDK\nJSON\n1 import asyncio\n2 from hume import AsyncHumeClient\n3 from hume.expression_measurement.batch import Prosody, Transcription, Models\n4 from hume.expression_measurement.batch.types import InferenceBaseRequest\n5\n6 async def main():\n7     # Initialize an authenticated client\n8     client = AsyncHumeClient(api_key=\"<YOUR_API_KEY>\")\n9\n10     # Define the filepath(s) of the file(s) you would like to analyze\n11     local_filepaths = [\n12         open(\"<YOUR_FILE_PATH>\", mode=\"rb\"),\n13     ]\n14\n15     # Create a default configuration for the prosody model\n16     prosody_config = Prosody()\n17\n18     # Create a transcription coniguration with the language set to English\n19     transcription_config = Transcription(language=\"en\")\n20\n21     # Create a Models object\n22     models_chosen = Models(prosody=prosody_config)\n23     \n24     # Create a stringified object containing the configuration\n25     stringified_configs = InferenceBaseRequest(models=models_chosen, transcription=transcription_config)\n26\n27     # Start an inference job and print the job_id\n28     job_id = await client.expression_measurement.batch.start_inference_job_from_local_file(\n29         json=stringified_configs, file=local_filepaths\n30     )\n31     print(job_id)\n32\n33 if __name__ == \"__main__\":\n34     asyncio.run(main())\nSee the full list of languages supported by the Expression Measurement API here.\nYou may specify any of the following BCP-47 tags for transcription: zh, da, nl, en, en-AU, en-IN, en-NZ, en-GB, fr, fr-CA, de, hi, hi-Latn, id, it, ja, ko, no, pl, pt, pt-BR, pt-PT, ru, es, es-419, sv, ta, tr, or uk.\nWas this page helpful?\nYes\nNo\nPrevious\nAbout the Science\nNext\nBuilt with"
            }
        }
    },
    "About the science": {
        "About the science": {
            "url": "https://dev.hume.ai/docs/resources/science",
            "content": {
                "title": "About the Science | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nModalities\nFacial Expression\nSpeech Prosody\nVocal Bursts\nEmotional Language\nPublished Research\nResources\nAbout the Science\nCopy page\nWhat is it about speaking in person that allows us to understand each other so much more accurately than text alone? It isn’t what we say—it’s the way we say it. Science consistently demonstrates that expressions convey important information that is vital for social interaction and forms the building blocks of empathy.\nThat being said, expressions aren’t direct windows into the human mind. Measuring and interpreting expressive behavior is a complex and nuanced task that is the subject of ongoing scientific research.\nThe scientists at Hume AI have run some of the largest-ever psychology studies to better understand how humans express themselves. By investigating expressions around the world and what they mean to the people making them, we’ve mapped out the nuances of expression in the voice, language, and face in unprecedented detail. We’ve published this research in the world’s leading scientific journals and, for the first time, translated it into cutting-edge machine learning models.\nThese models, shaped by a new understanding of human expression, include:\nFacial Expression\nSpeech Prosody\nVocal Bursts\nEmotional Language\nModalities\nFacial Expression\nFacial expression is the most well-studied modality of expressive behavior, but the overwhelming focus has been on six discrete categories of facial movement or time-consuming manual annotations of facial movements (the scientifically useful, but outdated, Facial Action Coding System). Our research shows that these approaches capture less than 30% of what typical facial expressions convey.\nHume’s Facial Emotional Expression model generates 48 outputs encompassing the dimensions of emotional meaning people reliably attribute to facial expressions. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.\nHume’s FACS 2.0 model is a new generation automated facial action coding system (FACS). With 55 outputs encompassing 26 traditional actions units (AUs) and 29 other descriptive features (e.g., smile, scowl), FACS 2.0 is even more comprehensive than manual FACS annotations.\nOur facial expression models are packaged with face detection and work on both images and videos.\nIn addition to our image-based facial expression models, we also offer an Anonymized Facemesh model for applications in which it is essential to keep personally identifiable data on-device (e.g., for compliance with local laws). Instead of face images, our facemesh model processes facial landmarks detected using Google’s MediaPipe library. It achieves about 80% accuracy relative to our image-based model.\nTo read more about the team’s research on facial expressions, check out our publications in American Psychologist (2018), Nature (2021), and iScience (2024).\nSpeech Prosody\nSpeech prosody is not about the words you say, but the way you say them. It is distinct from language (words) and from non-linguistic vocal utterances.\nOur Speech Prosody model generates 48 outputs encompassing the 48 dimensions of emotional meaning that people reliably distinguish from variations in speech prosody. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.\nOur Speech Prosody model is packaged with speech detection and works on both audio files and videos.\nTo read more about the team’s research on speech prosody, check out our publications in Nature Human Behaviour (2019) and Proceedings of the 31st ACM International Conference on Multimedia (2023).\nVocal Bursts\nNon-linguistic vocal utterances, including sighs, laughs, oohs, ahhs, umms, and shrieks (to name but a few), are a particularly powerful and understudied modality of expressive behavior. Recent studies reveal that they reliably convey distinct emotional meanings that are extremely well-preserved across most cultures.\nNon-linguistic vocal utterances have different acoustic characteristics than speech emotional intonation (prosody) and need to be modeled separately.\nOur Vocal Burst Expression model generates 48 outputs encompassing the distinct dimensions of emotional meaning that people distinguish in vocal bursts. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.\nOur Vocal Burst Description model provides a more descriptive and categorical view of nonverbal vocal expressions (“gasp,” “mhm,” etc.) intended for use cases such as audio captioning. It generates 67 descriptors, including 30 call types (“sigh,” “laugh,” “shriek,” etc.) and 37 common onomatopoeia transliterations of vocal bursts (“hmm,” “ha,” “mhm,” etc.).\nOur vocal burst models are packaged with non-linguistic vocal utterance detection and works on both audio files and videos.\nTo read more about the team’s research on vocal bursts, check out our publications in American Psychologist (2019), Interspeech 2022, ICASSP 2023, and Nature Human Behaviour (2023).\nEmotional Language\nThe words we say include explicit disclosures of emotion and implicit emotional connotations. These meanings are complex and high-dimensional.\nFrom written or spoken words, our Emotional Language model generates 53 outputs encompassing different dimensions of emotion that people often perceive from language. As with every model, the labels for each dimension are proxies for how people tend to label the underlying patterns of behavior. They should not be treated as direct inferences of emotional experience.\nOur Emotional Language model is packaged with speech transcription and works on audio files, videos, and text.\nOur Named Entity Recognition (NER) model can also identify topics or entities (people, places, organizations, etc.) mentioned in speech or text and the tone of language they are associated with, as identified by our emotional language model.\nPublished Research\nYou can access a comprehensive list of our published research papers along with PDFs for download here.\nWas this page helpful?\nYes\nNo\nPrevious\nUse case guidelines\nNext\nBuilt with"
            }
        }
    },
    "Use case guidelines": {
        "Use case guidelines": {
            "url": "https://dev.hume.ai/docs/resources/use-case-guidelines",
            "content": {
                "title": "Use case guidelines | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nEthical guidelines\nScientific best practices\nResources\nUse case guidelines\nCopy page\nEthical guidelines\nUnderstanding expressive communication is essential to building technologies that address our needs and improve our well-being. But technologies that recognize language and nonverbal behavior can also pose risks. That’s why we require that all commercial applications incorporating our APIs adhere to the ethical guidelines of The Hume Initiative.\nScientific best practices\nUse inductive methods to identify the expressive signals that matter for your application. Even if you are interested in a specific emotion like “anger,” how that emotion is expressed depends on setting: anger on a football field sounds different than anger on a customer service call. Our models succinctly compress the representation of emotional expression so that, even with limited data, you can examine how their outputs can be used in your specific research or application setting. You can do this by using statistical methods like regression or classification, or by examining the distribution of expressions in your data using our Playground.\nNever assume a one-to-one mapping between emotional experience and expression. The outputs of our models should be treated as measurements of complex expressive behavior. We provide labels to our outputs indicating what these dimensions of expression are often reported to mean, but these labels should not be interpreted as direct inferences of how someone is feeling at any given time. Rather, “a full understanding of emotional expression and experience requires an appreciation of a wide degree of variability in display behavior, subjective experience, patterns of appraisal, and physiological response, both within and across emotion categories” (Cowen et al., 2019).\nNever overlook the nuances in emotional expression. For instance, avoid the temptation to focus on just the top label. We provide interactive visualizations in our Playground to help you map out complex patterns in real-life emotional behavior. These visualizations are informed by recent advances in emotion science, departing from reductive models that long “anchored the science of emotion to a predominant focus on prototypical facial expressions of the “basic six”: anger, disgust, fear, sadness, surprise, and happiness,” and embracing how “new discoveries reveal that the two most commonly studied models of emotion—the basic six and the affective circumplex (comprising valence and arousal)—each capture at most 30% of the variance in the emotional experiences people reliably report and in the distinct expressions people reliably recognize.” (Cowen et al., 2019)\nAccount for culture-specific meanings and display tendencies. Studies have routinely observed subtle cultural differences in the meaning of expressions as well as broader “variations in the frequency and intensity with which different expressions were displayed” (Cowen et al., 2022). Given these differences, empathic AI applications should be tested in each population in which they are deployed and fine-tuned when necessary. Read about the science behind our models if you’d like to delve deeper into how they work.\nWas this page helpful?\nYes\nNo\nPrevious\nPrivacy\nNext\nBuilt with"
            }
        }
    },
    "Privacy": {
        "Privacy": {
            "url": "https://dev.hume.ai/docs/resources/privacy",
            "content": {
                "title": "Privacy | Hume API",
                "content": "Search or ask AI\n/\nDocumentation\nAPI Reference\nChangelog\nDiscord\nIntroduction\nWelcome to Hume AI\nGetting your API keys\nSupport\nPricing\nText-to-speech (TTS)\nOverview\nQuickstart\nVoices\nPrompting\nActing instructions\nContinuation\nHume MCP Server\nFAQ\nEmpathic Voice Interface (EVI)\nOverview\nQuickstart\nConfiguration\nFeatures\nGuides\nFAQ\nExpression Measurement\nOverview\nProcessing batches of media files\nReal-time measurement streaming\nCustom models\nFAQ\nResources\nTerms of use\nBilling\nErrors\nAbout the science\nUse case guidelines\nPrivacy\nStatus\nRoadmap\nStart building\nGet support\nSystem\nOn this page\nPrivacy Policy\nZero Data Retention and Data Usage Options for the EVI API\nTo enable or disable these options\nExpression Measurement API Data Privacy\nAPI Data Usage Policy\nConsumer Services FAQ\nResources\nPrivacy\nCopy page\nPrivacy Policy\nOur Privacy Policy governs how we collect and use personal information submitted to our products.\nZero Data Retention and Data Usage Options for the EVI API\nHume AI is HIPAA compliant, with features to enhance user privacy and data control. Our portal currently supports enabling/disabling these features in the user’s profile page.\nZero data retention: This feature allows users to turn off the storage of all chat histories (transcripts) or voice recordings for the EVI API. Other metadata such as API usage information will still be stored.\nOpt-out of data being used for training: By default, anonymized data from user interactions with the EVI API is used to improve our models. Users can toggle this option to prevent data from being used for training purposes.\nFor added control, use a custom language model and obtain a Business Associate Agreement (BAA) directly with the model provider. To request a BAA and/or Data Processing Addendum (DPA) with Hume, please contact legal@hume.ai.\nBy default, data retention for EVI is enabled, and user data may be used for model training. Users must explicitly opt out to disable these features.\nTo enable or disable these options\nLog into your Hume AI account on platform.hume.ai.\nNavigate to your Profile page by clicking on the profile icon on the sidebar.\nScroll down to the Privacy section where you will see the options for “Do not retain data” and “Do not use for training.”\nToggle the switches next to these options to enable or disable them according to your preference.\nClick on ‘Save changes’ to apply your settings.\nOpting out of data retention will disable certain features, including the ability to resume chats and access your chat history.\nExpression Measurement API Data Privacy\nFor the Expression Measurement API, we maintain strict data privacy practices:\nNo file retention: Image, audio, and video files processed by the API are not retained - they are only used temporarily during analysis. All files sent to the expression measurement API are immediately deleted after they are processed.\nOutput data: Expression measurement results are retained in our database until the user requests deletion. This allows us to return results for specific previous jobs through the get job predictions endpoint. These results are accessible only to the user, although they may sometimes be viewed by select members of the Hume AI team for debugging.\nTranscription data: When using the batch API with transcription enabled, the transcripts will be retained along with the expression measures. Developers can set the transcription option to null to disable transcription.\nNo training usage: Data submitted through the Expression Measurement API is never used to train or improve our models. Datasets for the Custom Models API are only used to train the developer’s model, and are not used by Hume AI.\nAPI Data Usage Policy\nOur API Data Usage Policy details how and when we store API data.\nConsumer Services FAQ\nOur Consumer Services FAQ explains how and when we store data processed by our frontend applications like our Playground.\nDoes Hume AI train on my content to improve model performance?\nHow do I delete my account?\nIs my content shared with third parties?\nWhere is my content stored?\nDo humans view my content?\nDoes Hume AI sell my data?\nHow do I submit a data privacy request?\nWas this page helpful?\nYes\nNo\nPrevious\nBuilt with"
            }
        }
    },
    "Status": {
        "Status": {
            "url": "https://status.hume.ai/",
            "content": {
                "title": "Hume API Status",
                "content": "Report a problem\nSubscribe to updates\nWe’re fully operational\nWe’re not aware of any issues affecting our systems.\nSystem status\nFeb 2025\n-\nMay 2025\nAPIs\n3 components\n99.77% uptime\nWebsites\n3 components\n99.92% uptime\nPayments & Usage\n2 components\n99.97% uptime\nDocumentation\n1 component\nCalendar\nMay 2025\nM\nT\nW\nT\nF\nS\nS\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nPowered by\nPrivacy policy\n·\nTerms of service"
            }
        }
    },
    "Roadmap": {
        "Roadmap": {
            "url": "https://hume.canny.io/",
            "content": {
                "title": "Hume AI Feedback",
                "content": "Hume AI\nLog in\nSign up\nRoadmap\nFeedback\nSearch\nBoards\nFeature Requests\n115\nBugs\n31\nLanguages\n33\nRoadmap\nFilters\nPlanned\n29\nOrganizations / teams support\nFEATURE REQUESTS\n9\nSupport Octave voices with EVI\nFEATURE REQUESTS\n20\nClientless tool calls (during phone calls)\nFEATURE REQUESTS\nIn Progress\n29\nEVI 2 Voice Hallucinations (voice not matching transcript, unwanted voice changes)\nBUGS\n29\nReducing false positive interruptions\nBUGS\n12\nEVI completely stops speaking when there is a false positive interruption\nBUGS\n7\nCustom character creation in iOS app\nFEATURE REQUESTS\nComplete\n3\nAbility to rename custom voices\nFEATURE REQUESTS\n3\nspecific pause durations in the playground\nFEATURE REQUESTS\n1\nDelete custom voices via API\nFEATURE REQUESTS\n5\nSending EVI call info (summaries, structured data) to webhook URL after calls\nFEATURE REQUESTS\n5\niOS background call functionality\nFEATURE REQUESTS\n9\nEVI webhooks\nFEATURE REQUESTS\n22\nTTS (Text-to-speech) endpoint\nFEATURE REQUESTS\n3\nAbility to track every user's billing info with a single Hume API key\nFEATURE REQUESTS\n3\nAPI endpoint to playback user's own EVI call recordings\nFEATURE REQUESTS\n3\nCustom voice sliders do not work consistently\nBUGS\n4\nPrevent EVI 2 from misspeaking short phrases\nBUGS\n4\nReduce latency with EVI 2\nFEATURE REQUESTS\n6\nTool use for Gemini supplemental LLMs\nFEATURE REQUESTS\n7\n*Updated with prompting fix* Improving EVI 2 pronunciation with integers, IDs, phone numbers, dates, and other non-word strings\nBUGS\n3\nEVI 2 issues with speaking emails\nFEATURE REQUESTS\nPowered by Canny"
            }
        }
    }
}